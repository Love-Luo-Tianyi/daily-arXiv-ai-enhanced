<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.CL](#cs.CL) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Impact of Image Resolution on Age Estimation with DeepFace and InsightFace](https://arxiv.org/abs/2511.14689)
*Shiyar Jamo*

Main category: cs.CV

TL;DR: 本研究分析了图像分辨率对DeepFace和InsightFace进行年龄估计的影响，发现224x224像素为最佳分辨率，过低或过高分辨率均会降低准确性，且InsightFace整体性能更优。


<details>
  <summary>Details</summary>
Motivation: 自动年龄估计技术广泛应用于年龄验证场景，但在实际应用中，输入图像的分辨率往往存在很大差异。本研究旨在评估图像分辨率的变化如何影响使用DeepFace和InsightFace模型进行年龄估计的准确性。

Method: 研究使用了IMDB-Clean数据集中的1000张图像，将其处理为7种不同的分辨率，生成7000个测试样本。使用DeepFace和InsightFace两个框架进行处理，并通过平均绝对误差（MAE）、标准差（SD）和中位数绝对误差（MedAE）来评估性能。

Result: 研究发现224x224像素是两个框架的最佳分辨率，其中InsightFace的MAE为7.46年，DeepFace为10.83年。低分辨率会导致误差大幅增加，而过高的分辨率也会降低准确性。此外，InsightFace在所有分辨率下的运行速度均快于DeepFace。

Conclusion: 输入图像的分辨率对年龄估计的准确性具有清晰且一致的影响。DeepFace和InsightFace在224x224像素分辨率下均能达到最佳性能，且InsightFace在速度和准确度上总体表现更佳。

Abstract: Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.

</details>


### [2] [Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images](https://arxiv.org/abs/2511.14702)
*Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Meryem Jabrane,Vicente Grau,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合ECG生理信息和MRI解剖信息的多模态心肌瘢痕分割方法，通过时序感知特征融合机制解决非同步采集问题，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: LGE-MRI图像中的心肌瘢痕分割受限于对比度变化和成像伪影。虽然心电图（ECG）能提供关于瘢痕定位的互补生理信息，但如何有效融合非同步采集的ECG与MRI数据是一个挑战。

Method: 提出了一种多模态框架，将ECG衍生的电生理信息与AHA-17图谱的解剖先验相结合。针对ECG与MRI非同步采集的问题，引入了时序感知特征融合（TAFF）机制，根据采集时间差动态加权并融合特征。

Result: 在临床数据集上，该方法显著优于仅基于图像的SOTA基线（nnU-Net）。瘢痕分割的平均Dice评分从0.6149提升至0.8463，精确度达到0.9115，灵敏度达到0.9043。

Conclusion: 结果表明，融合生理（ECG）和解剖（AHA-17）知识使模型能够利用图像之外的信息，为实现更鲁棒且具有生理依据的心脏瘢痕分割确立了新方向。

Abstract: Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.

</details>


### [3] [FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation](https://arxiv.org/abs/2511.14712)
*Yunfeng Wu,Jiayi Song,Zhenxiong Tan,Zihao He,Songhua Liu*

Main category: cs.CV

TL;DR: 本文介绍了一种名为FreeSwim的无需训练的方法，通过结合滑动窗口注意力和双路径交叉注意力覆盖策略，利用预训练的视频扩散Transformer生成连贯的超高分辨率视频。


<details>
  <summary>Details</summary>
Motivation: 现代基于Transformer的视频生成器中，注意力机制的二次方时间和内存复杂度使得针对超高分辨率视频的端到端训练成本极高且难以实现。

Method: 1. **内向滑动窗口注意力机制**：基于保持查询token训练尺度感受野的观察，用于保留视觉保真度和细节。
2. **双路径管道与交叉注意力覆盖（Cross-Attention Override）**：为了解决局部窗口注意力导致的重复内容和全局连贯性缺失问题，引入全感受野分支通过交叉注意力指导局部注意力分支。
3. **交叉注意力缓存策略**：为全感受野分支引入缓存，避免频繁计算全3D注意力，从而提高效率。

Result: 实验表明，该方法能够生成具有细粒度视觉细节的超高分辨率视频，且效率很高。在VBench基准测试中，该方法表现优异，甚至超过了基于训练的替代方案，同时具备具有竞争力或更高的运行效率。

Conclusion: 本文提出了一种无需训练的高效范式，证明了通过精心设计的双路径注意力机制和缓存策略，可以利用现有的预训练视频扩散Transformer模型，在保持全局连贯性的同时生成具有细粒度细节的超高分辨率视频。

Abstract: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim

</details>


### [4] [Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising](https://arxiv.org/abs/2511.14719)
*Yifan Wang,Liya Ji,Zhanghan Ke,Harry Yang,Ser-Nam Lim,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的零样本合成视频增强框架，利用深度和边缘等结构信息引导生成，实现高保真和结构一致的真实感渲染。


<details>
  <summary>Details</summary>
Motivation: 为了增强合成视频（如从模拟器生成的视频）的真实感，使其达到照片级效果，同时在空间和时间域上保留原始视频的多层次结构。

Method: 采用零样本（Zero-shot）框架，建立在无需微调的视频扩散基础模型之上。具体通过辅助模型估计合成视频的深度图、语义图和边缘图，并将这些结构感知信息作为条件引入生成/去噪过程，以指导视频增强。

Result: 实验表明，该方法在保持最先进的照片级真实感质量的同时，在与原始视频的结构一致性方面优于现有的基准方法。

Conclusion: 这是一种简单、通用且强大的合成视频真实感增强方法，能够有效确保增强后的视频在结构和语义水平上与原始合成视频保持一致。

Abstract: We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.

</details>


### [5] [A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments](https://arxiv.org/abs/2511.14742)
*Stefan Cobeli,Kazi Shahrukh Omar,Rodrigo Valença,Nivan Ferreira,Fabio Miranda*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经场的3D城市环境隐式表示方法，通过高效的直接和反向查询机制，解决了大规模3D数据探索中常见的遮挡和视角调整难题。


<details>
  <summary>Details</summary>
Motivation: 尽管3D城市数据集日益增多，但由于计算瓶颈和交互的复杂性（特别是复杂几何结构导致的遮挡和繁琐的手动视角调整），从中高效提取有价值的信息仍然非常困难。

Method: 提出了一种基于视角的探索方法，利用神经场（Neural Field）构建3D环境的高效隐式表示。该方法将视角编码为向量场，支持快速的直接查询（计算视角评估指标）和反向查询（搜索匹配特定模式且避免遮挡的视角）。

Result: 该方法成功支持了可见性分析、日照评估和新开发项目视觉影响评估等任务。通过定量实验、实际案例和专家反馈验证，证明其在寻找理想视角、分析建筑立面可见性及评估室外空间视野方面具有显著效果。

Conclusion: 该研究证明了利用神经场进行隐式表示能有效解决大规模3D城市环境探索中的效率低下和交互困难问题，为城市规划和分析提供了强有力的工具，且代码和数据已开源。

Abstract: Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.

</details>


### [6] [Vision Large Language Models Are Good Noise Handlers in Engagement Analysis](https://arxiv.org/abs/2511.14749)
*Alexander Vedernikov,Puneet Kumar,Haoyu Chen,Tapio Seppänen,Xiaobai Li*

Main category: cs.CV

TL;DR: 本文提出利用视觉大语言模型（VLM）辅助细化噪声标签，并结合课程学习与软标签策略，显著提升了视频投入度识别的性能。


<details>
  <summary>Details</summary>
Motivation: 视频数据集中的投入度识别（Engagement Recognition）任务主要面临标签主观性强和噪声干扰的挑战，这限制了模型的性能，使其不同于传统的图像分类任务。

Method: 1. 利用视觉大语言模型（VLM）并通过问卷形式提取行为线索，将数据划分为高可靠性和低可靠性子集以优化标注。
2. 引入结合课程学习（Curriculum Learning）与软标签细化（Soft Label Refinement）的训练策略。
3. 逐步将模糊样本纳入训练过程，同时调整监督信号以反映不确定性。

Result: 1. 在EngageNet基准测试中，6个特征设置中有3个超越了SOTA，最大提升幅度为1.21%。
2. 在DREAMS和PAFE数据集上，F1分数分别提升了0.22和0.06。
3. 证明了在经VLM细化的高可靠性子集上训练并结合课程策略，能显著提升模型效果。

Conclusion: 该研究证明，利用VLM解决标签主观性问题，并结合课程学习策略，能有效增强经典计算机视觉模型在投入度识别任务上的表现，超越了现有的最先进方法。

Abstract: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.

</details>


### [7] [Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers](https://arxiv.org/abs/2511.14751)
*Yutian Chen,Yuheng Qiu,Ruogu Li,Ali Agha,Shayegan Omidshafiei,Jay Patrikar,Sebastian Scherer*

Main category: cs.CV

TL;DR: Co-Me是一种用于视觉几何Transformer的免重训练加速机制，通过合并低置信度Token，实现了最高11.3倍的加速并保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉几何Transformer计算开销大，难以满足实时3D感知和重建的需求；现有的基于相似度的合并或剪枝策略效率或准确性不足。

Method: 提出了置信度引导的Token合并（Co-Me）。该方法蒸馏了一个轻量级置信度预测器，根据不确定性对Token进行排序，并选择性地合并低置信度的Token，且无需对基础模型进行重训练或微调。

Result: Co-Me适用于多种多视图和流式视觉几何Transformer。在VGGT上实现了高达11.3倍的加速，在MapAnything上实现了7.2倍的加速。

Conclusion: Co-Me利用置信度信号在显著减少计算量的同时保持了空间覆盖和性能，使得视觉几何Transformer能够实际应用于实时3D感知任务。

Abstract: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.

</details>


### [8] [UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning](https://arxiv.org/abs/2511.14760)
*Rui Tian,Mingfei Gao,Haiming Gang,Jiasen Lu,Zhe Gan,Yinfei Yang,Zuxuan Wu,Afshin Dehghan*

Main category: cs.CV

TL;DR: UniGen-1.5 是一个统一的多模态大模型，通过引入统一强化学习策略和指令对齐，显著提升了图像理解、生成及编辑能力，性能媲美 GPT-Image-1。


<details>
  <summary>Details</summary>
Motivation: 旨在基于 UniGen 进一步增强图像理解和生成能力，同时解锁并强化图像编辑功能，以解决现有模型在多模态任务统一处理上的局限。

Method: 1. 全面改进模型架构和训练流程；
2. 提出统一强化学习（RL）策略，通过共享奖励模型联合优化生成和编辑任务；
3. 引入轻量级编辑指令对齐阶段，提升模型对编辑指令的理解能力。

Result: UniGen-1.5 在 GenEval (0.89) 和 ImgEdit (4.31) 上取得了优异成绩，超越了 BAGEL 等现有 SOTA 模型，性能可与 GPT-Image-1 等专有模型相媲美。

Conclusion: UniGen-1.5 通过架构优化和创新的训练策略，成功构建了一个具备高性能图像理解、生成和编辑能力的统一多模态大模型。

Abstract: We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.

</details>


### [9] [ARC Is a Vision Problem!](https://arxiv.org/abs/2511.14761)
*Keya Hu,Ali Cy,Linlu Qiu,Xiaoman Delores Ding,Runqian Wang,Yeyin Eva Zhu,Jacob Andreas,Kaiming He*

Main category: cs.CV

TL;DR: VARC框架将ARC任务重新表述为基于视觉的图像到图像翻译问题，利用ViT模型从零训练，在ARC-1基准上达到了60.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管ARC任务本质上是视觉拼图，但现有研究主要将其视为语言问题并使用大语言模型（LLM）处理，缺乏从视觉中心视角的探索。

Method: 提出了Vision ARC (VARC) 框架，将ARC建模为图像到图像的翻译任务。通过将输入表示为类似自然图像的“画布”，应用标准的Vision Transformer (ViT) 架构进行映射，仅使用ARC数据从零开始训练，并辅以测试时训练（Test-time training）策略。

Result: VARC在ARC-1基准测试中取得了60.4%的准确率，显著优于其他同样从零开始训练的方法，与顶尖LLM的表现相当，并接近人类平均水平。

Conclusion: 该研究证明了将视觉先验引入ARC任务的有效性，展示了标准视觉架构在抽象推理领域的巨大潜力，为解决此类任务提供了新的视觉范式。

Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Streamlining Industrial Contract Management with Retrieval-Augmented LLMs](https://arxiv.org/abs/2511.14671)
*Kristi Topollai,Tolga Dimlioglu,Anna Choromanska,Simon Odie,Reginald Hui*

Main category: cs.CL

TL;DR: 本文提出了一种基于RAG的模块化框架，利用合成数据和奖励对齐机制，在低资源环境下自动化合同条款的审查与修正，准确率超过80%。


<details>
  <summary>Details</summary>
Motivation: 合同管理涉及繁琐的条款审查与谈判，由于缺乏标注数据和存在大量非结构化遗留合同，自动化这一过程面临巨大挑战。

Method: 提出并通过检索增强生成（RAG）管道实现的模块化框架。该系统集成了合成数据生成、语义条款检索、可接受性分类以及基于奖励的对齐机制，以标记有问题的修订并生成改进方案。

Result: 与行业合作伙伴协作评估显示，该系统在识别和优化有问题修订方面均实现了超过80%的准确率。

Conclusion: 该框架在真实世界的低资源条件下表现优异，为加速合同修订工作流提供了有效的解决方案。

Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

</details>


### [11] [Quadratic Term Correction on Heaps' Law](https://arxiv.org/abs/2511.14683)
*Oscar Fontanelli,Wentian Li*

Main category: cs.CL

TL;DR: 本文提出在双对数坐标下使用二次函数来修正Heaps定律的简单幂律模型，从而更完美地拟合词型-词例曲线的凹陷特征。


<details>
  <summary>Details</summary>
Motivation: 传统的Heaps或Herdan定律假设词型与词例之间存在幂律关系（双对数坐标下为直线），但实际观察发现该曲线在双对数坐标下呈现微弱的凹陷，表明简单的幂律关系失效。

Method: 通过分析20本英语小说或著作，在双对数坐标下采用包含一次项和二次项的回归分析对数据进行拟合；同时利用“有放回的彩色球抽取”模型探讨曲率的物理意义。

Result: 双对数坐标下的二次函数能完美拟合数据，回归结果显示一次项系数略大于1，二次项系数约为-0.02；证明了曲线的曲率等同于负的“伪方差”。

Conclusion: 相比于传统的线性幂律模型，二次近似能更准确地描述词型-词例增长关系；尽管在大数据量下存在数值不稳定，该理论模型在小数据量下能有效估计曲率。

Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.

</details>


### [12] [Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries](https://arxiv.org/abs/2511.14685)
*Kiera McCormick,Rafael Martínez-Galarza*

Main category: cs.CL

TL;DR: 本研究利用稀疏自动编码器，以天体物理学为测试平台，探讨了大语言模型嵌入是否包含以及如何编码来自科学测量的物理信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽然基于文本训练，但表现出强大的跨域泛化能力。研究者希望探究LLM是否能通过文本描述，对通常只能通过严格科学测量获得的物理信息进行编码。

Method: 以天体物理学作为测试场景，利用稀疏自动编码器（sparse autoencoders）从LLM的文本嵌入中提取可解释特征，进而分析提示词（prompting）的影响以及语言特征在物理信息编码中的作用。

Result: 研究针对两点进行了探索：1）提示词策略对LLM编排物理量的方式有何影响；2）语言的哪些方面对于编码这些物理测量最为关键。

Conclusion: 通过分析LLM嵌入，本研究旨在揭示大模型如何处理和表征科学领域（如天体物理）中的物理统计信息及其潜在机制。

Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

</details>


### [13] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


### [14] [Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances](https://arxiv.org/abs/2511.14693)
*Rishu Kumar Singh,Navneet Shreya,Sarmistha Das,Apoorva Singh,Sriparna Saha*

Main category: cs.CL

TL;DR: 本文提出了VALOR框架，通过多专家推理和语义对齐技术，有效利用多模态（文本+图像）和多轮对话信息，实现了对客户投诉方面及严重程度的精细化分类。


<details>
  <summary>Details</summary>
Motivation: 现有的投诉分析方法主要依赖单模态、短文本内容（如推文或评论），难以处理现实中涉及文本和视觉证据（如截图）的多模态、多轮次客户支持对话，也缺乏对投诉方面和严重程度的细粒度分类能力。

Method: 文章提出了VALOR（Validation-Aware Learner with Expert Routing）框架。该方法针对多模态设置，采用多专家推理机制，利用大规模生成模型结合思维链（CoT）提示进行决策。此外，计算语义对齐分数以确保模态间的一致性，并通过元融合（meta-fusion）策略将其整合进最终分类中。

Result: 在标注了细粒度方面和严重程度标签的精选多模态投诉数据集上，VALOR的表现始终优于基线模型，特别是在信息分散于文本和图像中的复杂投诉场景下，优势更为明显。

Conclusion: 这项研究强调了多模态交互和专家验证在实际投诉理解系统中的重要价值。此外，该框架符合联合国可持续发展目标（SDG 9和12），有助于推动更稳健的服务基础设施和负责任的消费生产。相关数据和代码已开源。

Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

</details>


### [15] [Subword Tokenization Strategies for Kurdish Word Embeddings](https://arxiv.org/abs/2511.14696)
*Ali Salehi,Cassandra L. Jacobs*

Main category: cs.CL

TL;DR: 本文研究了库尔德语词嵌入的分词策略，发现虽然BPE在表面得分上较高，但在考虑覆盖率偏差后，语素级分词在语义空间组织和覆盖率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 旨在探究适合库尔德语（低资源语言）词嵌入的分词策略，并解决现有评估方法中可能存在的偏差问题。

Method: 研究对比了词级、语素级和BPE三种分词方法。为此开发了一个利用少量人工标注数据通过自举训练得到的BiLSTM-CRF形态分割器，并基于相似度保存、聚类质量和语义组织等指标对Word2Vec嵌入进行了综合评估。

Result: 研究揭示了分词比较中的关键评估偏差：BPE虽在形态相似度上看似优越，但仅覆盖了28.6%的测试用例，存在人为的性能夸大；相比之下，语素模型覆盖率达68.7%。综合评估显示，语素级分词在嵌入空间组织、语义邻域结构和形态复杂性覆盖平衡方面表现更佳。

Conclusion: 本文的研究结果强调了在低资源语言处理中进行“覆盖率感知”评估的重要性，并表明语素级分词在综合表现上优于BPE，为低资源语言处理提供了有效的分词方法参考。

Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

</details>


### [16] [Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance](https://arxiv.org/abs/2511.14709)
*Raha Aghaei,Ali A. Kiaei,Mahnaz Boush,Mahan Rofoosheh,Mohammad Zavvar*

Main category: cs.CL

TL;DR: 本研究通过分析多源数据，阐述了大型语言模型如何通过自动化和增强协作来优化研发流程，从而加速创新并缩短上市时间。


<details>
  <summary>Details</summary>
Motivation: 旨在分析大型语言模型（LLMs）在变革研发（R&D）流程中的多种功能，探讨其如何重塑研究与开发的方式。

Method: 本研究通过对科学文献、专利数据库和实验数据进行了广泛的分析。

Result: 研究发现，LLMs通过自动化知识发现、促进假设生成、整合跨学科见解以及支持创新生态系统内的合作，实现了更灵活且信息更丰富的研发工作流。

Conclusion: 大型语言模型显著提高了研究过程的效率和有效性，最终加速了创新周期并缩短了突破性创意的上市时间。

Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

</details>
