{"id": "2511.14671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14671", "abs": "https://arxiv.org/abs/2511.14671", "authors": ["Kristi Topollai", "Tolga Dimlioglu", "Anna Choromanska", "Simon Odie", "Reginald Hui"], "title": "Streamlining Industrial Contract Management with Retrieval-Augmented LLMs", "comment": null, "summary": "Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRAG\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u5956\u52b1\u5bf9\u9f50\u673a\u5236\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u81ea\u52a8\u5316\u5408\u540c\u6761\u6b3e\u7684\u5ba1\u67e5\u4e0e\u4fee\u6b63\uff0c\u51c6\u786e\u7387\u8d85\u8fc780%\u3002", "motivation": "\u5408\u540c\u7ba1\u7406\u6d89\u53ca\u7e41\u7410\u7684\u6761\u6b3e\u5ba1\u67e5\u4e0e\u8c08\u5224\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u548c\u5b58\u5728\u5927\u91cf\u975e\u7ed3\u6784\u5316\u9057\u7559\u5408\u540c\uff0c\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e76\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\u5b9e\u73b0\u7684\u6a21\u5757\u5316\u6846\u67b6\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u8bed\u4e49\u6761\u6b3e\u68c0\u7d22\u3001\u53ef\u63a5\u53d7\u6027\u5206\u7c7b\u4ee5\u53ca\u57fa\u4e8e\u5956\u52b1\u7684\u5bf9\u9f50\u673a\u5236\uff0c\u4ee5\u6807\u8bb0\u6709\u95ee\u9898\u7684\u4fee\u8ba2\u5e76\u751f\u6210\u6539\u8fdb\u65b9\u6848\u3002", "result": "\u4e0e\u884c\u4e1a\u5408\u4f5c\u4f19\u4f34\u534f\u4f5c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u8bc6\u522b\u548c\u4f18\u5316\u6709\u95ee\u9898\u4fee\u8ba2\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u52a0\u901f\u5408\u540c\u4fee\u8ba2\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14683", "abs": "https://arxiv.org/abs/2511.14683", "authors": ["Oscar Fontanelli", "Wentian Li"], "title": "Quadratic Term Correction on Heaps' Law", "comment": "3 figures", "summary": "Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement\" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance\" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u4f7f\u7528\u4e8c\u6b21\u51fd\u6570\u6765\u4fee\u6b63Heaps\u5b9a\u5f8b\u7684\u7b80\u5355\u5e42\u5f8b\u6a21\u578b\uff0c\u4ece\u800c\u66f4\u5b8c\u7f8e\u5730\u62df\u5408\u8bcd\u578b-\u8bcd\u4f8b\u66f2\u7ebf\u7684\u51f9\u9677\u7279\u5f81\u3002", "motivation": "\u4f20\u7edf\u7684Heaps\u6216Herdan\u5b9a\u5f8b\u5047\u8bbe\u8bcd\u578b\u4e0e\u8bcd\u4f8b\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff08\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u4e3a\u76f4\u7ebf\uff09\uff0c\u4f46\u5b9e\u9645\u89c2\u5bdf\u53d1\u73b0\u8be5\u66f2\u7ebf\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u5448\u73b0\u5fae\u5f31\u7684\u51f9\u9677\uff0c\u8868\u660e\u7b80\u5355\u7684\u5e42\u5f8b\u5173\u7cfb\u5931\u6548\u3002", "method": "\u901a\u8fc7\u5206\u679020\u672c\u82f1\u8bed\u5c0f\u8bf4\u6216\u8457\u4f5c\uff0c\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u91c7\u7528\u5305\u542b\u4e00\u6b21\u9879\u548c\u4e8c\u6b21\u9879\u7684\u56de\u5f52\u5206\u6790\u5bf9\u6570\u636e\u8fdb\u884c\u62df\u5408\uff1b\u540c\u65f6\u5229\u7528\u201c\u6709\u653e\u56de\u7684\u5f69\u8272\u7403\u62bd\u53d6\u201d\u6a21\u578b\u63a2\u8ba8\u66f2\u7387\u7684\u7269\u7406\u610f\u4e49\u3002", "result": "\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u7684\u4e8c\u6b21\u51fd\u6570\u80fd\u5b8c\u7f8e\u62df\u5408\u6570\u636e\uff0c\u56de\u5f52\u7ed3\u679c\u663e\u793a\u4e00\u6b21\u9879\u7cfb\u6570\u7565\u5927\u4e8e1\uff0c\u4e8c\u6b21\u9879\u7cfb\u6570\u7ea6\u4e3a-0.02\uff1b\u8bc1\u660e\u4e86\u66f2\u7ebf\u7684\u66f2\u7387\u7b49\u540c\u4e8e\u8d1f\u7684\u201c\u4f2a\u65b9\u5dee\u201d\u3002", "conclusion": "\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u7ebf\u6027\u5e42\u5f8b\u6a21\u578b\uff0c\u4e8c\u6b21\u8fd1\u4f3c\u80fd\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u8bcd\u578b-\u8bcd\u4f8b\u589e\u957f\u5173\u7cfb\uff1b\u5c3d\u7ba1\u5728\u5927\u6570\u636e\u91cf\u4e0b\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u8be5\u7406\u8bba\u6a21\u578b\u5728\u5c0f\u6570\u636e\u91cf\u4e0b\u80fd\u6709\u6548\u4f30\u8ba1\u66f2\u7387\u3002"}}
{"id": "2511.14685", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.14685", "abs": "https://arxiv.org/abs/2511.14685", "authors": ["Kiera McCormick", "Rafael Mart\u00ednez-Galarza"], "title": "Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries", "comment": "Accepted to the Machine Learning and the Physical Sciences Workshop at NeurIPS 2025, 11 pages, 4 figures", "summary": "Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u4ee5\u5929\u4f53\u7269\u7406\u5b66\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u662f\u5426\u5305\u542b\u4ee5\u53ca\u5982\u4f55\u7f16\u7801\u6765\u81ea\u79d1\u5b66\u6d4b\u91cf\u7684\u7269\u7406\u4fe1\u606f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u57fa\u4e8e\u6587\u672c\u8bad\u7ec3\uff0c\u4f46\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76LLM\u662f\u5426\u80fd\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\uff0c\u5bf9\u901a\u5e38\u53ea\u80fd\u901a\u8fc7\u4e25\u683c\u79d1\u5b66\u6d4b\u91cf\u83b7\u5f97\u7684\u7269\u7406\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002", "method": "\u4ee5\u5929\u4f53\u7269\u7406\u5b66\u4f5c\u4e3a\u6d4b\u8bd5\u573a\u666f\uff0c\u5229\u7528\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\uff08sparse autoencoders\uff09\u4eceLLM\u7684\u6587\u672c\u5d4c\u5165\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u8fdb\u800c\u5206\u6790\u63d0\u793a\u8bcd\uff08prompting\uff09\u7684\u5f71\u54cd\u4ee5\u53ca\u8bed\u8a00\u7279\u5f81\u5728\u7269\u7406\u4fe1\u606f\u7f16\u7801\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u9488\u5bf9\u4e24\u70b9\u8fdb\u884c\u4e86\u63a2\u7d22\uff1a1\uff09\u63d0\u793a\u8bcd\u7b56\u7565\u5bf9LLM\u7f16\u6392\u7269\u7406\u91cf\u7684\u65b9\u5f0f\u6709\u4f55\u5f71\u54cd\uff1b2\uff09\u8bed\u8a00\u7684\u54ea\u4e9b\u65b9\u9762\u5bf9\u4e8e\u7f16\u7801\u8fd9\u4e9b\u7269\u7406\u6d4b\u91cf\u6700\u4e3a\u5173\u952e\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790LLM\u5d4c\u5165\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5927\u6a21\u578b\u5982\u4f55\u5904\u7406\u548c\u8868\u5f81\u79d1\u5b66\u9886\u57df\uff08\u5982\u5929\u4f53\u7269\u7406\uff09\u4e2d\u7684\u7269\u7406\u7edf\u8ba1\u4fe1\u606f\u53ca\u5176\u6f5c\u5728\u673a\u5236\u3002"}}
{"id": "2511.14688", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14688", "abs": "https://arxiv.org/abs/2511.14688", "authors": ["Clovis Gladstone", "Zhao Fang", "Spencer Dean Stewart"], "title": "Ground Truth Generation for Multilingual Historical NLP using LLMs", "comment": "13 pages, 5 tables, 1 figure", "summary": "Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14693", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14693", "abs": "https://arxiv.org/abs/2511.14693", "authors": ["Rishu Kumar Singh", "Navneet Shreya", "Sarmistha Das", "Apoorva Singh", "Sriparna Saha"], "title": "Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances", "comment": "To be published in the Proceedings of the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026 Special Track on AI for Social Impact )", "summary": "Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VALOR\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u63a8\u7406\u548c\u8bed\u4e49\u5bf9\u9f50\u6280\u672f\uff0c\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\uff08\u6587\u672c+\u56fe\u50cf\uff09\u548c\u591a\u8f6e\u5bf9\u8bdd\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5ba2\u6237\u6295\u8bc9\u65b9\u9762\u53ca\u4e25\u91cd\u7a0b\u5ea6\u7684\u7cbe\u7ec6\u5316\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u7684\u6295\u8bc9\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u3001\u77ed\u6587\u672c\u5185\u5bb9\uff08\u5982\u63a8\u6587\u6216\u8bc4\u8bba\uff09\uff0c\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u4e2d\u6d89\u53ca\u6587\u672c\u548c\u89c6\u89c9\u8bc1\u636e\uff08\u5982\u622a\u56fe\uff09\u7684\u591a\u6a21\u6001\u3001\u591a\u8f6e\u6b21\u5ba2\u6237\u652f\u6301\u5bf9\u8bdd\uff0c\u4e5f\u7f3a\u4e4f\u5bf9\u6295\u8bc9\u65b9\u9762\u548c\u4e25\u91cd\u7a0b\u5ea6\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u80fd\u529b\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86VALOR\uff08Validation-Aware Learner with Expert Routing\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u9488\u5bf9\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u91c7\u7528\u591a\u4e13\u5bb6\u63a8\u7406\u673a\u5236\uff0c\u5229\u7528\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u8fdb\u884c\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u8ba1\u7b97\u8bed\u4e49\u5bf9\u9f50\u5206\u6570\u4ee5\u786e\u4fdd\u6a21\u6001\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u5143\u878d\u5408\uff08meta-fusion\uff09\u7b56\u7565\u5c06\u5176\u6574\u5408\u8fdb\u6700\u7ec8\u5206\u7c7b\u4e2d\u3002", "result": "\u5728\u6807\u6ce8\u4e86\u7ec6\u7c92\u5ea6\u65b9\u9762\u548c\u4e25\u91cd\u7a0b\u5ea6\u6807\u7b7e\u7684\u7cbe\u9009\u591a\u6a21\u6001\u6295\u8bc9\u6570\u636e\u96c6\u4e0a\uff0cVALOR\u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u5206\u6563\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u7684\u590d\u6742\u6295\u8bc9\u573a\u666f\u4e0b\uff0c\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u4e13\u5bb6\u9a8c\u8bc1\u5728\u5b9e\u9645\u6295\u8bc9\u7406\u89e3\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u4ef7\u503c\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u7b26\u5408\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff08SDG 9\u548c12\uff09\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u7a33\u5065\u7684\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u548c\u8d1f\u8d23\u4efb\u7684\u6d88\u8d39\u751f\u4ea7\u3002\u76f8\u5173\u6570\u636e\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.14696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14696", "abs": "https://arxiv.org/abs/2511.14696", "authors": ["Ali Salehi", "Cassandra L. Jacobs"], "title": "Subword Tokenization Strategies for Kurdish Word Embeddings", "comment": null, "summary": "We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\\% of test cases compared to 68.7\\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e93\u5c14\u5fb7\u8bed\u8bcd\u5d4c\u5165\u7684\u5206\u8bcd\u7b56\u7565\uff0c\u53d1\u73b0\u867d\u7136BPE\u5728\u8868\u9762\u5f97\u5206\u4e0a\u8f83\u9ad8\uff0c\u4f46\u5728\u8003\u8651\u8986\u76d6\u7387\u504f\u5dee\u540e\uff0c\u8bed\u7d20\u7ea7\u5206\u8bcd\u5728\u8bed\u4e49\u7a7a\u95f4\u7ec4\u7ec7\u548c\u8986\u76d6\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u65e8\u5728\u63a2\u7a76\u9002\u5408\u5e93\u5c14\u5fb7\u8bed\uff08\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u8bcd\u5d4c\u5165\u7684\u5206\u8bcd\u7b56\u7565\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u504f\u5dee\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u8bcd\u7ea7\u3001\u8bed\u7d20\u7ea7\u548cBPE\u4e09\u79cd\u5206\u8bcd\u65b9\u6cd5\u3002\u4e3a\u6b64\u5f00\u53d1\u4e86\u4e00\u4e2a\u5229\u7528\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u901a\u8fc7\u81ea\u4e3e\u8bad\u7ec3\u5f97\u5230\u7684BiLSTM-CRF\u5f62\u6001\u5206\u5272\u5668\uff0c\u5e76\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u4fdd\u5b58\u3001\u805a\u7c7b\u8d28\u91cf\u548c\u8bed\u4e49\u7ec4\u7ec7\u7b49\u6307\u6807\u5bf9Word2Vec\u5d4c\u5165\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5206\u8bcd\u6bd4\u8f83\u4e2d\u7684\u5173\u952e\u8bc4\u4f30\u504f\u5dee\uff1aBPE\u867d\u5728\u5f62\u6001\u76f8\u4f3c\u5ea6\u4e0a\u770b\u4f3c\u4f18\u8d8a\uff0c\u4f46\u4ec5\u8986\u76d6\u4e8628.6%\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5b58\u5728\u4eba\u4e3a\u7684\u6027\u80fd\u5938\u5927\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u8bed\u7d20\u6a21\u578b\u8986\u76d6\u7387\u8fbe68.7%\u3002\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u8bed\u7d20\u7ea7\u5206\u8bcd\u5728\u5d4c\u5165\u7a7a\u95f4\u7ec4\u7ec7\u3001\u8bed\u4e49\u90bb\u57df\u7ed3\u6784\u548c\u5f62\u6001\u590d\u6742\u6027\u8986\u76d6\u5e73\u8861\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e2d\u8fdb\u884c\u201c\u8986\u76d6\u7387\u611f\u77e5\u201d\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u8bed\u7d20\u7ea7\u5206\u8bcd\u5728\u7efc\u5408\u8868\u73b0\u4e0a\u4f18\u4e8eBPE\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u8bcd\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2511.14709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14709", "abs": "https://arxiv.org/abs/2511.14709", "authors": ["Raha Aghaei", "Ali A. Kiaei", "Mahnaz Boush", "Mahan Rofoosheh", "Mohammad Zavvar"], "title": "Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance", "comment": null, "summary": "This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u591a\u6e90\u6570\u636e\uff0c\u9610\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u589e\u5f3a\u534f\u4f5c\u6765\u4f18\u5316\u7814\u53d1\u6d41\u7a0b\uff0c\u4ece\u800c\u52a0\u901f\u521b\u65b0\u5e76\u7f29\u77ed\u4e0a\u5e02\u65f6\u95f4\u3002", "motivation": "\u65e8\u5728\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53d8\u9769\u7814\u53d1\uff08R&D\uff09\u6d41\u7a0b\u4e2d\u7684\u591a\u79cd\u529f\u80fd\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u91cd\u5851\u7814\u7a76\u4e0e\u5f00\u53d1\u7684\u65b9\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u79d1\u5b66\u6587\u732e\u3001\u4e13\u5229\u6570\u636e\u5e93\u548c\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u901a\u8fc7\u81ea\u52a8\u5316\u77e5\u8bc6\u53d1\u73b0\u3001\u4fc3\u8fdb\u5047\u8bbe\u751f\u6210\u3001\u6574\u5408\u8de8\u5b66\u79d1\u89c1\u89e3\u4ee5\u53ca\u652f\u6301\u521b\u65b0\u751f\u6001\u7cfb\u7edf\u5185\u7684\u5408\u4f5c\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u4e14\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u7814\u53d1\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u7814\u7a76\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u6700\u7ec8\u52a0\u901f\u4e86\u521b\u65b0\u5468\u671f\u5e76\u7f29\u77ed\u4e86\u7a81\u7834\u6027\u521b\u610f\u7684\u4e0a\u5e02\u65f6\u95f4\u3002"}}
{"id": "2511.14689", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14689", "abs": "https://arxiv.org/abs/2511.14689", "authors": ["Shiyar Jamo"], "title": "Impact of Image Resolution on Age Estimation with DeepFace and InsightFace", "comment": "6 pages, 7 figures, 7 tables. Evaluation of DeepFace and InsightFace age estimation across seven image resolutions (64 to 1080 px)", "summary": "Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u56fe\u50cf\u5206\u8fa8\u7387\u5bf9DeepFace\u548cInsightFace\u8fdb\u884c\u5e74\u9f84\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0224x224\u50cf\u7d20\u4e3a\u6700\u4f73\u5206\u8fa8\u7387\uff0c\u8fc7\u4f4e\u6216\u8fc7\u9ad8\u5206\u8fa8\u7387\u5747\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u4e14InsightFace\u6574\u4f53\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u81ea\u52a8\u5e74\u9f84\u4f30\u8ba1\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5e74\u9f84\u9a8c\u8bc1\u573a\u666f\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8f93\u5165\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u5f80\u5f80\u5b58\u5728\u5f88\u5927\u5dee\u5f02\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u4f7f\u7528DeepFace\u548cInsightFace\u6a21\u578b\u8fdb\u884c\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86IMDB-Clean\u6570\u636e\u96c6\u4e2d\u76841000\u5f20\u56fe\u50cf\uff0c\u5c06\u5176\u5904\u7406\u4e3a7\u79cd\u4e0d\u540c\u7684\u5206\u8fa8\u7387\uff0c\u751f\u62107000\u4e2a\u6d4b\u8bd5\u6837\u672c\u3002\u4f7f\u7528DeepFace\u548cInsightFace\u4e24\u4e2a\u6846\u67b6\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u3001\u6807\u51c6\u5dee\uff08SD\uff09\u548c\u4e2d\u4f4d\u6570\u7edd\u5bf9\u8bef\u5dee\uff08MedAE\uff09\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0224x224\u50cf\u7d20\u662f\u4e24\u4e2a\u6846\u67b6\u7684\u6700\u4f73\u5206\u8fa8\u7387\uff0c\u5176\u4e2dInsightFace\u7684MAE\u4e3a7.46\u5e74\uff0cDeepFace\u4e3a10.83\u5e74\u3002\u4f4e\u5206\u8fa8\u7387\u4f1a\u5bfc\u81f4\u8bef\u5dee\u5927\u5e45\u589e\u52a0\uff0c\u800c\u8fc7\u9ad8\u7684\u5206\u8fa8\u7387\u4e5f\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0cInsightFace\u5728\u6240\u6709\u5206\u8fa8\u7387\u4e0b\u7684\u8fd0\u884c\u901f\u5ea6\u5747\u5feb\u4e8eDeepFace\u3002", "conclusion": "\u8f93\u5165\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u5bf9\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u5177\u6709\u6e05\u6670\u4e14\u4e00\u81f4\u7684\u5f71\u54cd\u3002DeepFace\u548cInsightFace\u5728224x224\u50cf\u7d20\u5206\u8fa8\u7387\u4e0b\u5747\u80fd\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u4e14InsightFace\u5728\u901f\u5ea6\u548c\u51c6\u786e\u5ea6\u4e0a\u603b\u4f53\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2511.14702", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14702", "abs": "https://arxiv.org/abs/2511.14702", "authors": ["Farheen Ramzan", "Yusuf Kiberu", "Nikesh Jathanna", "Meryem Jabrane", "Vicente Grau", "Shahnaz Jamil-Copley", "Richard H. Clayton", "Chen", "Chen"], "title": "Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images", "comment": null, "summary": "Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to \"see beyond the image\", setting a new direction for robust and physiologically grounded cardiac scar segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ECG\u751f\u7406\u4fe1\u606f\u548cMRI\u89e3\u5256\u4fe1\u606f\u7684\u591a\u6a21\u6001\u5fc3\u808c\u7622\u75d5\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u5e8f\u611f\u77e5\u7279\u5f81\u878d\u5408\u673a\u5236\u89e3\u51b3\u975e\u540c\u6b65\u91c7\u96c6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "LGE-MRI\u56fe\u50cf\u4e2d\u7684\u5fc3\u808c\u7622\u75d5\u5206\u5272\u53d7\u9650\u4e8e\u5bf9\u6bd4\u5ea6\u53d8\u5316\u548c\u6210\u50cf\u4f2a\u5f71\u3002\u867d\u7136\u5fc3\u7535\u56fe\uff08ECG\uff09\u80fd\u63d0\u4f9b\u5173\u4e8e\u7622\u75d5\u5b9a\u4f4d\u7684\u4e92\u8865\u751f\u7406\u4fe1\u606f\uff0c\u4f46\u5982\u4f55\u6709\u6548\u878d\u5408\u975e\u540c\u6b65\u91c7\u96c6\u7684ECG\u4e0eMRI\u6570\u636e\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06ECG\u884d\u751f\u7684\u7535\u751f\u7406\u4fe1\u606f\u4e0eAHA-17\u56fe\u8c31\u7684\u89e3\u5256\u5148\u9a8c\u76f8\u7ed3\u5408\u3002\u9488\u5bf9ECG\u4e0eMRI\u975e\u540c\u6b65\u91c7\u96c6\u7684\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u65f6\u5e8f\u611f\u77e5\u7279\u5f81\u878d\u5408\uff08TAFF\uff09\u673a\u5236\uff0c\u6839\u636e\u91c7\u96c6\u65f6\u95f4\u5dee\u52a8\u6001\u52a0\u6743\u5e76\u878d\u5408\u7279\u5f81\u3002", "result": "\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u56fe\u50cf\u7684SOTA\u57fa\u7ebf\uff08nnU-Net\uff09\u3002\u7622\u75d5\u5206\u5272\u7684\u5e73\u5747Dice\u8bc4\u5206\u4ece0.6149\u63d0\u5347\u81f30.8463\uff0c\u7cbe\u786e\u5ea6\u8fbe\u52300.9115\uff0c\u7075\u654f\u5ea6\u8fbe\u52300.9043\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u878d\u5408\u751f\u7406\uff08ECG\uff09\u548c\u89e3\u5256\uff08AHA-17\uff09\u77e5\u8bc6\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u56fe\u50cf\u4e4b\u5916\u7684\u4fe1\u606f\uff0c\u4e3a\u5b9e\u73b0\u66f4\u9c81\u68d2\u4e14\u5177\u6709\u751f\u7406\u4f9d\u636e\u7684\u5fc3\u810f\u7622\u75d5\u5206\u5272\u786e\u7acb\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.14712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14712", "abs": "https://arxiv.org/abs/2511.14712", "authors": ["Yunfeng Wu", "Jiayi Song", "Zhenxiong Tan", "Zihao He", "Songhua Liu"], "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation", "comment": "13 pages, 8 figures", "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aFreeSwim\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u53cc\u8def\u5f84\u4ea4\u53c9\u6ce8\u610f\u529b\u8986\u76d6\u7b56\u7565\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563Transformer\u751f\u6210\u8fde\u8d2f\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8eTransformer\u7684\u89c6\u9891\u751f\u6210\u5668\u4e2d\uff0c\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u65b9\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u4f7f\u5f97\u9488\u5bf9\u8d85\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6210\u672c\u6781\u9ad8\u4e14\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "1. **\u5185\u5411\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236**\uff1a\u57fa\u4e8e\u4fdd\u6301\u67e5\u8be2token\u8bad\u7ec3\u5c3a\u5ea6\u611f\u53d7\u91ce\u7684\u89c2\u5bdf\uff0c\u7528\u4e8e\u4fdd\u7559\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u3002\n2. **\u53cc\u8def\u5f84\u7ba1\u9053\u4e0e\u4ea4\u53c9\u6ce8\u610f\u529b\u8986\u76d6\uff08Cross-Attention Override\uff09**\uff1a\u4e3a\u4e86\u89e3\u51b3\u5c40\u90e8\u7a97\u53e3\u6ce8\u610f\u529b\u5bfc\u81f4\u7684\u91cd\u590d\u5185\u5bb9\u548c\u5168\u5c40\u8fde\u8d2f\u6027\u7f3a\u5931\u95ee\u9898\uff0c\u5f15\u5165\u5168\u611f\u53d7\u91ce\u5206\u652f\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6307\u5bfc\u5c40\u90e8\u6ce8\u610f\u529b\u5206\u652f\u3002\n3. **\u4ea4\u53c9\u6ce8\u610f\u529b\u7f13\u5b58\u7b56\u7565**\uff1a\u4e3a\u5168\u611f\u53d7\u91ce\u5206\u652f\u5f15\u5165\u7f13\u5b58\uff0c\u907f\u514d\u9891\u7e41\u8ba1\u7b97\u51683D\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\uff0c\u4e14\u6548\u7387\u5f88\u9ad8\u3002\u5728VBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u57fa\u4e8e\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u65f6\u5177\u5907\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u9ad8\u7684\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\u548c\u7f13\u5b58\u7b56\u7565\uff0c\u53ef\u4ee5\u5229\u7528\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563Transformer\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u8fde\u8d2f\u6027\u7684\u540c\u65f6\u751f\u6210\u5177\u6709\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002"}}
{"id": "2511.14719", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14719", "abs": "https://arxiv.org/abs/2511.14719", "authors": ["Yifan Wang", "Liya Ji", "Zhanghan Ke", "Harry Yang", "Ser-Nam Lim", "Qifeng Chen"], "title": "Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising", "comment": "Project Page: https://wyf0824.github.io/Video_Realism_Enhancement/", "summary": "We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u5408\u6210\u89c6\u9891\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u8fb9\u7f18\u7b49\u7ed3\u6784\u4fe1\u606f\u5f15\u5bfc\u751f\u6210\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u548c\u7ed3\u6784\u4e00\u81f4\u7684\u771f\u5b9e\u611f\u6e32\u67d3\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u5408\u6210\u89c6\u9891\uff08\u5982\u4ece\u6a21\u62df\u5668\u751f\u6210\u7684\u89c6\u9891\uff09\u7684\u771f\u5b9e\u611f\uff0c\u4f7f\u5176\u8fbe\u5230\u7167\u7247\u7ea7\u6548\u679c\uff0c\u540c\u65f6\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u4e0a\u4fdd\u7559\u539f\u59cb\u89c6\u9891\u7684\u591a\u5c42\u6b21\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\uff08Zero-shot\uff09\u6846\u67b6\uff0c\u5efa\u7acb\u5728\u65e0\u9700\u5fae\u8c03\u7684\u89c6\u9891\u6269\u6563\u57fa\u7840\u6a21\u578b\u4e4b\u4e0a\u3002\u5177\u4f53\u901a\u8fc7\u8f85\u52a9\u6a21\u578b\u4f30\u8ba1\u5408\u6210\u89c6\u9891\u7684\u6df1\u5ea6\u56fe\u3001\u8bed\u4e49\u56fe\u548c\u8fb9\u7f18\u56fe\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7ed3\u6784\u611f\u77e5\u4fe1\u606f\u4f5c\u4e3a\u6761\u4ef6\u5f15\u5165\u751f\u6210/\u53bb\u566a\u8fc7\u7a0b\uff0c\u4ee5\u6307\u5bfc\u89c6\u9891\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5728\u4e0e\u539f\u59cb\u89c6\u9891\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u4e14\u5f3a\u5927\u7684\u5408\u6210\u89c6\u9891\u771f\u5b9e\u611f\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u786e\u4fdd\u589e\u5f3a\u540e\u7684\u89c6\u9891\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u6c34\u5e73\u4e0a\u4e0e\u539f\u59cb\u5408\u6210\u89c6\u9891\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2511.14742", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.14742", "abs": "https://arxiv.org/abs/2511.14742", "authors": ["Stefan Cobeli", "Kazi Shahrukh Omar", "Rodrigo Valen\u00e7a", "Nivan Ferreira", "Fabio Miranda"], "title": "A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments", "comment": "Accepted at IEEE Transactions on Visualization and Computer Graphics. Code and data are publicly available at https://urbantk.org/neural-3d", "summary": "Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u76843D\u57ce\u5e02\u73af\u5883\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u76f4\u63a5\u548c\u53cd\u5411\u67e5\u8be2\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a213D\u6570\u636e\u63a2\u7d22\u4e2d\u5e38\u89c1\u7684\u906e\u6321\u548c\u89c6\u89d2\u8c03\u6574\u96be\u9898\u3002", "motivation": "\u5c3d\u7ba13D\u57ce\u5e02\u6570\u636e\u96c6\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u74f6\u9888\u548c\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff08\u7279\u522b\u662f\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u5bfc\u81f4\u7684\u906e\u6321\u548c\u7e41\u7410\u7684\u624b\u52a8\u89c6\u89d2\u8c03\u6574\uff09\uff0c\u4ece\u4e2d\u9ad8\u6548\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u4ecd\u7136\u975e\u5e38\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89d2\u7684\u63a2\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u573a\uff08Neural Field\uff09\u6784\u5efa3D\u73af\u5883\u7684\u9ad8\u6548\u9690\u5f0f\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5c06\u89c6\u89d2\u7f16\u7801\u4e3a\u5411\u91cf\u573a\uff0c\u652f\u6301\u5feb\u901f\u7684\u76f4\u63a5\u67e5\u8be2\uff08\u8ba1\u7b97\u89c6\u89d2\u8bc4\u4f30\u6307\u6807\uff09\u548c\u53cd\u5411\u67e5\u8be2\uff08\u641c\u7d22\u5339\u914d\u7279\u5b9a\u6a21\u5f0f\u4e14\u907f\u514d\u906e\u6321\u7684\u89c6\u89d2\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u652f\u6301\u4e86\u53ef\u89c1\u6027\u5206\u6790\u3001\u65e5\u7167\u8bc4\u4f30\u548c\u65b0\u5f00\u53d1\u9879\u76ee\u89c6\u89c9\u5f71\u54cd\u8bc4\u4f30\u7b49\u4efb\u52a1\u3002\u901a\u8fc7\u5b9a\u91cf\u5b9e\u9a8c\u3001\u5b9e\u9645\u6848\u4f8b\u548c\u4e13\u5bb6\u53cd\u9988\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5176\u5728\u5bfb\u627e\u7406\u60f3\u89c6\u89d2\u3001\u5206\u6790\u5efa\u7b51\u7acb\u9762\u53ef\u89c1\u6027\u53ca\u8bc4\u4f30\u5ba4\u5916\u7a7a\u95f4\u89c6\u91ce\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u795e\u7ecf\u573a\u8fdb\u884c\u9690\u5f0f\u8868\u793a\u80fd\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a213D\u57ce\u5e02\u73af\u5883\u63a2\u7d22\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u4ea4\u4e92\u56f0\u96be\u95ee\u9898\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5de5\u5177\uff0c\u4e14\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.14749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14749", "abs": "https://arxiv.org/abs/2511.14749", "authors": ["Alexander Vedernikov", "Puneet Kumar", "Haoyu Chen", "Tapio Sepp\u00e4nen", "Xiaobai Li"], "title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis", "comment": null, "summary": "Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8f85\u52a9\u7ec6\u5316\u566a\u58f0\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u8f6f\u6807\u7b7e\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6295\u5165\u5ea6\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u7684\u6295\u5165\u5ea6\u8bc6\u522b\uff08Engagement Recognition\uff09\u4efb\u52a1\u4e3b\u8981\u9762\u4e34\u6807\u7b7e\u4e3b\u89c2\u6027\u5f3a\u548c\u566a\u58f0\u5e72\u6270\u7684\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002", "method": "1. \u5229\u7528\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5e76\u901a\u8fc7\u95ee\u5377\u5f62\u5f0f\u63d0\u53d6\u884c\u4e3a\u7ebf\u7d22\uff0c\u5c06\u6570\u636e\u5212\u5206\u4e3a\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u53ef\u9760\u6027\u5b50\u96c6\u4ee5\u4f18\u5316\u6807\u6ce8\u3002\n2. \u5f15\u5165\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\uff08Curriculum Learning\uff09\u4e0e\u8f6f\u6807\u7b7e\u7ec6\u5316\uff08Soft Label Refinement\uff09\u7684\u8bad\u7ec3\u7b56\u7565\u3002\n3. \u9010\u6b65\u5c06\u6a21\u7cca\u6837\u672c\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u540c\u65f6\u8c03\u6574\u76d1\u7763\u4fe1\u53f7\u4ee5\u53cd\u6620\u4e0d\u786e\u5b9a\u6027\u3002", "result": "1. \u5728EngageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c6\u4e2a\u7279\u5f81\u8bbe\u7f6e\u4e2d\u67093\u4e2a\u8d85\u8d8a\u4e86SOTA\uff0c\u6700\u5927\u63d0\u5347\u5e45\u5ea6\u4e3a1.21%\u3002\n2. \u5728DREAMS\u548cPAFE\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u5347\u4e860.22\u548c0.06\u3002\n3. \u8bc1\u660e\u4e86\u5728\u7ecfVLM\u7ec6\u5316\u7684\u9ad8\u53ef\u9760\u6027\u5b50\u96c6\u4e0a\u8bad\u7ec3\u5e76\u7ed3\u5408\u8bfe\u7a0b\u7b56\u7565\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\uff0c\u5229\u7528VLM\u89e3\u51b3\u6807\u7b7e\u4e3b\u89c2\u6027\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u7ecf\u5178\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u6295\u5165\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2511.14751", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14751", "abs": "https://arxiv.org/abs/2511.14751", "authors": ["Yutian Chen", "Yuheng Qiu", "Ruogu Li", "Ali Agha", "Shayegan Omidshafiei", "Jay Patrikar", "Sebastian Scherer"], "title": "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers", "comment": null, "summary": "We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\\times$ and $7.2\\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.", "AI": {"tldr": "Co-Me\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u51e0\u4f55Transformer\u7684\u514d\u91cd\u8bad\u7ec3\u52a0\u901f\u673a\u5236\uff0c\u901a\u8fc7\u5408\u5e76\u4f4e\u7f6e\u4fe1\u5ea6Token\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad811.3\u500d\u7684\u52a0\u901f\u5e76\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u51e0\u4f55Transformer\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f63D\u611f\u77e5\u548c\u91cd\u5efa\u7684\u9700\u6c42\uff1b\u73b0\u6709\u7684\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u5408\u5e76\u6216\u526a\u679d\u7b56\u7565\u6548\u7387\u6216\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684Token\u5408\u5e76\uff08Co-Me\uff09\u3002\u8be5\u65b9\u6cd5\u84b8\u998f\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u5bf9Token\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u9009\u62e9\u6027\u5730\u5408\u5e76\u4f4e\u7f6e\u4fe1\u5ea6\u7684Token\uff0c\u4e14\u65e0\u9700\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u91cd\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "result": "Co-Me\u9002\u7528\u4e8e\u591a\u79cd\u591a\u89c6\u56fe\u548c\u6d41\u5f0f\u89c6\u89c9\u51e0\u4f55Transformer\u3002\u5728VGGT\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe11.3\u500d\u7684\u52a0\u901f\uff0c\u5728MapAnything\u4e0a\u5b9e\u73b0\u4e867.2\u500d\u7684\u52a0\u901f\u3002", "conclusion": "Co-Me\u5229\u7528\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7a7a\u95f4\u8986\u76d6\u548c\u6027\u80fd\uff0c\u4f7f\u5f97\u89c6\u89c9\u51e0\u4f55Transformer\u80fd\u591f\u5b9e\u9645\u5e94\u7528\u4e8e\u5b9e\u65f63D\u611f\u77e5\u4efb\u52a1\u3002"}}
{"id": "2511.14760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14760", "abs": "https://arxiv.org/abs/2511.14760", "authors": ["Rui Tian", "Mingfei Gao", "Haiming Gang", "Jiasen Lu", "Zhe Gan", "Yinfei Yang", "Zuxuan Wu", "Afshin Dehghan"], "title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning", "comment": null, "summary": "We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.", "AI": {"tldr": "UniGen-1.5 \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u6307\u4ee4\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7406\u89e3\u3001\u751f\u6210\u53ca\u7f16\u8f91\u80fd\u529b\uff0c\u6027\u80fd\u5ab2\u7f8e GPT-Image-1\u3002", "motivation": "\u65e8\u5728\u57fa\u4e8e UniGen \u8fdb\u4e00\u6b65\u589e\u5f3a\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u89e3\u9501\u5e76\u5f3a\u5316\u56fe\u50cf\u7f16\u8f91\u529f\u80fd\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u7edf\u4e00\u5904\u7406\u4e0a\u7684\u5c40\u9650\u3002", "method": "1. \u5168\u9762\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\uff1b\n2. \u63d0\u51fa\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u5171\u4eab\u5956\u52b1\u6a21\u578b\u8054\u5408\u4f18\u5316\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\uff1b\n3. \u5f15\u5165\u8f7b\u91cf\u7ea7\u7f16\u8f91\u6307\u4ee4\u5bf9\u9f50\u9636\u6bb5\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u7f16\u8f91\u6307\u4ee4\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "UniGen-1.5 \u5728 GenEval (0.89) \u548c ImgEdit (4.31) \u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\uff0c\u8d85\u8d8a\u4e86 BAGEL \u7b49\u73b0\u6709 SOTA \u6a21\u578b\uff0c\u6027\u80fd\u53ef\u4e0e GPT-Image-1 \u7b49\u4e13\u6709\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "UniGen-1.5 \u901a\u8fc7\u67b6\u6784\u4f18\u5316\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u5907\u9ad8\u6027\u80fd\u56fe\u50cf\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\u7684\u7edf\u4e00\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002"}}
{"id": "2511.14761", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14761", "abs": "https://arxiv.org/abs/2511.14761", "authors": ["Keya Hu", "Ali Cy", "Linlu Qiu", "Xiaoman Delores Ding", "Runqian Wang", "Yeyin Eva Zhu", "Jacob Andreas", "Kaiming He"], "title": "ARC Is a Vision Problem!", "comment": "Technical Report. Project webpage: https://github.com/lillian039/VARC", "summary": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.", "AI": {"tldr": "VARC\u6846\u67b6\u5c06ARC\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u95ee\u9898\uff0c\u5229\u7528ViT\u6a21\u578b\u4ece\u96f6\u8bad\u7ec3\uff0c\u5728ARC-1\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e8660.4%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5c3d\u7ba1ARC\u4efb\u52a1\u672c\u8d28\u4e0a\u662f\u89c6\u89c9\u62fc\u56fe\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c06\u5176\u89c6\u4e3a\u8bed\u8a00\u95ee\u9898\u5e76\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5904\u7406\uff0c\u7f3a\u4e4f\u4ece\u89c6\u89c9\u4e2d\u5fc3\u89c6\u89d2\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86Vision ARC (VARC) \u6846\u67b6\uff0c\u5c06ARC\u5efa\u6a21\u4e3a\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u7ffb\u8bd1\u4efb\u52a1\u3002\u901a\u8fc7\u5c06\u8f93\u5165\u8868\u793a\u4e3a\u7c7b\u4f3c\u81ea\u7136\u56fe\u50cf\u7684\u201c\u753b\u5e03\u201d\uff0c\u5e94\u7528\u6807\u51c6\u7684Vision Transformer (ViT) \u67b6\u6784\u8fdb\u884c\u6620\u5c04\uff0c\u4ec5\u4f7f\u7528ARC\u6570\u636e\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\uff0c\u5e76\u8f85\u4ee5\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08Test-time training\uff09\u7b56\u7565\u3002", "result": "VARC\u5728ARC-1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8660.4%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u540c\u6837\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4e0e\u9876\u5c16LLM\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u5e76\u63a5\u8fd1\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u89c6\u89c9\u5148\u9a8c\u5f15\u5165ARC\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6807\u51c6\u89c6\u89c9\u67b6\u6784\u5728\u62bd\u8c61\u63a8\u7406\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u89e3\u51b3\u6b64\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89c9\u8303\u5f0f\u3002"}}
