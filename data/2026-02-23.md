<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 35]
- [cs.CL](#cs.CL) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文针对视频描述任务中细粒度运动细节描述不准确和幻觉问题，提出了KPM-Bench数据集和MoPE算法，通过运动解析和提取技术改进运动中心视频描述的质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频描述模型在描述细粒度运动细节时存在严重不足，特别是在运动中心视频中，对复杂动作和肢体动态的精确描述常常被忽视，同时存在严重的幻觉问题。

Method: 1) 开发自动化标注流程，整合基于运动学的运动计算和语言解析；2) 构建KPM-Bench数据集，包含细粒度视频-描述对、运动理解问答对和幻觉评估集；3) 提出基于语言的运动解析和提取(MoPE)算法；4) 将MoPE集成到GRPO后训练框架中。

Result: 1) 创建了KPM-Bench开源数据集；2) 开发了MoPE算法用于从文本描述中准确提取运动属性；3) 提出了不依赖大规模视觉-语言或纯语言模型的精确幻觉评估指标；4) 有效缓解了幻觉问题，显著提高了运动中心视频描述模型的可靠性。

Conclusion: 该研究通过创新的数据集构建和算法设计，系统解决了视频描述中的细粒度运动理解和幻觉问题，为运动中心视频理解提供了有效的解决方案和评估框架。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了3D-HIW数据集和CLUTCH系统，用于解决野外环境下3D手部动作建模的挑战，在文本到动作和动作到文本任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 手部动作在日常生活中至关重要，但现有方法依赖工作室采集的数据集，动作和上下文有限，难以扩展到野外环境，且难以保证动画保真度和文本-动作对齐。

Method: 1) 构建3D-HIW数据集：结合视觉语言模型和先进3D手部追踪器，从大量第一人称动作视频中提取32K个3D手部动作序列和对应文本；2) 提出CLUTCH系统：包含SHIFT（部分模态分解的VQ-VAE架构）用于手部动作标记化，以及几何精炼阶段微调LLM。

Result: 在文本到动作和动作到文本任务上实现了最先进的性能，建立了可扩展的野外手部动作建模的首个基准。

Conclusion: 通过3D-HIW数据集和CLUTCH系统，成功解决了野外环境下手部动作建模的挑战，为可扩展的野外手部动作建模设立了新标准。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net框架，通过跨模态特征幻觉而非像素级图像生成，直接从H&E切片预测HER2表达水平，避免重建伪影并提高效率


<details>
  <summary>Details</summary>
Motivation: 标准IHC染色资源密集、昂贵且耗时，在许多地区不可用。现有基于H&E切片的虚拟IHC图像生成方法计算成本高且易产生重建伪影，可能传播诊断错误

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，采用跨模态特征幻觉而非显式像素级图像生成。模型学习将形态学H&E特征直接映射到分子潜在空间，通过教师IHC编码器引导训练，并使用核分布和膜染色强度等任务特定领域知识进行正则化

Result: 在公开BCI数据集上的广泛实验表明，LGD-Net实现了最先进的性能，显著优于基线方法，同时支持使用单模态H&E输入进行高效推理

Conclusion: LGD-Net通过特征幻觉而非图像生成的方法，有效解决了虚拟IHC预测中的计算成本和伪影问题，为乳腺癌HER2表达评估提供了更高效可靠的替代方案

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [4] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一种无需额外训练、仅利用现有基础模型的遥感图像文本引导分割方法，结合对比式和生成式视觉语言模型与SAM，实现完全零样本或轻量LoRA调优的分割流程。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型和视觉基础模型为零样本遥感图像分割提供了新机会，但现有方法仍依赖额外的可训练组件，限制了泛化能力和实际应用。本研究旨在探索仅利用现有基础模型、无需额外训练就能实现文本引导遥感分割的可能性。

Method: 提出两种方法：1) 对比式方法使用CLIP作为SAM网格提议的掩码选择器，实现完全零样本的开放词汇语义分割；2) 生成式方法使用GPT-5（零样本）和LoRA调优的Qwen-VL模型生成点击提示给SAM，实现推理和指代分割。

Result: 在19个遥感基准测试（包括开放词汇、指代和基于推理的任务）上进行广泛实验，证明了方法的强大能力。对比式方法在完全零样本设置下实现了最先进的开放词汇语义分割，生成式方法中LoRA调优的Qwen-VL模型表现最佳。

Conclusion: 研究表明仅利用现有基础模型、无需额外训练即可实现有效的文本引导遥感分割，提出的训练免费或轻量LoRA调优流程具有实际应用价值，代码将开源。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [5] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统，共56K文本查询和51K视频，为视频领域的QPP研究提供了标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在信息检索中具有重要应用，但现有研究主要集中在文本和图像检索领域，基于内容的视频检索（CBVR）中的QPP研究严重不足，缺乏标准化的评估基准。

Method: 构建了首个视频查询性能预测基准VQPP，包含两个文本到视频检索数据集和两个CBVR系统，提供了官方训练、验证和测试划分。探索了多种检索前和检索后性能预测器，并使用最佳检索前预测器作为奖励模型，通过直接偏好优化训练大语言模型进行查询重写。

Result: 检索前预测器取得了有竞争力的性能，能够在执行检索步骤前进行应用。通过将最佳检索前预测器作为奖励模型，成功训练LLM进行查询重写任务，证明了VQPP的实际应用价值。

Conclusion: VQPP为视频领域的查询性能预测研究提供了首个标准化基准，促进了该领域的直接比较和可复现结果。检索前预测器的有效性为实际应用提供了可能，基准的发布将推动视频QPP研究的进一步发展。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [6] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文对Liu和Szirányi提出的手势识别方法进行方法学分析，指出其评估协议存在严重数据泄漏问题，导致报告的接近完美准确率指标不可靠。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析Liu和Szirányi手势识别方法的评估协议有效性，揭示其框架级随机训练-测试分割导致的数据泄漏问题，强调在基于视觉的手势识别研究中，特别是对于无人机-人交互等需要识别未见个体手势的应用，主体独立数据划分的重要性。

Method: 通过检查已发布的混淆矩阵、学习曲线和数据集构建，分析评估协议是否存在数据泄漏。具体关注训练-测试分割是否混合了相同主体的样本，从而无法测量对未见个体的泛化能力。

Result: 研究发现报告的接近完美准确率指标源于框架级随机训练-测试分割，该分割不可避免地混合了相同主体在训练集和测试集中的样本，导致严重的数据泄漏。评估协议未能测量对未见个体的泛化能力。

Conclusion: 研究强调了在基于视觉的手势识别研究中主体独立数据划分的重要性，特别是在需要可靠识别未见个体手势的应用中。评估协议的设计必须确保能够测量模型对未见个体的泛化能力，避免数据泄漏导致的虚假高准确率。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [7] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出了一种用于长视频理解的新型端到端框架，包含基于信息密度的自适应视频采样器和基于自动编码器的时空视频压缩器，与多模态大语言模型集成，有效处理长视频冗余问题。


<details>
  <summary>Details</summary>
Motivation: 随着视频骨干架构的进步和大语言模型的成功，分析长达数十分钟的长视频变得可行且普遍。然而，视频序列固有的冗余性给当前最先进模型带来了两大挑战：1) 在内存限制内高效处理更多帧；2) 从大量输入数据中提取判别性信息。

Method: 提出了一个端到端的长视频理解框架，包含两个核心组件：1) 基于信息密度的自适应视频采样器(AVS)，根据信息密度自适应捕获视频关键信息；2) 基于自动编码器的时空视频压缩器(SVC)，与多模态大语言模型(MLLM)集成，实现高压缩率同时保留关键判别信息。

Result: 该框架在多个基准测试中表现出色，不仅在长视频理解任务中表现优异，在标准视频理解基准测试中也取得良好结果，证明了该方法在处理长视频序列复杂性方面的有效性和通用性。

Conclusion: 提出的框架能够自适应有效地从不同时长的视频序列中捕获关键信息，实现高压缩率的同时保留重要判别信息，为长视频理解提供了一种高效且通用的解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [8] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 该研究发现当前视觉语言模型在细粒度图像分类任务上表现不佳，通过实验发现更好的视觉编码器能显著提升细粒度分类性能，预训练阶段特别是语言模型权重未冻结时对细粒度性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在各种视觉问答基准上取得了显著进展，但近期研究表明这些模型在传统的细粒度图像分类基准上表现落后。本研究旨在探索视觉语言模型在细粒度视觉知识与通用视觉能力之间的脱节原因。

Method: 研究测试了大量近期视觉语言模型在细粒度分类基准上的表现，通过一系列消融实验分析影响性能的因素，包括不同LLM和视觉编码器的影响，以及预训练阶段语言模型权重冻结状态对性能的影响。

Result: 研究发现：1）使用更好的LLM能同等提升所有基准分数；2）更好的视觉编码器能不成比例地提升细粒度分类性能；3）预训练阶段对细粒度性能至关重要，特别是当语言模型权重在预训练期间未冻结时。

Conclusion: 这些发现为增强视觉语言模型的细粒度视觉理解和视觉中心能力提供了重要见解，指出了改进方向：需要更强大的视觉编码器和优化的预训练策略。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [9] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该研究提出了一种多模态深度重建框架，利用极稀疏的雷达或激光雷达测距数据生成密集深度图，作为基于扩散模型的新视角合成的几何条件，显著提升了单图像新视角合成的几何一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的单图像新视角合成方法依赖于单目深度估计的几何信息，但在低纹理、恶劣天气和遮挡严重的真实场景中，深度估计的可靠性有限，导致合成视图的质量和一致性受到影响。

Method: 提出多模态深度重建框架，利用极稀疏的测距数据（如汽车雷达或激光雷达），在角度域中使用局部高斯过程建模深度，实现计算高效推理并显式量化观测有限区域的不确定性。重建的深度和不确定性可直接替换现有扩散渲染流程中的单目深度估计器。

Result: 在真实世界多模态驾驶场景实验中，用稀疏测距重建深度替换纯视觉深度，显著提高了单图像新视角视频生成的几何一致性和视觉质量。

Conclusion: 研究强调了可靠几何先验对基于扩散模型的视角合成的重要性，并展示了即使在极端稀疏情况下，多模态传感的实际优势。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [10] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑的视觉Transformer，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer依赖位置嵌入和类别标记，这些固定的空间先验在医学图像中可能成为障碍，因为医学图像的空间布局信息较弱或不一致，特别是在资源受限的临床环境中。

Method: 提出ZACH-ViT，移除位置嵌入和[CLS]标记，使用全局平均池化聚合补丁表示，实现排列不变性。采用自适应残差投影保持训练稳定性，同时严格控制参数预算。

Result: 在7个MedMNIST数据集上评估，ZACH-ViT（0.25M参数）在BloodMNIST上表现最佳，在PathMNIST上与TransMIL竞争，但在具有强解剖先验的数据集（OCTMNIST、OrganAMNIST）上优势减弱。尽管尺寸小且无预训练，仍保持亚秒级推理时间。

Conclusion: 架构的归纳偏置与数据结构对齐比追求通用基准主导更重要。ZACH-ViT在资源受限的临床环境中具有部署潜力，代码和模型已开源。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [11] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出基于记忆驱动的质量感知框架（MQAF），通过建立存储失真模式的记忆库，在有无参考图像时动态切换双模式质量评估策略，减少对高质量参考图像的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估（FR-IQA）方法依赖参考图像质量，限制了在理想参考源不可用的实际应用。受人类视觉系统通过长期记忆存储进行质量评估的能力启发，需要减少对高质量参考图像的依赖。

Method: 提出记忆驱动的质量感知框架（MQAF），建立存储失真模式的记忆库，动态切换双模式评估策略：有参考图像时，通过自适应加权参考信息并与记忆库中的失真模式比较获得质量分数；无参考图像时，依赖记忆库中的失真模式推断图像质量，实现无参考质量评估。

Result: 实验结果表明，该方法在多个数据集上优于现有最先进方法，同时能够适应无参考和全参考两种任务。

Conclusion: 提出的记忆驱动框架通过模拟人类视觉记忆机制，有效减少了对高质量参考图像的依赖，在有无参考图像的情况下都能实现高质量评估，具有更好的实际应用价值。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [12] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 本文提出了首个伪多模态水下目标跟踪基准MUOT_3M（包含300万个帧）和基于SAM的多模态到单模态跟踪器MUTrack，通过视觉几何对齐、视觉语言融合和四级知识蒸馏，在五个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋机器人、生态监测和海洋探索至关重要，但现有基准数据集规模小且仅包含RGB模态，限制了在颜色失真、浑浊和低能见度条件下的鲁棒性。

Method: 1) 构建MUOT_3M基准：包含3030个视频的300万帧，标注32个跟踪属性、677个细粒度类别，以及同步的RGB、增强RGB、深度和语言模态；2) 提出MUTrack跟踪器：基于SAM架构，采用视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识迁移到单模态学生模型中。

Result: 在五个水下目标跟踪基准测试中，MUTrack比最强SOTA基线在AUC上提升8.40%，在精度上提升7.80%，同时以24 FPS实时运行。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下跟踪建立了新基础，解决了水下跟踪领域的数据稀缺和模态限制问题。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [13] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型的LLM中心化情感视觉定制（L-AVC）任务，旨在通过编辑图像的主观情感内容来生成新图像，并开发了高效精确的情感操纵方法（EPEM）来解决情感语义转换和情感无关内容保留的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要关注语言、布局等客观控制信号与编辑图像的客观对齐，忽视了主观情感内容，并且缺乏通用的情感视觉定制基础模型。因此需要开发能够有效编辑图像主观情感的视觉定制方法。

Method: 提出EPEM（高效精确情感操纵）方法，包含两个核心模块：1）高效情感间转换（EIC）模块，使LLM在编辑前后高效对齐情感语义转换；2）精确情感外保留（PER）模块，精确保留情感无关内容。

Result: 在构建的L-AVC数据集上进行全面实验评估，结果显示EPEM方法在L-AVC任务上优于多个最先进的基线方法，证明了情感信息对L-AVC的重要性以及EPEM在高效精确操纵情感信息方面的有效性。

Conclusion: 情感信息对于LLM中心化情感视觉定制任务至关重要，提出的EPEM方法能够高效且精确地操纵图像中的情感信息，为情感视觉定制提供了有效的解决方案。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [14] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度安全导向视频理解任务DeepSVU，旨在不仅识别和定位威胁，还能归因和评估威胁原因。为解决建模物理世界信息和权衡因素的挑战，提出了统一物理世界正则化MoE方法UPRM，在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注检测和定位威胁（如枪击、抢劫），但缺乏生成和评估威胁原因的有效能力。为填补这一空白，本文提出了深度安全导向视频理解任务DeepSVU。

Method: 提出了统一物理世界正则化MoE方法UPRM，包含两个关键组件：统一物理世界增强MoE块用于建模从粗到细的物理世界信息（人类行为、物体交互、背景上下文），以及物理世界权衡正则化器用于自适应权衡这些因素。

Result: 在DeepSVU指令数据集（UCF-C指令和CUVA指令）上的广泛实验表明，UPRM优于多个先进的视频LLM和非VLM方法，验证了从粗到细物理世界信息在DeepSVU任务中的重要性以及UPRM捕获此类信息的有效性。

Conclusion: 本文提出的DeepSVU任务和UPRM方法成功解决了安全导向视频理解中威胁原因归因和评估的挑战，通过建模物理世界信息和自适应权衡机制，显著提升了视频安全分析的能力。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [15] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR方法，一种无需训练、即插即用的模块，通过不确定性感知的观测信息再注入机制，提升视觉-语言-动作模型在机器人操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常需要额外的观测线索（如深度图、点云）或辅助模块（如物体检测器）来提高任务执行精度，但这些方法需要昂贵的数据收集和额外训练。作者希望开发一种无需训练、即插即用的方法来增强VLA模型的性能。

Method: 提出不确定性感知观测再注入（UAOR）方法：当语言模型层表现出高不确定性（通过动作熵衡量）时，通过注意力检索机制将关键观测信息重新注入到下一层的FFN中，帮助模型在推理过程中更好地关注观测信息。

Result: 实验表明，该方法能持续提升多种VLA模型在仿真和真实世界任务中的性能，且开销极小。UAOR无需额外观测线索或模块，可作为现有VLA管道的通用即插即用插件。

Conclusion: UAOR是一种有效、无需训练、即插即用的模块，通过不确定性感知的观测信息再注入机制，显著提升VLA模型在机器人操作任务中的性能，具有很好的实用性和通用性。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [16] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出DCAG框架，通过同时操纵DiT注意力机制中的Key和Value通道，实现无需训练的图像编辑强度控制，在编辑保真度权衡上优于仅使用Key通道的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力操纵方法仅关注Key空间来调节注意力路由，而完全忽略了控制特征聚合的Value空间。本文发现DiT的多模态注意力层中Key和Value投影都表现出明显的偏置-增量结构，这为同时操纵两个通道提供了理论基础。

Method: 提出双通道注意力引导（DCAG）框架，同时操纵Key通道（控制注意力位置）和Value通道（控制特征聚合）。理论分析表明Key通道通过非线性softmax函数作为粗粒度控制旋钮，而Value通道通过线性加权求和作为细粒度补充。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）上，DCAG在所有保真度指标上始终优于仅使用Key通道的引导方法，在对象删除（LPIPS降低4.9%）和对象添加（LPIPS降低3.2%）等局部编辑任务中改进最为显著。

Conclusion: 二维参数空间(δ_k, δ_v)能够实现比任何单通道方法更精确的编辑-保真度权衡，证明了同时操纵Key和Value通道在扩散模型图像编辑中的有效性。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [17] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST提出了一种用于少样本动作识别的分解-融合框架，利用大语言模型提供的解耦空间和时间知识来学习表达性多粒度原型，在五个标准数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 少样本动作识别需要从少量标记视频中识别新动作类别。现有方法通常使用语义粗糙的类别名称作为辅助上下文，但这种上下文过于有限，无法为捕捉动作中的新颖空间和时间概念提供足够的背景知识。

Method: 提出DiST框架，包含分解和融合两个阶段：1）分解阶段：将原始动作名称解耦为多样化的时空属性描述；2）融合阶段：提出空间/时间知识补偿器（SKC/TKC）分别发现判别性对象级和帧级原型，SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模。

Result: 实验结果显示DiST在五个标准少样本动作识别数据集上取得了最先进的结果。

Conclusion: 通过利用大语言模型提供的解耦空间和时间知识，DiST能够学习表达性多粒度原型，为捕捉细粒度空间细节和多样化时间模式提供透明度，从而显著提升少样本动作识别性能。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [18] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard：一种用于分散式监控中隐私保护身份检索的拓扑感知Transformer框架，通过自适应度量学习、空间条件注意力和差分隐私嵌入映射，在保护隐私的同时实现跨视角身份匹配。


<details>
  <summary>Details</summary>
Motivation: 城市规模的人员重识别面临视角变化、遮挡和域偏移等挑战，同时需要遵守数据保护法规，禁止共享原始图像数据。需要开发既能保护隐私又能有效进行身份检索的解决方案。

Method: 1. 分散自适应度量学习：根据特征分布调整实例级边界，增强类内紧凑性；2. 空间条件注意力：将粗略几何信息（如GPS或部署平面图）注入基于图的自注意力机制，实现投影一致的跨视角对齐；3. 差分隐私嵌入映射：结合紧凑近似索引，支持安全且成本高效的部署。

Result: 在Market-1501和其他公共基准测试中，该框架在检索精度和查询吞吐量方面均优于强基线方法，并通过数据库规模的检索研究验证了其实际可行性。

Conclusion: CityGuard框架能够生成对视角变化、遮挡和域偏移具有鲁棒性的描述符，在严格的差分隐私核算下实现隐私与效用的可调平衡，为隐私关键的城市身份匹配提供了实用解决方案。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [19] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M是一个时间一致性感知的文本到动作生成框架，通过跨序列时间对齐和运动约束提升动作生成的语义对齐和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本到动作生成框架通常忽略跨序列时间一致性，导致语义错位和物理上不可信的动作。需要解决跨动作实例共享时间结构的问题。

Method: 提出TCA-T2M框架：1）时间一致性感知空间VQ-VAE用于跨序列时间对齐；2）掩码运动变换器用于文本条件动作生成；3）运动学约束块减轻离散化伪影确保物理合理性。

Result: 在HumanML3D和KIT-ML基准测试中达到最先进性能，证明了时间一致性对鲁棒和连贯文本到动作生成的重要性。

Conclusion: TCA-T2M通过关注跨序列时间一致性，显著提升了文本到动作生成的语义对齐和物理合理性，为动作生成研究提供了新方向。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [20] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段训练策略，通过自监督预训练和半监督微调，在减少BEV标注数据使用的同时提升道路标记分割性能


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头BEV语义地图方法依赖昂贵且标注不一致的地面真值数据，需要减少对完全监督的依赖

Method: 采用两阶段训练：1）自监督预训练阶段，将BEVFormer预测结果可微分地重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练，并加入时间一致性损失；2）监督微调阶段仅使用50%数据集

Result: 在nuScenes数据集上，相比完全监督基线模型，性能提升高达+2.5pp mIoU，同时减少一半标注数据使用，总训练时间减少三分之二

Conclusion: 可微分重投影加相机视角伪标签能够产生可迁移的BEV特征，为减少标注的自动驾驶感知提供了一条可扩展的路径

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [21] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript是一个大规模多作者手写印地语数据集，包含531位贡献者书写的六首传统印地语对句，旨在解决德瓦纳格里文字手写文本在公开基准数据集中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 德瓦纳格里文字手写文本在公开基准数据集中代表性严重不足。现有资源规模有限，主要关注孤立字符或短词，缺乏受控的词汇内容和作者多样性，无法捕捉德瓦纳格里手写体连续、融合和结构复杂的特性（如共享的shirorekha横线和丰富的连字形式）。

Method: 收集531位独特贡献者的手写印地语文本，设计为平行风格语料库，所有作者转录相同的六首传统印地语对句。数据集包含非识别性人口统计元数据，基于客观清晰度和分辨率标准的严格质量筛选，以及页面级布局难度标注。

Result: 基线实验显示清晰的质量分离和对未见作者的强泛化能力，突显了数据集的可靠性和实用价值。DohaScript可作为标准化、可复现的基准，用于推进低资源脚本环境下连续手写德瓦纳格里文本的研究。

Conclusion: DohaScript是一个大规模、多作者、受控设计的手写印地语数据集，支持手写识别、作者识别、风格分析和生成建模等任务，旨在解决德瓦纳格里文字手写文本研究中的数据稀缺问题。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [22] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT：一种无需训练的DiT加速框架，通过线性多步方法预测模型输出，结合校正器和动态步长调制，实现5.54倍延迟降低且保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)在图像和视频生成中广泛应用，但其迭代去噪过程计算成本高。现有无训练加速方法依赖特征缓存和重用，但重用多个步骤可能导致潜在漂移和视觉质量下降。作者观察到模型输出在扩散轨迹的大部分区域平滑演化，可以进行预测而非简单重用。

Method: 提出PrediT框架：1) 将特征预测建模为线性多步问题，使用经典线性多步方法从历史信息预测未来模型输出；2) 在高动态区域激活校正器防止误差累积；3) 通过监测特征变化率自适应调整预测步长的动态步长调制机制。

Result: 在各种基于DiT的图像和视频生成模型上实现了高达5.54倍的延迟降低，同时质量下降可以忽略不计。实验验证了该方法在保持生成保真度的同时实现显著加速。

Conclusion: PrediT是一种有效的无训练加速框架，通过预测而非简单重用特征，结合自适应机制，在显著降低计算成本的同时保持了生成质量，为DiT模型的实时应用提供了实用解决方案。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [23] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench：一个自动化的基准测试框架，用于评估视觉语言模型处理分布外数据的能力，包含40K个实例-类别对，并提出了可靠的自评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型通常假设数据独立同分布，但现实应用中常遇到分布外数据，处理不当可能带来安全风险。目前缺乏全面评估VLM处理OOD数据能力的有效基准。

Method: 提出OODBench方法，采用最小人工验证的自动化方式构建新基准，包含40K实例级OOD实例-类别对，并提出从基础到高级的渐进式提示问题自动评估指标。

Result: 当前VLM在OODBench上表现出显著性能下降，即使底层图像类别很常见。提出的自动评估指标能更全面地评估OOD数据对不同难度问题的影响。

Conclusion: OODBench为未来研究提供了获取和评估OOD数据的基准框架，总结了重要发现和见解以促进相关研究发展。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [24] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在可视化图形感知任务中表现不如CNNs，与人类感知对齐有限


<details>
  <summary>Details</summary>
Motivation: 虽然ViTs在多种图像任务中表现出色，但其在可视化图形感知任务中的能力尚未被充分探索，这些任务对视觉解释至关重要

Method: 基于Cleveland和McGill的基础研究，设计了一系列受控的图形感知任务，将ViTs与CNNs和人类参与者进行对比评估

Result: ViTs在通用视觉任务中表现强劲，但在可视化领域的图形感知任务中与人类感知的对齐程度有限

Conclusion: ViTs在可视化系统中应用时存在感知差距，需要在图形感知建模中考虑这些局限性

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [25] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard是一个用于短视频广告内容审核的框架，结合思维链推理、基于规则的政策原则和批评引导奖励，通过强化学习提升模型性能，能有效检测模态内和跨模态的欺骗性内容。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告包含欺骗性的视觉、语音和字幕内容，需要比社区安全过滤器更精细、基于政策的审核机制。

Method: 1. 结合思维链推理与基于规则的政策原则和批评引导奖励；2. 使用规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签；3. 通过强化学习使用平衡因果一致性和政策遵从性的复合奖励来优化模型；4. 采用多任务架构建模模态内操作（如夸张图像）和跨模态不匹配（如字幕-语音漂移）。

Result: 在真实短视频广告上的实验显示，BLM-Guard在准确性、一致性和泛化能力方面超越了强基线模型。

Conclusion: BLM-Guard框架为商业广告内容审核提供了一种有效的解决方案，能够处理多模态欺骗性内容，同时降低标注成本并提升审核质量。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [26] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督数据驱动方法修正文本生成动作中的物理不合理性（如脚部漂浮），同时保持语义一致性，可应用于各种文本到动作生成模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成人体动作的方法在语义对齐方面进展迅速，但难以同时保证语义和物理合理性。现有方法生成的动经常出现物理上不合理的问题（如脚部漂浮），需要一种既能修正物理不合理性又能保持语义一致性的解决方案。

Method: 提出Distortion-aware Motion Calibrator (DMC)，这是一个后处理模块，采用自监督和数据驱动的方法。DMC学习在给定故意扭曲的动作和原始文本描述的情况下，生成物理上合理的动作，而不依赖复杂的物理建模。

Result: DMC在多种文本到动作生成模型上表现优异：在T2M上FID分数降低42.74%，在T2M-GPT上降低13.20%，同时达到最高的R-Precision。应用于MoMask等高质量模型时，穿透减少33.0%，漂浮伪影调整更接近真实参考。

Conclusion: DMC作为一个有前景的后处理动作精炼框架，能够通过结合文本语义和物理合理性，提升任何文本到动作生成模型的效果，在保持语义一致性的同时显著改善物理合理性。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [27] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 本文首次研究了离散图像分词器的对抗攻击脆弱性，提出了针对特征提取的攻击方法，并开发了无监督对抗训练防御策略，显著提升了分词器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中越来越流行，但相比CLIP编码器，其对抗攻击脆弱性尚未被研究。本文旨在填补这一空白，研究分词器的安全漏洞并提出防御方法。

Method: 首先制定针对离散分词器特征提取的攻击方法，旨在扰动特征并改变提取的令牌。然后，受鲁棒CLIP编码器研究启发，采用无监督对抗训练对流行分词器进行微调，保持其他组件不变。

Result: 攻击方法计算高效、应用无关，在分类、多模态检索和字幕生成任务中均有效。防御方法显著提升了对无监督和端到端监督攻击的鲁棒性，并能很好地泛化到未见任务和数据。

Conclusion: 本文强调了分词器鲁棒性在下游任务中的关键作用，为开发安全的多模态基础模型迈出了重要一步。无监督对抗训练方法比监督方法更灵活，可利用未标记图像。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [28] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的新框架，通过实例细节提取器和细节融合模块解决现有方法在复杂文本描述下的语义理解挑战，在空间一致性、语义准确性和组合泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多实例生成方法在空间布局和属性绑定方面已有进展，但在处理复杂文本描述时仍面临细粒度语义理解的挑战，特别是在防止实例间属性泄漏和精确匹配局部化文本描述方面存在局限。

Method: 提出DEIG框架，包含两个核心组件：1) 实例细节提取器(IDE)，将文本编码器嵌入转换为紧凑的实例感知表示；2) 细节融合模块(DFM)，应用基于实例的掩码注意力机制防止实例间属性泄漏。同时构建高质量数据集支持细粒度监督，并创建DEIG-Bench基准测试。

Result: 实验表明DEIG在多个基准测试中，在空间一致性、语义准确性和组合泛化方面持续优于现有方法。DEIG可作为即插即用模块，易于集成到标准基于扩散的流程中。

Conclusion: DEIG通过创新的实例细节提取和融合机制，实现了对复杂文本描述的细粒度语义理解，能够生成视觉连贯且精确匹配局部化描述的多实例场景，为可控多实例生成提供了有效解决方案。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [29] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS框架通过结合全局草图引导和多个局部草图-文本对来增强时尚图像生成，在保持全局结构的同时利用局部语义指导


<details>
  <summary>Details</summary>
Motivation: 草图为设计师提供了早期时尚构思的简洁表达媒介，而文本描述则补充了材质、颜色和风格细节。有效结合文本和视觉模态需要在利用文本局部属性指导时保持草图视觉结构

Method: 提出LOTS框架，包含多级条件阶段和扩散对引导阶段。多级条件阶段在共享潜在空间中独立编码局部特征，同时保持全局结构协调；扩散对引导阶段通过基于注意力的引导在扩散模型的多步去噪过程中整合局部和全局条件

Result: 创建了首个时尚数据集Sketchy，包含每个图像对应的多个文本-草图对。实验表明该方法在保持全局结构一致性的同时利用了更丰富的局部语义指导，优于现有最先进方法

Conclusion: LOTS框架通过结合全局草图引导和局部草图-文本对，有效增强了时尚图像生成的质量和结构一致性，为时尚设计提供了强大的多模态生成工具

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [30] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS是一个用于手术场景实时重建的两阶段框架，通过扩散模型修复被器械遮挡的组织，并使用2D高斯泼溅和可学习变形模型捕捉动态组织变形，在图像质量和几何精度上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实时重建可变形手术场景对于推进机器人手术、改善外科医生指导和实现自动化至关重要。现有方法在遮挡区域的重建质量有限，且深度精度未得到充分评估，因为现有基准缺乏3D真实数据。

Method: 提出Diff2DGS两阶段框架：第一阶段使用基于扩散的视频模块，利用时间先验修复被器械遮挡的组织，保持高时空一致性；第二阶段采用2D高斯泼溅（2DGS）结合可学习变形模型（LDM）来捕捉动态组织变形和解剖几何结构。

Result: 在EndoNeRF上达到38.02 dB PSNR，在StereoMIS上达到34.40 dB PSNR，优于现有方法。实验表明仅优化图像质量不一定能获得最优3D重建精度，因此进一步优化深度质量以确保更准确的几何重建。

Conclusion: Diff2DGS在图像外观和几何精度方面都优于现有方法，通过两阶段框架实现了可靠的手术场景3D重建，并首次在SCARED数据集上进行定量深度精度分析，验证了方法的有效性。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [31] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++：基于3D高斯泼溅的鲁棒新视角合成框架，通过全局自适应亮度调整和局部像素级残差细化解决多视角光照不一致问题，保持实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 真实环境中的高质量图像采集面临复杂光照变化和相机成像管道限制的挑战。在多视角捕获中，光照、传感器响应和ISP配置的差异导致光度学和色彩不一致，违反了现代3D新视角合成方法（如NeRF和3DGS）所依赖的光度一致性假设，导致重建和渲染质量下降。

Method: 提出Luminance-GS++框架，结合全局视角自适应亮度调整和局部像素级残差细化进行精确色彩校正。设计无监督目标函数，联合强制执行亮度校正以及多视角几何和光度一致性。保持显式3DGS表示，不修改底层表示。

Result: 在低光照、过曝光以及复杂亮度和色彩变化等挑战性场景中实现了最先进的性能。相比之前修改底层表示的方法，该方法保持了3DGS的显式表示，提高了重建保真度，同时保持了实时渲染效率。

Conclusion: Luminance-GS++通过创新的亮度校正和色彩细化策略，有效解决了多视角捕获中的光照不一致问题，为复杂光照条件下的鲁棒新视角合成提供了有效解决方案，同时保持了3DGS的实时渲染优势。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [32] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯-拉普拉斯算子的双过滤方法（G-LoG），用于医学图像拓扑数据分析，在多参数持久性模块中生成更有效的特征，并在MedMNIST数据集上验证了其优于单参数过滤的性能。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析中，构建实用的过滤方法来检测拓扑和几何特征是一个重要任务。医学图像边界增强对于特征提取至关重要，而拉普拉斯高斯算子在这方面具有优势。本文旨在开发更适合多参数持久性模块的特征生成方法。

Method: 1. 利用拉普拉斯高斯算子增强医学图像边界的能力，定义G-LoG（高斯-拉普拉斯高斯）双过滤方法；2. 将体积图像建模为有界函数；3. 证明从有界函数的双过滤获得的持久性模块的交错距离相对于有界函数的最大范数是稳定的；4. 在MedMNIST数据集上进行实验，与单参数过滤和深度学习基线模型（Google AutoML Vision、ResNet、AutoKeras、auto-sklearn）进行比较。

Result: 1. G-LoG双过滤显著优于单参数过滤；2. 使用双过滤生成的拓扑特征训练的简单多层感知器（MLP）性能与在原始数据集上训练的复杂深度学习模型相当；3. 证明了持久性模块交错距离的稳定性。

Conclusion: G-LoG双过滤方法为医学图像拓扑数据分析提供了有效的多参数特征提取工具，能够在保持理论稳定性的同时，实现与复杂深度学习模型相媲美的性能，为拓扑数据分析在医学图像处理中的应用提供了新思路。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [33] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种基于头部和手部姿态控制的人为中心视频世界模型，用于扩展现实(XR)中的交互式虚拟环境生成


<details>
  <summary>Details</summary>
Motivation: 当前视频世界模型只能接受文本或键盘等粗粒度控制信号，无法响应用户真实运动追踪，限制了在具身交互中的实用性。扩展现实(XR)需要能够响应用户追踪现实世界运动的生成模型。

Method: 引入基于追踪头部姿态和关节级手部姿态的人为中心视频世界模型；评估现有扩散变换器条件策略，提出有效的3D头部和手部控制机制；训练双向视频扩散模型教师，并将其蒸馏为因果交互系统生成第一人称虚拟环境。

Result: 通过人类受试者评估显示，相比相关基线，该系统显著提高了任务性能，并显著提升了用户对执行动作的感知控制水平。

Conclusion: 提出的基于头部和手部姿态控制的人为中心视频世界模型能够有效支持扩展现实中的交互式虚拟环境生成，实现了对用户动作的精细控制。

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [34] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: CapNav是一个评估视觉语言模型在考虑智能体物理能力约束下进行室内导航的新基准，包含5种代表性智能体、45个真实室内场景、473个导航任务和2365个问答对，测试发现当前VLM在严格移动约束下导航性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界导航决策需要考虑智能体的物理移动约束（如扫地机器人不能爬楼梯，四足机器人可以），但现有视觉语言导航研究缺乏对智能体具体能力条件的评估，需要建立能力条件导航基准来评估VLM在实际约束下的表现。

Method: 创建CapNav基准：1)定义5种代表性人类和机器人智能体，描述其物理尺寸、移动能力和环境交互能力；2)提供45个真实室内场景和473个导航任务；3)构建2365个问答对；4)评估13个现代VLM在考虑智能体能力约束下的导航表现。

Result: 评估发现：1)当前VLM的导航性能随着移动约束收紧而急剧下降；2)即使是state-of-the-art模型在处理需要空间维度推理的障碍类型时也表现不佳；3)现有模型在能力感知导航方面存在显著局限性。

Conclusion: CapNav基准揭示了当前VLM在能力条件导航方面的不足，强调了未来VLM发展中需要考虑智能体具体物理约束的重要性，为推进具身空间推理提供了重要方向和评估工具。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [35] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过增加token预算、自适应选择策略和免训练检索专家混合，提升流式视频理解性能，在多个基准上显著超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的每帧token数量，导致细粒度视觉细节丢失，且在处理密集视频流时存在查询-帧相似度随时间增加的问题，偏向检索后期帧

Method: 1. 增加token预算以实现更细粒度的时空理解；2. 引入自适应选择策略减少token冗余同时保留局部时空信息；3. 提出免训练检索专家混合，利用外部模型更好地识别相关帧

Result: 在CG-Bench上提升+8.0%，LVBench上提升+8.5%，VideoMME(Long)上提升+2.4%，显著超越ReKV with Qwen2.5-VL-7B

Conclusion: MemStream通过增加token预算、自适应选择和检索专家混合，有效解决了流式视频理解中的信息丢失和检索偏差问题，显著提升了视频问答性能

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [36] [QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration](https://arxiv.org/abs/2602.17784)
*Meng Ye,Xiao Lin,Georgina Lukoczki,Graham W. Lederer,Yi Yao*

Main category: cs.CL

TL;DR: QueryPlot是一个语义检索和制图框架，通过自然语言处理技术整合地质文本数据和地质图数据，实现矿物远景区的自动化识别和可视化。


<details>
  <summary>Details</summary>
Motivation: 传统的矿物远景区制图需要人工整合异质地质知识，包括文本矿床模型和地理空间数据集，这个过程既耗时又依赖专家知识。需要一种自动化方法来提高效率和可扩展性。

Method: 开发QueryPlot框架，整合120多种矿床类型的描述性模型，将地质图多边形转换为结构化文本表示。使用预训练嵌入模型对用户自然语言查询和区域描述进行编码，计算语义相似度得分，对区域进行排序和空间可视化。支持组合查询和多个相似度层的聚合分析。

Result: 在钨矽卡岩矿床案例研究中，基于嵌入的检索方法能够高召回率地识别已知矿床，产生的远景区与专家定义的许可区域高度一致。相似度得分作为特征加入监督学习流程后，显著提高了分类性能。

Conclusion: QueryPlot通过整合地质文本和空间数据，提供了一种高效、可扩展的矿物远景区制图方法，支持交互式查询和可视化，并可作为监督学习流程的特征增强工具。

Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.

</details>


### [37] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: 大型语言模型在预测眼动和阅读时间数据方面表现更好，但会低估人类反应概率；模型越大，对完形填空数据的预测质量越高，但对词汇共现统计敏感性降低


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在预测人类语言处理行为（如眼动和阅读时间）方面的表现差异，探索模型规模如何影响对语义信息和词汇统计信息的敏感性

Method: 通过比较不同规模的语言模型在眼动、阅读时间数据和完形填空任务中的表现，分析模型对词汇共现统计的敏感性和语义对齐程度

Result: 大型语言模型在预测眼动和阅读时间数据方面表现更好，但普遍低估人类反应概率；模型规模越大，对完形填空数据的预测质量越高，对词汇共现统计的敏感性降低，但与人类语义反应更一致

Conclusion: 大型语言模型更强的记忆能力使其能猜测更语义合适的词语，但对词汇识别相关的低层信息敏感性降低；模型规模增加有助于语义对齐，但可能牺牲对词汇统计信息的敏感性

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [38] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 研究发现，转向向量在控制语言模型行为时存在可靠性差异，主要取决于训练激活差异的余弦相似度和行为数据中正负激活在转向方向上的分离程度。


<details>
  <summary>Details</summary>
Motivation: 转向向量虽然平均有效，但在不同样本和许多目标行为上效果不稳定。研究旨在探究转向可靠性在不同行为间存在差异的原因，以及训练数据如何影响转向效果。

Method: 通过分析转向向量训练数据，研究：1）训练激活差异的余弦相似度与转向可靠性的关系；2）行为数据中正负激活在转向方向上的分离程度；3）不同提示变体训练的转向向量的方向差异和效果相关性。

Result: 1）训练激活差异的余弦相似度越高，转向越可靠；2）正负激活在转向方向上分离更好的行为数据集更容易被可靠转向；3）不同提示变体训练的转向向量方向不同但效果相似，且在不同数据集上效果相关。

Conclusion: 转向向量不可靠的原因是潜在目标行为表示无法被线性转向方向有效近似。这些发现为诊断转向不可靠性提供了实用方法，并激励开发更鲁棒的转向方法，需明确考虑非线性潜在行为表示。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [39] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出使用语言模型生成语义软标签目标来改进神经主题模型，通过重构这些软标签而非传统词袋表示，获得更高质量的主题和更好的文档检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型通过重构文档的词袋表示进行优化，忽略了上下文信息且难以处理数据稀疏性问题，导致主题质量不高。

Method: 使用语言模型通过特定提示生成下一个词的概率分布，将其投影到预定义词汇表上获得语义软标签目标，然后训练主题模型重构这些软标签，利用语言模型的隐藏状态。

Result: 在三个数据集上的实验表明，该方法在主题一致性和纯度方面显著优于现有基线方法，同时引入的检索指标显示在识别语义相似文档方面表现优异。

Conclusion: 通过语言模型生成的语义软标签目标能够有效提升神经主题模型的质量，特别是在主题一致性和文档检索应用方面具有显著优势。

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [40] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 提出了CondMedQA基准测试和Condition-Gated Reasoning框架，用于解决生物医学问答中条件推理的不足，通过构建条件感知知识图谱和选择性激活推理路径来提升医疗决策的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学问答系统假设医学知识普遍适用，但真实临床推理本质上是条件性的，几乎所有决策都依赖于患者特定因素。现有基准测试无法评估这种条件推理，检索增强或基于图的方法缺乏确保检索知识适用于给定上下文的明确机制。

Method: 提出了Condition-Gated Reasoning（CGR）框架，构建条件感知知识图谱，并基于查询条件选择性激活或剪枝推理路径。同时创建了CondMedQA基准测试，包含答案随患者条件变化的多跳问题。

Result: CGR框架在更可靠地选择条件适当答案的同时，在生物医学问答基准测试上达到或超过最先进性能，突显了显式建模条件性对于稳健医疗推理的重要性。

Conclusion: 该研究填补了生物医学问答中条件推理评估的空白，提出的CGR框架通过显式建模条件性显著提升了医疗决策的可靠性和准确性，为临床推理系统的发展提供了重要方向。

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [41] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 本文首次基于DSPy优化框架系统比较了表格事实验证中的指令优化技术，评估了四种提示技术，发现指令优化能持续提升验证准确率，不同优化器对不同提示技术有特定优势。


<details>
  <summary>Details</summary>
Motivation: 指令优化为提升大语言模型推理性能提供了一种轻量级、模型无关的方法，但缺乏对表格事实验证任务的系统比较研究。

Method: 基于DSPy优化框架，评估了四种提示技术（直接预测、思维链、带SQL工具的ReAct、带Python执行的CodeAct），使用三种优化器（COPRO、MiPROv2、SIMBA）在四个基准测试和三个模型家族上进行实验。

Result: 指令优化持续提升验证准确率：MiPROv2对思维链提示带来最稳定的增益，SIMBA对ReAct智能体提供最大收益（尤其在更大模型规模时）。行为分析显示SIMBA通过启发式方法鼓励更直接的推理路径。

Conclusion: 思维链提示在表格事实检查中仍然有效（尤其对小模型），而基于大模型的ReAct智能体虽然能达到竞争性性能，但需要仔细的指令优化。SIMBA优化器能改善数值比较能力和减少不必要的工具调用。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [42] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: CUICurate是一个基于图检索增强生成（GraphRAG）的框架，用于自动化UMLS概念集构建，结合知识图谱检索和LLM过滤，显著减少人工工作量并提高概念集完整性。


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别工具通常将自由文本映射到UMLS概念唯一标识符（CUI），但许多下游任务需要的是包含相关同义词、子类型和超类型的概念集。目前构建这样的概念集是劳动密集型的，执行不一致，现有工具支持不足。

Method: 构建UMLS知识图谱并进行嵌入以实现语义检索。针对每个目标概念，从知识图谱中检索候选CUI，然后通过大语言模型（GPT-5和GPT-5-mini）进行过滤和分类。在五个词汇异质性临床概念上评估该框架。

Result: CUICurate生成的概念集比人工基准更大、更完整，同时保持与人工相当的精确度。GPT-5-mini在过滤阶段召回率更高，而GPT-5的分类结果更符合临床医生判断。输出结果在重复运行中稳定且计算成本低。

Conclusion: CUICurate提供了一个可扩展且可重复的方法来支持UMLS概念集构建，显著减少人工工作量。通过整合基于图的检索和LLM推理，该框架生成聚焦的候选概念集，可适应不同表型和分析需求的临床NLP流程。

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [43] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 该研究针对金融监管文件问答中的检索失败模式，提出了一种针对页面级检索优化的领域微调方法，显著提升了页面召回率和块级检索性能。


<details>
  <summary>Details</summary>
Motivation: 金融监管文件问答中，即使检索到正确文档，但经常因错过包含答案的具体页面或文本块而导致生成器基于不完整上下文进行推断，这种文档内检索失败模式在金融QA文献中缺乏系统研究。

Method: 1) 在多粒度级别（文档、页面、块）评估检索性能；2) 引入基于oracle的分析提供检索和生成性能的实证上限；3) 在FinanceBench的150个问题子集上复现和比较多种检索策略；4) 提出领域微调的页面评分器，将页面作为文档和块之间的中间检索单元，专门针对金融文件微调双编码器。

Result: 1) 文档发现率的提升通常转化为更强的页面召回率；2) Oracle性能显示页面和块级检索仍有改进空间；3) 提出的领域微调页面评分器显著改善了页面召回率和块检索性能。

Conclusion: 针对金融监管文件问答中的文档内检索失败问题，通过领域微调的页面级检索方法能够有效利用页面的语义连贯性，显著提升检索精度，为高风险的金融问答应用提供了更可靠的解决方案。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [44] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 论文认为传统基于静态基准测试的AI评估方法已不适应工具使用型智能体系统，主张将评估重构为持续测量学科而非一次性性能展示


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从静态模型演变为复合型工具使用智能体，传统评估方法（静态基准、聚合分数、一次性成功标准）已无法有效衡量系统在变化环境中的可信行为，反而可能掩盖系统真实问题

Method: 通过分析评估管道本身引入的隐性故障模式，解释高基准分数误导团队的原因，探讨智能体系统如何从根本上改变性能测量的意义，而非提出新指标或更难的基准测试

Result: 揭示了传统评估方法在智能体时代的问题：评估管道可能引入隐性故障，高基准分数经常误导团队，智能体系统改变了性能测量的本质含义

Conclusion: 评估应重构为核心控制功能而非最终检查点，成为建立信任、支持迭代和治理非确定性系统的测量学科，而非性能展示剧场

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [45] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 研究发现，当用户被告知ChatGPT对其所属政党有偏见时，其说服效果会降低28%，表明AI说服力受政治中立性感知影响


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨精英阶层对LLM的政治偏见指控是否会削弱其纠正公众误解和传播准确信息的能力，特别是在党派冲突日益加剧的背景下

Method: 采用预注册的美国调查实验（N=2144），参与者与ChatGPT进行三轮对话讨论个人持有的经济政策误解，实验组收到提示表明LLM对参与者所属政党有偏见

Result: 与中性对照组相比，收到偏见警告的实验组说服效果降低了28%。对话记录分析显示警告改变了互动模式：受访者更多反驳且接受度降低

Conclusion: 对话AI的说服效果具有政治条件性，受党派一致性感知的制约。精英对LLM的政治偏见指控会显著削弱其纠正公众误解的能力

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [46] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 本文提出对抗性提问生成框架，通过对比待适应模型与专家模型的输出，生成紧凑的语义挑战性问题，以更少样本提升LLM在专业领域的适应能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域适应能力有限，现有微调方法面临高质量任务相关数据稀缺的问题。传统合成数据生成方法（如转述或知识提取）虽然擅长事实回忆和概念知识，但存在两个关键缺陷：1) 对专业领域的解释推理能力支持不足；2) 生成的合成语料库通常过大且冗余，样本效率低下。

Method: 提出对抗性提问生成框架，通过对比待适应模型与基于参考文档的专家模型的输出，采用迭代反馈驱动过程，生成紧凑的语义挑战性问题。该方法旨在揭示和解决模型理解差距，产生高质量的训练样本。

Result: 在LegalBench语料库的专业子集上评估表明，该方法能够以显著更少的合成样本实现更高的准确性，证明了其样本效率优势。

Conclusion: 对抗性提问生成框架有效解决了传统合成数据生成方法的局限性，通过生成紧凑的语义挑战性问题，以更高效的方式提升LLM在专业领域的适应能力。

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [47] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 该论文提出了一种基于频率分析的注意力机制检测大语言模型幻觉的方法，通过分析注意力分布的高频成分来识别幻觉标记，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力机制的幻觉检测方法通常依赖粗粒度总结，无法捕捉注意力中的细粒度不稳定性。需要一种更精细的方法来分析注意力模式以检测幻觉。

Method: 受信号处理启发，将注意力分布建模为离散信号，提取反映注意力快速局部变化的高频成分。基于高频注意力特征开发轻量级幻觉检测器。

Result: 在RAGTruth和HalluRAG基准测试中，该方法在多个模型和任务上优于基于验证、内部表示和注意力的现有方法，取得了性能提升。

Conclusion: 注意力分布的高频成分能有效反映幻觉标记的不稳定接地行为，基于频率分析的注意力特征为轻量级幻觉检测提供了有效途径。

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [48] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: 通过无损压缩分析发现，大语言模型生成的文本比人类写作具有更高的结构规律性和可压缩性，但在小规模交互环境中这种差异会减弱


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型通过概率采样生成文本时，如何重塑语言的结构统计组织，以及如何通过简单、模型无关的方法来区分生成机制

Method: 使用无损压缩作为衡量统计规律性的方法，分析三个渐进复杂的信息生态系统：受控的人类-LLM延续、知识基础设施的生成中介（维基百科 vs Grokipedia）、完全合成的社交互动环境（Moltbook vs Reddit）

Result: 压缩分析揭示了概率生成的结构特征：在受控和中介环境中，LLM生成的语言比人类写作具有更高的结构规律性和可压缩性；但在碎片化的交互环境中，这种分离会减弱，表明在小尺度上存在表面可区分性的基本限制

Conclusion: 提出了一种简单而稳健的框架，用于量化生成系统如何重塑文本生产，为通信的演化复杂性提供了结构视角，这种方法可以在不依赖模型内部或语义评估的情况下直接从表面文本观察到

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [49] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: FENCE是一个用于金融领域多模态越狱检测的双语数据集，包含韩语和英语的金融相关查询与图像威胁配对，用于训练和评估越狱检测器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型面临越狱攻击风险，特别是在金融领域。目前缺乏针对多模态越狱检测的资源，尤其是金融领域的专门数据集。

Method: 创建FENCE双语多模态数据集，包含金融相关查询与图像威胁配对。使用商业和开源VLM进行实验，并训练基线检测器评估数据集效果。

Result: 实验显示GPT-4o存在可测量的攻击成功率，开源模型暴露更大风险。基于FENCE训练的基线检测器达到99%的分布内准确率，在外部基准测试中表现良好。

Conclusion: FENCE为金融领域多模态越狱检测提供了专门资源，有助于开发更安全可靠的AI系统，特别是在敏感领域。

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [50] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: 提出了一种混合方法检测点击诱饵标题，结合Transformer文本嵌入和语言学特征，XGBoost模型在增强特征后达到91%的F1分数，优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题降低了在线信息质量并损害用户信任，需要有效的检测方法来识别和过滤这类内容。

Method: 采用混合方法，结合基于Transformer的文本嵌入和语言学启发的信息性特征，使用NLP技术评估多种向量化方法，最终采用XGBoost分类器处理增强特征。

Result: 最佳模型（XGBoost结合增强特征）达到91%的F1分数，优于TF-IDF、Word2Vec、GloVe、LLM提示分类和仅特征基线，特征集提高了模型可解释性。

Conclusion: 提出的混合方法在点击诱饵检测中表现出色，增强的特征集不仅提高性能还增强可解释性，通过突出第二人称代词、最高级、数字等语言学线索实现透明预测。

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [51] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 本文提出Info-Gain Sampler，一种用于掩码扩散模型（MDMs）的改进解码框架，通过平衡即时不确定性和对未来掩码标记的信息增益，解决了现有贪婪采样器忽视当前解码选择对后续步骤影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型的采样器通常采用贪婪启发式方法，优先解码局部确定性最高的位置，但这种方法存在根本性局限：忽视当前解码选择对后续步骤的下游影响，未能充分利用MDMs的非因果特性来评估解码决策如何重塑所有剩余掩码位置的概率/不确定性。

Method: 提出Info-Gain Sampler解码框架，在解码过程中不仅考虑当前位置的即时不确定性，还考虑当前解码选择对未来掩码标记的信息增益，从而更全面地优化累积不确定性。

Result: 在推理、编码、创意写作和图像生成等多种架构和任务上的广泛评估表明，Info-Gain Sampler始终优于现有MDMs采样器。例如，在推理任务上平均准确率提升3.6%，创意写作任务上获得63.1%的胜率，并将累积不确定性从78.4降低到48.6。

Conclusion: Info-Gain Sampler通过平衡即时不确定性和信息增益，为掩码扩散模型提供了更优的解码策略，显著提升了生成质量，特别是在需要全局规划的任务中表现出色。

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [52] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 该研究提出了一种基于信息论的句子理解加工存储成本测量方法，使用预训练神经语言模型来量化先前词语对未来语境的信息承载量，相比传统基于语法的离散测量更加连续和理论中立。


<details>
  <summary>Details</summary>
Motivation: 实时句子理解给工作记忆带来显著负荷，但现有的加工负荷测量主要基于符号语法，采用离散、统一的句法预测成本。需要一种更连续、理论中立的测量方法来更好地反映实际认知加工过程。

Method: 提出基于信息论的加工存储成本测量方法，定义为在不确定性条件下，先前词语对未来语境所承载的信息量。该方法可通过预训练神经语言模型进行估计，是连续且理论中立的。

Result: 在英语中的三项分析显示：1）该方法能够恢复中心嵌套和关系从句中已知的加工不对称性；2）与语法标注语料库中的语法基础存储成本相关；3）在两个大规模自然数据集上，能够预测阅读时间方差，且优于包含传统信息预测因子的基线模型。

Conclusion: 基于信息论的加工存储成本测量方法为句子理解中的工作记忆负荷提供了有效的连续量化工具，能够捕捉传统离散语法方法无法反映的加工复杂性，为心理语言学理论提供了新的形式化工具。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [53] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: 提出了一种名为"Thinking by Subtraction"的置信度驱动对比解码方法，通过检测解码过程中的低置信度令牌并选择性干预，提高大语言模型的推理可靠性，同时减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设分配更多推理时间计算能均匀提高正确性，但研究表明推理不确定性高度局部化：一小部分低置信度令牌对推理错误和不必要的输出扩展贡献不成比例。

Method: 提出置信度驱动对比解码方法：检测解码过程中的低置信度令牌，在这些位置选择性干预；通过用最小占位符替换高置信度令牌构建对比参考，在低置信度位置通过减去参考分布来优化预测。

Result: 实验显示CCD显著提高了数学推理基准测试的准确性，同时大幅减少了输出长度，且KV缓存开销最小。作为无需训练的方法，CCD通过针对性低置信度干预提高了推理可靠性，避免了计算冗余。

Conclusion: 通过置信度驱动的对比解码方法，实现了针对低置信度令牌的选择性干预，在提高推理可靠性的同时减少了不必要的输出扩展，为LLM推理提供了高效的计算优化方案。

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [54] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: ELIA是一个交互式Web应用，通过集成多种语言模型分析技术并引入AI生成的自然语言解释，降低了机制可解释性分析的门槛，使非专家也能理解复杂的LLM内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性分析工具虽然强大，但过于复杂，仅限于专家使用，存在可访问性差距。需要设计一个系统来简化语言模型组件分析结果，让更广泛的受众能够理解。

Method: 开发了ELIA交互式Web应用，集成归因分析、函数向量分析和电路追踪三种关键技术，并引入创新方法：使用视觉语言模型自动为这些方法产生的复杂可视化生成自然语言解释。

Result: 通过混合方法用户研究验证了有效性，用户明显偏好交互式可探索界面而非静态可视化。AI生成的解释帮助非专家弥合知识差距，统计分析显示用户先前的LLM经验与理解分数无显著相关性，表明系统降低了不同经验水平的理解障碍。

Conclusion: AI系统确实可以简化复杂的模型分析，但其真正潜力在于与以用户为中心的设计相结合，优先考虑交互性、特异性和叙事指导，从而降低理解门槛。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


### [55] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 创建了首个罗马尼亚语抑郁和焦虑心理语料库PsihoRo，包含205名受访者的文本数据，填补了罗马尼亚语心理健康NLP资源的空白。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语目前缺乏开源的心理健康语料库，而现有社交媒体数据收集方法存在假设偏差问题。需要更实用的数据收集策略来支持罗马尼亚语心理健康NLP研究。

Method: 通过包含6个开放式问题的问卷收集数据，配合标准化的PHQ-9和GAD-7筛查问卷。使用统计分析、罗马尼亚语LIWC文本分析、情感检测和主题建模等方法分析语料特征。

Result: 成功创建了包含205名受访者文本的PsihoRo语料库，这是首个罗马尼亚语抑郁和焦虑心理语料库，为罗马尼亚语心理健康文本分析提供了基础资源。

Conclusion: PsihoRo语料库是理解罗马尼亚人口心理健康文本的重要第一步，为罗马尼亚语NLP社区提供了有价值的心理健康分析资源，填补了该语言领域的空白。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [56] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara是一个针对印度司法系统的AI框架，用于预测和解释上诉判决，通过将案件文档分解为决策点，采用类似IRAC的解释格式，在多个数据集上超越了现有基准。


<details>
  <summary>Details</summary>
Motivation: 印度法院面临大量案件积压，特别是上诉案件，需要AI技术来帮助预测判决并提高司法效率。上诉案件是高等法院审查下级法院裁决的正式决定，处理这些案件对减轻司法负担至关重要。

Method: Vichara框架处理英文上诉案件程序文档，将其分解为决策点——包含法律问题、决定机构、结果、推理和时间背景的离散法律决定。采用结构化表示分离核心决定及其背景，解释格式基于IRAC框架并适应印度法律推理。

Result: 在PredEx和ILDC_expert两个数据集上评估，使用GPT-4o mini、Llama-3.1-8B、Mistral-7B和Qwen2.5-7B四个大语言模型。GPT-4o mini表现最佳（PredEx F1: 81.5，ILDC_expert F1: 80.3），超越现有判决预测基准。人类评估显示GPT-4o mini在清晰度、关联性和实用性方面解释能力最强。

Conclusion: Vichara框架为印度司法系统提供了有效的判决预测和解释解决方案，通过结构化决策点表示和适应性的解释格式，既提高了预测准确性，又增强了可解释性，有助于法律专业人员评估预测的合理性。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [57] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 该论文提出了一个双尺度验证框架，结合点对点和成对人工标注，用于评估政治立场预测等主观连续属性的知识表示，并在大规模辩论数据集上验证了语言模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识表示常涉及主观连续属性（如政治立场），这与广泛接受的成对验证黄金标准存在冲突。需要解决如何有效评估这类主观知识表示的问题。

Method: 采用双尺度验证框架，结合点对点（pointwise）和成对（pairwise）人工标注。使用22个语言模型对来自30场英国政治电视节目辩论的23,228个论点进行政治立场预测，构建大规模知识库。

Result: 点对点评估显示中等水平的人机一致性（Krippendorff's α=0.578），反映了内在主观性；而成对验证显示人机排名对齐显著更强（最佳模型α=0.86）。成功构建了经过验证的结构化论证知识库。

Conclusion: 该工作贡献包括：实用的主观连续知识验证方法；经过验证的结构化论证知识库支持基于图的推理和检索增强生成；证明可以从点对点语言模型预测中提取序数结构，推进了传统符号或分类方法不足领域的知识表示能力。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [58] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: VIRAASAT是一个针对印度文化的多跳问答数据集生成方法，通过知识图谱构建了3200多个多跳问题，并提出SCoM框架来提升LLMs在文化推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程等推理任务上表现出色，但在需要丰富社会文化知识和多样本地语境的任务中表现不佳，特别是在涉及印度文化的任务上。现有的文化基准测试存在三个主要问题：手工制作、只包含测试事实记忆的单跳问题、扩展成本过高，导致这一缺陷难以被有效衡量。

Method: 提出了VIRAASAT方法，这是一种半自动化的多跳方法，用于生成印度文化特定的多跳问答数据集。该方法利用包含700多个专家策划文化实体的知识图谱，涵盖印度文化的13个关键属性（历史、节日等）。此外，提出了符号化操作链（SCoM）框架，训练模型在内部模拟原子知识图谱操作，可靠地遍历图谱的拓扑结构。

Result: VIRAASAT覆盖了印度所有28个邦和8个中央直辖区，生成了3200多个需要链式文化推理的多跳问题。实验表明，当前最先进的LLMs在VIRAASAT上表现不佳，特别是在推理方面，即使对思维链轨迹进行微调也无法有效处理低概率事实。SCoM框架在监督微调实验中比标准思维链基线提升了高达20%的性能。

Conclusion: VIRAASAT数据集和SCoM框架为构建文化感知推理模型奠定了坚实基础，解决了LLMs在文化特定推理任务上的局限性，特别是在需要多跳推理和低概率事实合成的场景中。

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>
