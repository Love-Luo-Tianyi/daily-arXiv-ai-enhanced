<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 160]
- [cs.CL](#cs.CL) [Total: 80]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文开发了一种分层贝叶斯模型来量化鞋印中"偶然特征"的稀有性，通过潜在高斯模型和空间变化系数提高了法医鞋印分析的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在法医调查中，仅凭鞋印匹配鞋子的品牌型号通常不够，因为同款鞋子可能生产数千双。需要分析鞋底磨损产生的"偶然特征"（如划痕、切口）的稀有性来量化证据强度，但现有方法在这方面存在局限。

Method: 开发了分层贝叶斯模型，采用潜在高斯模型框架，通过集成嵌套拉普拉斯近似实现大规模标注鞋印数据的高效推理，并引入空间变化系数来建模鞋底花纹图案与偶然特征位置之间的关系。

Result: 在保留数据上表现出优于现有方法的性能，提高了法医鞋印分析的准确性和可靠性，能够更精确地量化偶然特征模式的稀有性。

Conclusion: 该方法通过先进的统计建模技术显著提升了法医鞋印证据的量化分析能力，为犯罪现场调查提供了更可靠的科学依据。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 该研究提出了MAU-Set数据集和MAU-GPT模型，用于解决工业制造中细粒度产品图像分析的自动化问题，通过多领域数据集和新型适应机制提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着工业制造规模扩大，自动化细粒度产品图像分析对质量控制变得至关重要。现有方法受限于数据集覆盖范围有限，以及模型在多样复杂异常模式上的泛化能力差。

Method: 1. 引入MAU-Set数据集，涵盖多个工业领域，具有从二元分类到复杂推理的分层任务结构；2. 建立严格的评估协议；3. 提出MAU-GPT模型，采用新颖的AMoE-LoRA机制，统一异常感知和通用专家适应，增强对不同缺陷类别的理解和推理能力。

Result: 大量实验表明，MAU-GPT在所有领域都持续优于先前的最先进方法，展示了在可扩展和自动化工业检测方面的强大潜力。

Conclusion: 该研究通过MAU-Set数据集和MAU-GPT模型，为工业异常理解提供了全面的解决方案，解决了现有方法的局限性，在工业质量控制的自动化图像分析方面具有重要应用价值。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [3] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM是一个通用的视网膜分割和量化框架，能够从眼底图像中分割多种解剖结构和病变，并提取标准化生物标志物，支持大规模眼科研究。


<details>
  <summary>Details</summary>
Motivation: 视网膜成像快速、无创且广泛可用，为眼科和全身健康评估提供了可量化的结构和血管信号。然而，由于公共多标签数据集的有限性以及缺乏统一的分割到量化流程，大规模分析仍然困难。

Method: RetSAM是一个通用的视网膜分割和量化框架，采用多阶段训练策略，使用私人和公共眼底数据进行训练。它支持三类任务，分割五种解剖结构、四种视网膜表型模式和20多种不同的病变类型，并将分割结果转化为30多种标准化生物标志物。

Result: RetSAM在17个公共数据集上实现了优异的分割性能，平均DSC比先前最佳方法提高了3.9个百分点，在具有挑战性的多任务基准上最高提高了15个百分点。该框架在不同人群、成像设备和临床环境中具有良好的泛化能力。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的定量表型，支持主要眼科疾病的系统性相关性分析，包括糖尿病视网膜病变、年龄相关性黄斑变性、青光眼和病理性近视，从而实现了大规模眼科研究和转化应用。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [4] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: CR-VLM是一种基于激活引导的可配置拒绝机制，用于视觉语言模型，通过教师强制机制提取可配置拒绝向量、门控机制防止过度拒绝，以及反事实视觉增强模块，实现用户自适应的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的拒绝机制通常是"一刀切"的，无法适应不同用户需求和上下文约束，导致要么拒绝不足要么过度拒绝。需要一种可配置的拒绝机制来实现更精细的安全控制。

Method: CR-VLM包含三个组件：1) 通过教师强制机制提取可配置拒绝向量以放大拒绝信号；2) 引入门控机制，通过保留对范围内查询的接受来减轻过度拒绝；3) 设计反事实视觉增强模块，将视觉表示与拒绝要求对齐。

Result: 在多个数据集和各种视觉语言模型上的综合实验表明，CR-VLM实现了有效、高效且鲁棒的可配置拒绝，为视觉语言模型中的用户自适应安全对齐提供了可扩展的路径。

Conclusion: CR-VLM提供了一种强大的可配置拒绝方法，能够更好地平衡安全性和实用性，为视觉语言模型的安全对齐提供了新的解决方案。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [5] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: Vectra是首个无参考、基于MLLM的电商图像内机器翻译视觉质量评估框架，包含多维质量度量系统、大规模数据集和4B参数模型，在人类排名相关性上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有电商图像内机器翻译研究中，视觉渲染质量对用户参与度至关重要，但当前基于参考的方法缺乏可解释性，而模型作为评判者的方法缺乏领域基础和细粒度奖励信号。

Method: 提出Vectra框架：1) Vectra Score - 将视觉质量分解为14个可解释维度，通过空间感知的缺陷面积比量化减少标注歧义；2) Vectra Dataset - 从110万真实产品图像构建，包含2K基准集、30K推理标注和3.5K专家偏好标注；3) Vectra Model - 4B参数MLLM，能生成量化分数和诊断推理。

Result: Vectra在人类排名相关性上达到最先进水平，其模型在评分性能上优于包括GPT-5和Gemini-3在内的领先MLLM。

Conclusion: Vectra填补了电商图像内机器翻译视觉质量评估的空白，通过无参考、可解释的多维评估框架，为领域提供了细粒度的质量信号，数据集和模型将在接受后发布。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [6] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出一种用于孟加拉国纸币识别的新型混合CNN架构，结合MobileNetV3-Large和EfficientNetB0进行特征提取，配合MLP分类器，在资源受限设备上实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 准确的纸币识别对辅助技术至关重要，特别是对依赖他人识别纸币的视障人士，这种依赖性使他们面临欺诈和剥削的风险。

Method: 1) 构建新的孟加拉国纸币数据集，包括受控和真实场景；2) 整合四个额外数据集增强鲁棒性；3) 提出结合MobileNetV3-Large和EfficientNetB0的混合CNN架构进行特征提取；4) 使用多层感知机(MLP)分类器；5) 采用五折交叉验证和七个评估指标；6) 集成LIME和SHAP等可解释AI方法。

Result: 模型在受控数据集上达到97.95%准确率，复杂背景上92.84%准确率，所有数据集组合上94.98%准确率。模型性能通过多种指标全面评估，并具备良好的可解释性。

Conclusion: 提出的混合CNN架构在孟加拉国纸币识别任务中表现出色，兼顾高准确率和计算效率，适合资源受限设备，同时通过可解释AI方法增强了透明度，有助于视障人士的金融独立。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [7] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 该论文研究了如何利用高斯约束表示改进无监督3D场景重建，特别是在图像来自多个无关场景且存在视觉模糊性的情况下。通过基于LeJEPA架构的三种渐进式管道，证明了高斯约束嵌入能提升场景分离和相机姿态估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化图像集合中进行无监督3D场景重建的挑战，特别是在图像来自多个无关场景、存在视觉模糊性和异常值的真实世界条件下。IMC2025挑战赛凸显了这些困难，需要同时进行场景发现和相机姿态估计。

Method: 提出了三种渐进式精炼的管道，最终采用受LeJEPA启发的方案，对学习到的图像嵌入施加各向同性高斯约束。该方法不是引入新的理论保证，而是经验性地评估这些约束如何影响聚类一致性和姿态估计的鲁棒性。

Result: 在IMC2025上的实验结果表明，与启发式基线相比，高斯约束嵌入能够改善场景分离和姿态合理性，特别是在视觉模糊性较高的场景中。

Conclusion: 理论驱动的表示约束为连接自监督学习原理和实际运动结构重建管道提供了有前景的方向，高斯约束嵌入在实践中能有效提升多场景重建的性能。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [8] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: XAI-CLIP：基于多模态视觉语言模型的ROI引导扰动框架，用于生成更清晰、边界感知的显著性图，显著提升医学图像分割的可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在医学图像分割中性能优越，但其有限的可解释性阻碍了临床信任和部署。现有的可解释人工智能技术计算成本高、需要多次前向传播，且常产生噪声大或解剖学无关的解释。

Method: 提出XAI-CLIP框架，利用多模态视觉语言模型嵌入定位临床相关的解剖区域，通过语言引导的区域定位与医学图像分割结合，应用目标明确的区域感知扰动，生成边界感知的显著性图。

Result: 在FLARE22和CHAOS数据集上的实验表明，XAI-CLIP实现了60%的运行时间减少、44.6%的Dice分数提升，以及基于遮挡的解释中96.7%的交并比提升，生成更干净、解剖学一致的归因图。

Conclusion: 将多模态视觉语言表示整合到基于扰动的XAI框架中，显著提升了医学图像分割系统的可解释性和效率，为实现透明且可临床部署的系统提供了有效途径。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [9] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 该研究通过分析开放权重视觉语言模型（Qwen、InternVL、Gemma）的表征几何，提出概念向量方法，揭示了模型在多物体视觉任务中产生幻觉和识别错误的内在机制。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多物体视觉任务中表现出令人困惑的失败，如幻觉不存在的元素或无法在干扰物中识别最相似的物体。这些错误反映了类似人类"绑定问题"的认知约束，但人工系统中驱动这些错误的内在机制仍不清楚。

Method: 通过分析开放权重VLMs（Qwen、InternVL、Gemma）的表征几何，比较提取"概念向量"的方法——编码视觉概念的潜在方向。通过转向干预验证概念向量，在简化和自然视觉任务中可靠地操纵模型行为。

Result: 观察到这些向量之间的几何重叠与特定错误模式强相关，为理解内部表征如何塑造模型行为和驱动视觉失败提供了基于量化的框架。

Conclusion: 该研究通过概念向量分析揭示了VLMs在多物体视觉任务中失败的内在机制，为理解模型内部表征如何影响行为提供了量化框架，有助于解释模型产生幻觉和识别错误的根本原因。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [10] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出ReVision框架，通过ReAlign方法解决多模态对比学习中的模态间隙问题，利用未配对数据统计信息对齐文本和图像表示，为MLLMs提供高效扩展路径


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习虽然成功对齐视觉和语言表示，但存在模态间隙问题：表达相同语义的不同模态嵌入占据系统偏移区域。先前方法受限于过度简化的各向同性假设，难以应用于大规模场景。

Method: 1. 提出固定框架模态间隙理论，将模态间隙分解为稳定偏差和各向异性残差；2. 提出ReAlign训练自由模态对齐策略，通过Anchor、Trace、Centroid Alignment三步将文本表示对齐到图像表示分布；3. 基于ReAlign提出ReVision可扩展训练范式，将ReAlign集成到预训练阶段，使模型能在视觉指令调优前从未配对文本学习视觉表示分布。

Result: 该框架证明统计对齐的未配对数据可以有效替代昂贵的图像-文本对，为多模态大语言模型的高效扩展提供了稳健路径。

Conclusion: 通过精确建模模态间隙的几何形状并利用大规模未配对数据统计信息，可以高效解决多模态表示对齐问题，为大规模多模态模型的扩展提供了新的训练范式。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [11] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: 提出Fair Context Learning (FCL)框架，通过公平性驱动的校准来解决CLIP等视觉语言模型在分布偏移下的性能下降问题，避免传统基于熵最小化的测试时适应方法导致的虚假相关性和过度自信错误。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本识别方面表现出色，但在分布偏移下性能显著下降。现有的基于提示的测试时适应方法主要依赖熵最小化，这种方法在类别共享视觉特征时会放大虚假相关性并导致过度自信的错误。

Method: 提出公平上下文学习（FCL）框架，采用情节式测试时适应方法。基于加性证据分解假设，将适应过程解耦为：(1) 基于增强的探索来识别合理的类别候选；(2) 公平性驱动的校准，通过调整文本上下文来平等化对常见视觉证据的敏感性，避免对部分特征的过度关注。

Result: 通过广泛的评估，实证验证了理论动机，并表明FCL在多种领域偏移和细粒度基准测试中，相对于最先进的测试时适应方法实现了有竞争力的适应性能。

Conclusion: FCL通过避免熵最小化并采用公平性约束来校准文本嵌入，有效缓解了共享证据偏差问题，为视觉语言模型的测试时适应提供了一种更稳健的方法。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [12] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: ANFIS增强的CNN在对抗攻击下的鲁棒性表现不一致：ResNet18-ANFIS有所改善，VGG-ANFIS反而下降，表明神经模糊增强并非普遍有效


<details>
  <summary>Details</summary>
Motivation: 虽然CNN在图像分类上表现优异，但缺乏可解释性且易受对抗攻击。神经模糊混合方法（如DCNFIS）用ANFIS替代CNN全连接层以提升可解释性，但其鲁棒性尚未充分研究

Method: 比较标准CNN（ConvNet、VGG、ResNet18）与其ANFIS增强版本在MNIST、Fashion-MNIST、CIFAR-10、CIFAR-100数据集上的表现，使用基于梯度的PGD攻击和无梯度的Square攻击进行鲁棒性评估

Result: ANFIS集成并未一致提升干净准确率，且对鲁棒性的影响具有架构依赖性：ResNet18-ANFIS表现出改善的对抗鲁棒性，而VGG-ANFIS通常表现不如其基线模型

Conclusion: 神经模糊增强可以在特定架构中提升鲁棒性，但并非普遍有益，需要针对具体架构进行仔细评估

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [13] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: UNIKIE-BENCH是一个统一基准，用于全面评估大型多模态模型在文档关键信息提取任务上的能力，包含约束类别和开放类别两个轨道，揭示了现有模型在多样化模式定义、长尾关键字段和复杂布局下的性能挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界文档的关键信息提取面临布局结构差异大、视觉质量不一和任务特定信息需求多样化的挑战。虽然大型多模态模型在端到端KIE方面显示出潜力，但缺乏全面系统的评估基准来测试其在多样应用场景下的实际能力。

Method: 提出了UNIKIE-BENCH统一基准，包含两个互补轨道：1) 约束类别KIE轨道，基于场景预定义模式反映实际应用需求；2) 开放类别KIE轨道，提取文档中明确存在的任何关键信息。在15个最先进的大型多模态模型上进行实验评估。

Result: 实验显示，在多样化模式定义、长尾关键字段和复杂布局下，模型性能显著下降。不同文档类型和场景之间存在明显的性能差异。这些发现突显了基于LMM的KIE在基础准确性和布局感知推理方面面临的持续挑战。

Conclusion: UNIKIE-BENCH为评估大型多模态模型的关键信息提取能力提供了全面基准。实验结果揭示了现有模型在实际应用场景中的局限性，特别是在处理多样化模式、长尾字段和复杂布局时。该基准有助于推动更鲁棒、布局感知的KIE模型发展。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [14] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent是一个数据高效且可解释的牙科诊断框架，通过将临床推理原则融入视觉语言模型，利用智能手机多视角照片进行牙齿级评估，无需牙科特定微调。


<details>
  <summary>Details</summary>
Motivation: 当前牙科诊断存在两个主要问题：1）许多人缺乏及时的专业评估机会；2）现有AI方法仅将诊断视为视觉模式识别任务，未反映牙科专业人员的结构化临床推理，且需要大量专家标注数据，在多样化真实世界成像条件下泛化能力有限。

Method: OMNI-Dent框架将牙科专家的诊断启发式方法嵌入到视觉语言模型（VLM）管道中，基于多视角智能手机照片进行操作，引导通用VLM进行牙齿级评估，无需对VLM进行牙科特定微调，利用VLM现有的视觉-语言能力。

Result: 该框架旨在支持在缺乏精选临床影像的环境中进行诊断评估，作为早期辅助工具，帮助用户识别潜在异常并确定何时需要专业评估，为缺乏面对面护理机会的个人提供实用选择。

Conclusion: OMNI-Dent通过将临床推理原则融入VLM管道，提供了一种数据高效、可解释的牙科诊断方法，能够利用通用VLM的能力在资源有限的环境中支持早期诊断评估。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [15] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: COMBOOD是一个用于图像识别中离群检测的无监督半参数框架，通过结合最近邻和马氏距离两种信号，在近离群和远离群场景下都能提供准确的置信度评分。


<details>
  <summary>Details</summary>
Motivation: 在推理时识别离群数据对许多机器学习应用至关重要，特别是自动化应用。现有方法在近离群场景下表现不佳，需要一种能同时处理近离群和远离群情况的鲁棒方法。

Method: COMBOOD是一个半参数框架，结合了最近邻（非参数方法）和马氏距离（参数方法）两种距离度量信号。最近邻方法提供非参数离群检测，马氏距离方法在远离群场景下简单有效。框架将这两种信号结合生成置信度评分。

Result: 在OpenOOD（1.0和1.5版本）基准数据集和文档数据集上，COMBOOD在准确率上优于最先进的离群检测方法，包括近离群和远离群场景。在大多数基准数据集上，准确率提升具有统计显著性。框架的复杂度与嵌入空间大小呈线性关系。

Conclusion: COMBOOD框架通过结合最近邻和马氏距离信号，提供了一种有效的半参数离群检测方法，在近离群和远离群场景下都表现出色，适用于许多实际应用场景。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [16] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: PipeMFL-240K：首个大规模公开的管道漏磁检测数据集与基准，包含24万张图像和19万标注，用于解决漏磁图像中的复杂目标检测挑战


<details>
  <summary>Details</summary>
Motivation: 管道完整性对工业安全和环境保护至关重要，漏磁检测是主要无损检测技术。尽管深度学习在自动化漏磁解释方面有前景，但缺乏大规模公开数据集和基准限制了可靠模型的开发，使得公平比较和可重复评估变得困难。

Method: 构建了PipeMFL-240K数据集，包含240,320张图像和191,530个高质量边界框标注，收集自11条总长约1,480公里的管道。该数据集反映了真实检测复杂性，具有三个独特挑战：1）12个类别的极端长尾分布；2）大量仅占几个像素的微小目标；3）显著的类内变异性。

Result: 通过最先进的目标检测器进行广泛实验建立基线。结果表明，现代检测器在处理漏磁数据固有特性方面仍然存在困难，显示出巨大的改进空间。PipeMFL-240K为未来研究提供了可靠且具有挑战性的测试平台。

Conclusion: 作为首个公开的、具有如此规模和范围的管道漏磁检测数据集和基准，PipeMFL-240K为高效管道诊断和维护规划提供了关键基础，有望加速基于漏磁的管道完整性评估的算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [17] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: VLRS-Bench是首个专门针对遥感复杂推理的基准测试，包含2000个问答对，涵盖认知、决策、预测三个维度，揭示了现有MLLMs在遥感推理任务上的显著瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准测试主要偏向感知任务（如目标识别和场景分类），这限制了多模态大语言模型在认知要求高的遥感应用中的发展，需要专门的复杂推理基准来推动该领域进步。

Method: 提出VLRS-Bench基准测试，包含2000个问答对，平均长度71词，涵盖14个任务和最多8个时间阶段。采用专门构建流程，整合遥感特定先验知识和专家知识，确保地理空间真实性和推理复杂性。

Result: 实验结果显示现有最先进的多模态大语言模型在遥感复杂推理任务上存在显著瓶颈，为遥感社区推进多模态推理提供了关键见解。

Conclusion: VLRS-Bench填补了遥感领域复杂推理基准测试的空白，揭示了现有MLLMs的局限性，为未来遥感多模态推理研究提供了重要基础和方向。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [18] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: ShapBPT是一种基于分层Shapley公式的新型数据感知XCV方法，通过将Shapley系数分配给为图像定制的多尺度分层结构（BPT），使特征归因与图像形态对齐，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的分层Shapley方法未能利用图像数据的多尺度结构，导致收敛慢且与真实形态特征对齐弱；同时缺乏针对计算机视觉任务的数据感知层次结构，存在模型可解释性空白。

Method: 提出ShapBPT方法，基于分层Shapley公式，将Shapley系数分配给专门为图像设计的二进制分区树（BPT）多尺度分层结构，通过数据感知的分层分区确保特征归因与内在图像形态对齐。

Result: 实验结果表明ShapBPT有效，相比现有XCV方法具有更好的图像结构对齐性和计算效率，20人用户研究证实人类更偏好ShapBPT的解释。

Conclusion: ShapBPT将分层Shapley方法与图像数据连接起来，为视觉可解释性提供了更高效、语义更丰富的方法，填补了结构化视觉数据模型可解释性的空白。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [19] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 提出ECHWR训练框架，通过临时辅助分支和双重对比目标提升基于IMU的在线手写识别性能，不增加推理成本，在OnHW-Words500数据集上显著降低字符错误率。


<details>
  <summary>Details</summary>
Motivation: 边缘硬件上的在线手写识别面临内存限制，需要在保持推理效率的同时提升识别精度。现有方法在特征表示和识别准确性方面仍有改进空间。

Method: 提出ECHWR训练框架：1）使用临时辅助分支在训练阶段对齐传感器信号与语义文本嵌入；2）采用双重对比目标：批内对比损失用于模态对齐，新颖的错误对比损失区分正确信号与合成困难负样本；3）训练后丢弃辅助分支，保持原始高效架构。

Result: 在OnHW-Words500数据集上显著优于现有方法：在作者独立划分上字符错误率降低7.4%，在作者依赖划分上降低10.4%。错误对比损失在处理未见书写风格方面表现出有效性。

Conclusion: ECHWR框架在不增加推理成本的情况下提升了手写识别性能，错误对比损失对处理未知书写风格特别有效，但解决特定挑战需要特定的架构和目标配置。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [20] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 研究发现现代视频编码器通过分布式而非分解式表示物理变量，存在"物理涌现区"的中间层过渡，其中方向信息以高维圆形几何结构编码


<details>
  <summary>Details</summary>
Motivation: 探究视频模型是否依赖分解式物理变量表示来做出准确物理预测，还是采用任务特定的分布式表示，并首次直接研究大规模视频编码器内部的物理表示

Method: 使用分层探测、子空间几何分析、补丁级解码和针对性注意力消融等方法，分析视频编码器内部物理信息的可访问性和组织方式

Result: 发现所有架构中都存在"物理涌现区"的中间层过渡，物理变量在此变得可访问；标量信息早期即可获得，而方向信息仅在涌现区出现，且以高维圆形几何结构编码

Conclusion: 现代视频模型不使用经典物理引擎的分解式物理变量表示，而是采用分布式表示，但这种表示足以进行物理预测

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [21] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: Neural Sentinel使用视觉语言模型统一实现车牌识别、状态分类和车辆属性提取，单次前向传播完成多项任务，相比传统多阶段ALPR系统显著提升准确率并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR系统采用多阶段流水线（目标检测+OCR模块），存在误差累积、延迟增加和架构复杂的问题。需要一种更高效、更准确且能处理多任务的统一解决方案。

Method: 采用基于PaliGemma 3B的视觉语言模型，通过LoRA进行微调，实现单次前向传播同时回答多个关于车辆图像的视觉问题。引入人机协同持续学习框架，通过经验回放防止灾难性遗忘。

Result: 车牌识别准确率达到92.3%，比EasyOCR提升14.1%，比PaddleOCR提升9.9%。平均推理延迟152ms，预期校准误差0.048。零样本泛化到车辆颜色检测(89%)、安全带检测(82%)和乘员计数(78%)等辅助任务。

Conclusion: 统一的视觉语言方法代表了ALPR系统的范式转变，提供了优于传统流水线方法的准确性、降低的架构复杂性以及新兴的多任务能力。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [22] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 该论文提出了一种无标记神经导航方法，使用低成本可见光和红外光相机结合立体和深度感知，通过面部几何建模替代传统标记系统，验证显示其精度足以用于经颅磁刺激。


<details>
  <summary>Details</summary>
Motivation: 传统神经导航系统依赖主体安装的标记，需要手动配准，可能在过程中移位，并引起不适。这些系统成本高且复杂，限制了神经导航在临床和研究环境中的可及性。

Method: 采用无标记方法，使用低成本可见光和红外光相机，结合立体视觉和深度感知技术，通过算法对面部几何进行建模，替代昂贵的硬件和物理标记。

Result: 在50名人类受试者上的验证显示，最佳无标记算法与传统标记系统相比，中位跟踪误差仅为2.32毫米和2.01度，精度足以用于经颅磁刺激，且比先前无标记结果有显著改进。

Conclusion: 提出的无标记神经导航方法可以降低设置成本和复杂性，提高患者舒适度，并扩大神经导航在临床和研究环境中的可及性。不同相机传感器数据的整合可以进一步提高整体精度。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [23] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: RECITYGEN是一个结合潜在扩散模型和交互式语义分割的工具，允许用户通过文本提示交互式创建城市环境的变分街景图像，用于参与式城市设计。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法往往忽视公众意见，导致设计愿景与现实之间存在差距。数字工具如城市信息建模和增强现实虽然促进了更多利益相关者参与，但仍需进一步降低设计生成门槛。

Method: 结合最先进的潜在扩散模型与交互式语义分割技术，开发了RECITYGEN工具，用户可以通过文本提示交互式生成城市街景的变分图像。

Result: 在北京的试点项目中，用户使用RECITYGEN为正在进行的城市更新项目提出改进建议。尽管存在一些局限性，但该工具显示出与公众偏好高度一致的潜力。

Conclusion: RECITYGEN展示了向更动态、包容的城市规划方法转变的潜力，通过降低设计生成门槛，使更多利益相关者能够参与城市设计过程。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [24] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: FADE是一种用于文本到图像扩散模型的快速数据擦除方法，通过参数定位和自蒸馏实现轻量级、可逆的概念遗忘，在保持整体性能的同时有效移除特定数据影响。


<details>
  <summary>Details</summary>
Motivation: 随着数据保护法规和负责任AI实践的要求，需要从训练模型中移除特定数据或概念的影响。当前文本到图像扩散模型的遗忘方法面临计算成本高、难以平衡有效遗忘与无关概念保留的挑战。

Method: FADE采用两阶段遗忘方法：1) 基于梯度显著性的参数定位，通过稀疏LoRA适配器约束更新；2) 自蒸馏目标，用用户定义的替代概念覆盖遗忘概念，同时保留对保留数据的行为。

Result: 在UnlearnCanvas基准测试和多个数据集上的消融研究表明，FADE实现了最先进的遗忘性能，在遗忘-保留权衡方面具有细粒度控制，在不同领域都表现出强大的概念擦除能力和高保留性。

Conclusion: FADE提供了一种内存高效、可逆且可在运行时合并或移除的适配器解决方案，适用于扩散基图像生成模型的选择性遗忘，是生产系统中灵活部署的合适方案。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [25] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 开发了一个计算机视觉系统，通过分析铁路车厢卸载过程中的图像，自动评估废钢中的非金属夹杂物污染程度并分类废钢类型，减少人工检查的主观性和危险性。


<details>
  <summary>Details</summary>
Motivation: 废钢质量直接影响钢铁制造的能源消耗、排放和安全。目前通过人工视觉检查非金属夹杂物污染程度的方法存在主观性强、危险性高（粉尘和移动机械）的问题。

Method: 将污染评估建模为铁路车厢级别的回归任务，采用多实例学习（MIL）和多任务学习（MTL）处理序列数据。系统包括磁铁/车厢检测分割时间层、版本化推理服务生成置信度评分、操作员结构化覆盖审查，以及主动学习循环持续改进。

Result: MIL方法达到MAE 0.27和R² 0.83；MTL设置达到MAE 0.36，废钢分类F1分数0.79。系统已集成到验收工作流程中，实现近实时处理。

Conclusion: 该管道减少了主观变异性，提高了人员安全性，并能够集成到验收和熔炼计划工作流程中，通过主动学习实现持续改进。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [26] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR是一个双向奖励引导的扩散框架，通过奖励反馈学习将超分辨率建模为轨迹级偏好优化，联合利用合成LR-HR对和真实世界LR图像，在保持结构一致性的同时提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率方法能合成丰富细节，但在合成配对数据上训练的模型往往无法适应真实世界LR图像的分布偏移。需要一种既能利用合成数据又能适应真实世界图像的方法。

Method: 提出Bird-SR框架：1）早期扩散步骤直接在合成对上优化以保持结构保真度；2）后期采样步骤对合成和真实LR图像应用质量引导奖励；3）采用相对优势空间和语义对齐约束防止奖励黑客；4）动态保真度-感知权重策略平衡结构保持和感知优化。

Result: 在真实世界超分辨率基准测试中，Bird-SR在感知质量方面持续优于最先进方法，同时保持结构一致性，验证了其对真实世界超分辨率的有效性。

Conclusion: Bird-SR通过奖励反馈学习和双向优化策略，成功解决了合成数据训练的扩散模型在真实世界图像上的分布偏移问题，实现了结构保真度和感知质量的平衡提升。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [27] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: MosaicThinker是一种推理时计算技术，通过将多帧视频的空间信息整合到统一的全局语义地图中，增强小型视觉语言模型在跨帧空间推理任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 随着具身AI从传统物体检测扩展到机器人操作和动作规划等高级任务，需要从视频输入中进行视觉空间推理来感知物体空间关系并指导设备动作。然而，现有视觉语言模型由于缺乏3D空间信息知识，在空间推理方面能力很弱，特别是在涉及跨多帧复杂空间关系的推理任务上。

Method: 提出MosaicThinker技术，将多帧视频中的碎片化空间信息整合到统一的全局语义地图表示中，然后通过视觉提示引导视觉语言模型在语义地图上进行空间推理。

Result: 实验结果表明，该技术能显著提高资源受限的具身AI设备在跨帧空间推理任务上的准确性，适用于多种类型和复杂度的推理任务。

Conclusion: MosaicThinker通过整合多帧空间信息到统一语义地图并利用视觉提示引导推理，有效增强了小型视觉语言模型在跨帧空间推理任务上的能力，为资源受限的具身AI设备提供了有效的解决方案。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [28] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: WorldEdit数据集专注于解决图像编辑模型处理隐式指令的难题，通过真实世界因果逻辑指导的编辑样本提升模型的世界知识和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在处理显式指令（如属性操作、风格迁移）方面表现出色，但在处理隐式指令时面临挑战。隐式指令描述视觉变化的原因而不明确说明结果，需要复杂的世界知识和推理能力，而现有模型缺乏这种能力。

Method: 1. 构建WorldEdit数据集，包含高质量编辑样本，使用符合真实世界因果逻辑的转述指令指导；2. 提供WorldEdit-Test用于评估模型在因果编辑场景的表现；3. 采用两阶段训练框架微调Bagel等模型，结合因果验证奖励机制。

Result: 提出的数据集和方法显著缩小了与GPT-4o和Nano-Banana的差距，在指令遵循和知识合理性方面表现出竞争力，而开源系统通常在这些方面表现不佳。

Conclusion: WorldEdit数据集和相应方法有效解决了图像编辑模型处理隐式指令的局限性，通过真实世界因果逻辑驱动的训练框架提升了模型的世界知识和推理能力，为更智能的图像编辑系统奠定了基础。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [29] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: TLC-Plan提出了一种分层生成模型，直接合成矢量平面图，避免了传统方法在栅格空间操作和后处理矢量化带来的结构不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在栅格空间操作并依赖后处理矢量化，导致结构不一致并阻碍端到端学习。受组合空间推理启发，需要开发与人类建筑设计流程一致的矢量平面图生成方法。

Method: 采用两级VQ-VAE编码全局布局（语义标记的房间边界框）和细化局部几何（多边形级编码），通过CodeTree统一表示，使用自回归变换器在边界条件下采样代码生成多样且拓扑有效的设计。

Result: 在RPLAN数据集上取得最先进性能（FID = 1.84，MSE = 2.06），在LIFULL数据集上也取得领先结果，无需显式房间拓扑或维度先验。

Conclusion: 该框架推进了面向实际建筑应用的约束感知和可扩展矢量平面图生成，与人类建筑设计流程保持一致，基于模块化和可重用模式。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [30] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 提出一种基于可重光照3D高斯溅射的端到端强化学习框架，用于无人机在非结构化室外环境中的零样本导航，解决了模拟与真实世界之间的视觉域差距问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在非结构化室外环境中的单目视觉导航面临模拟与现实之间的视觉域差距问题。现有3D高斯溅射方法将静态光照与几何结构耦合，限制了策略对动态真实世界光照的泛化能力。

Method: 提出可重光照3D高斯溅射技术，分解场景组件以实现对神经表示中环境光照的显式、物理基础编辑。在高保真模拟中训练端到端强化学习策略，通过多样化的合成光照条件（从强方向性阳光到漫射阴天）增强训练，迫使策略学习鲁棒的、光照不变的视觉特征。

Result: 真实世界实验表明，轻量级四旋翼无人机在复杂森林环境中以高达10米/秒的速度实现鲁棒、无碰撞导航，对剧烈光照变化表现出显著韧性，无需微调。

Conclusion: 该方法通过可重光照神经表示和光照增强训练，有效解决了模拟到现实的视觉域差距问题，实现了无人机在动态光照条件下的零样本导航泛化。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [31] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: Ex-Omni是一个开源的全模态框架，通过解耦语义推理和时序生成，将语音伴随的3D面部动画集成到全模态大语言模型中，解决了离散语义推理与密集时序动态之间的表示不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型旨在统一多模态理解和生成，但将语音与3D面部动画结合的研究仍然不足，这对于自然交互至关重要。主要挑战在于LLMs的离散、token级语义推理与3D面部运动所需的密集、细粒度时序动态之间存在表示不匹配，使得在有限数据下直接建模难以优化。

Method: 提出Ex-Omni框架，通过解耦语义推理和时序生成来降低学习难度：1) 利用语音单元作为时序支架；2) 使用统一的token-as-query门控融合机制进行受控语义注入；3) 引入InstructEx数据集来增强OLLMs的语音伴随3D面部动画能力。

Result: 大量实验表明，Ex-Omni在性能上与现有开源OLLMs竞争相当，同时能够稳定生成对齐的语音和面部动画。

Conclusion: Ex-Omni成功解决了将语音伴随3D面部动画集成到全模态大语言模型中的挑战，通过解耦语义推理和时序生成的方法，实现了稳定对齐的多模态生成，为自然交互提供了有效解决方案。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [32] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 研究发现在LAION-400M数据集中存在大量包含敏感个人信息的孕期超声图像，这些图像可能被用于重新识别或冒充身份，提出了数据隐私保护和伦理使用的建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的兴起，大规模互联网数据集的使用日益普遍，但往往缺乏数据筛选，这引发了包含敏感或私人信息的担忧。本研究特别关注孕期超声图像这类包含敏感个人信息且常被在线分享的内容。

Method: 通过系统性地检查LAION-400M数据集，使用CLIP嵌入相似性检索包含孕期超声的图像，并检测其中的私人信息实体（如姓名和位置）。

Result: 研究发现数千个包含私人信息的实体，多个图像包含高风险信息，可能被用于重新识别或冒充身份。

Conclusion: 研究提出了数据集筛选、数据隐私保护和公共图像数据集伦理使用的建议实践。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [33] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: DuMeta++是一个无需配对纵向数据的双元学习框架，用于跨年龄段的脑组织MRI分割，通过元特征学习和元初始化学习提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑组织MRI分割在神经科学和临床应用中至关重要，但由于大脑外观和形态随年龄变化的动态性，实现跨人类生命周期的稳定性能具有挑战性。现有方法通常依赖配对纵向数据进行自监督正则化，但这类数据在实践中往往难以获得。

Method: 提出DuMeta++双元学习框架：1）元特征学习提取与年龄无关的时空演化脑结构语义表示；2）元初始化学习实现数据高效的分割模型适应；3）基于记忆库的类感知正则化策略，无需显式纵向监督即可强制纵向一致性。理论证明了算法的收敛性。

Result: 在iSeg-2019、IBIS、OASIS、ADNI等多个数据集上的少样本设置实验中，DuMeta++在跨年龄泛化方面优于现有方法。

Conclusion: DuMeta++无需配对纵向数据即可实现跨年龄段的脑组织分割，通过双元学习和类感知正则化有效解决了年龄相关变化带来的挑战，为脑影像分析提供了更实用的解决方案。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [34] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 该研究提出使用视角不变语义特征作为条件输入，以解决传统全头3D GAN中因使用视角角度作为条件导致的生成偏差问题，从而提高3D头部生成的全局一致性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统全头3D GAN使用视角角度作为条件输入，导致学习的3D头部空间存在视角方向偏差，造成条件视角与非条件视角之间生成质量和多样性的显著差异，以及不同头部区域的全局不一致性。

Method: 提出使用视角不变语义特征作为条件输入，通过FLUX.1 Kontext扩展现有高质量正面人脸数据集到多视角，使用正面视角提取的图像clip特征作为所有视角的共享语义条件，确保语义对齐并消除方向偏差。

Result: 在全头合成和单视角GAN反转实验中，该方法在保真度、多样性和泛化能力方面均取得显著提升，生成的3D头部具有更好的全局一致性。

Conclusion: 使用视角不变语义条件能够有效解耦3D头部的生成能力与视角方向，消除传统视角条件导致的偏差，促进持续学习和多样化生成，提高3D GAN训练的稳定性和生成质量。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [35] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: RoadSafe365是一个大规模视觉语言基准数据集，用于细粒度交通安全性分析，包含36,196个标注视频片段和864K候选选项，旨在连接官方安全标准与数据驱动的交通理解系统。


<details>
  <summary>Details</summary>
Motivation: 现有交通基准缺乏与官方安全标准的系统性对齐评估，需要填补这一空白，以支持更精细的交通安全性分析。

Method: 构建大规模、多样化的真实世界视频数据集，采用分层分类法系统组织，扩展事故、事件和违规的基础定义，提供丰富的属性标注和多选题问答集。

Result: 建立了包含36,196个标注片段、864K候选选项、8.4K唯一答案和36K详细场景描述的数据集，通过微调实验显示一致性能提升，跨域实验验证了有效性。

Conclusion: RoadSafe365为大规模训练和标准化评估提供了全面基准，可推进真实世界交通安全性分析的可重复研究。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [36] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: AdvSR框架展示了对抗性行为可以直接嵌入超分辨率模型权重中，在推理时无需访问输入，通过联合优化重建质量和目标对抗结果，产生看似良性但能诱导下游错误分类的模型。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的超分辨率方法常作为预处理步骤集成到成像管道中，以提高分类和检测等下游任务性能。然而，这些SR模型引入了以前未探索的攻击面。本文旨在研究如何将对抗行为直接嵌入SR模型权重中，形成模型级威胁。

Method: 提出AdvSR框架，在训练期间将对抗行为直接嵌入SR模型权重中，无需在推理时访问输入。通过联合优化重建质量和目标对抗结果，使模型在标准图像质量指标下看似良性，同时诱导下游错误分类。评估了三种SR架构（SRCNN、EDSR、SwinIR）与YOLOv11分类器的组合。

Result: AdvSR模型能够实现高攻击成功率，同时保持最小的质量退化。这表明模型级威胁在成像管道中是真实存在的，对安全关键应用中模型的来源和验证具有重要影响。

Conclusion: AdvSR框架揭示了成像管道中新的模型级威胁，强调了在安全关键应用中，从业者需要重新考虑如何获取和验证模型，因为看似良性的SR模型可能隐藏着对抗性行为。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [37] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 3D-TBM是一个基于传输的形态测量工具，用于3D医学图像分析，通过可逆变换将图像嵌入传输域，支持分类、回归等任务，并能将分析结果投影回原始图像空间进行空间解释。


<details>
  <summary>Details</summary>
Motivation: 促进传输基形态测量（TBM）在临床影像研究中的更广泛采用，为研究人员提供专门用于3D医学图像形态分析的工具。

Method: 开发了3D-TBM框架，包括数据预处理、最优传输嵌入计算、可视化主要传输方向的分析方法，以及识别区分方向和相关的分析技术。

Result: 提供了完整的3D-TBM工具，包含全面的文档和实践教程，源代码通过PyTransKit公开可用。

Conclusion: 3D-TBM为医学影像研究人员提供了一个强大的传输基形态测量工具，支持临床特征的直接空间解释，有助于推进3D医学图像分析研究。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [38] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: TwistNet-2D是一个轻量级模块，通过局部成对通道乘积和方向性空间位移来编码纹理特征，在保持低计算成本的同时超越了更大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有纹理识别方法存在根本性矛盾：双线性池化和Gram矩阵捕捉全局通道相关性但破坏了空间结构，而自注意力通过加权聚合建模空间上下文而非显式的成对特征交互。

Method: 提出TwistNet-2D模块，核心是螺旋扭曲通道交互（STCI）：将特征图沿预定方向位移后进行逐通道元素乘法，捕捉结构化纹理的跨位置共现模式。通过四个方向头聚合，使用学习通道重加权，并通过sigmoid门控残差路径注入结果。

Result: TwistNet仅增加ResNet-18 3.5%参数和2% FLOPs，但在四个纹理和细粒度识别基准上持续超越参数匹配和更大的基线模型，包括ConvNeXt、Swin Transformer和混合CNN-Transformer架构。

Conclusion: TwistNet-2D通过局部成对通道交互有效编码纹理特征，在保持轻量级的同时实现了优于现有方法的性能，解决了纹理识别中全局相关性与空间结构保持的矛盾。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [39] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: VideoNeuMat从视频扩散模型中提取可重用的神经材质资产，通过两阶段流程：1）微调视频模型生成受控相机和光照轨迹下的材质样本视频；2）从视频重建紧凑的神经材质参数，实现新视角和光照条件下的泛化。


<details>
  <summary>Details</summary>
Motivation: 创建逼真的3D渲染材质需要高超的艺术技能，而现有的生成模型受限于高质量训练数据的缺乏。虽然视频生成模型能轻松产生逼真的材质外观，但这些知识仍与几何和光照纠缠在一起。

Method: 提出两阶段流程：1）微调大型视频模型（Wan 2.1 14B）生成受控相机和光照轨迹下的材质样本视频，创建"虚拟测角反射计"；2）通过从较小视频骨干微调的大型重建模型（LRM），从17个生成视频帧中单次推理预测神经材质参数。

Result: 生成的材质在真实感和多样性方面远超有限的合成训练数据，证明材质知识可以从互联网规模的视频模型成功转移到独立的、可重用的神经3D资产中。

Conclusion: VideoNeuMat成功从视频扩散模型中提取了可重用的神经材质资产，实现了材质知识与几何和光照的解耦，为3D渲染提供了高质量、可泛化的材质资源。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [40] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: XVWM通过跨视角预测目标训练世界模型，使智能体能够从不同视角（如鸟瞰图）进行规划，同时保持自我中心视角执行，利用多视角一致性作为几何正则化学习环境3D结构


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常只从单一自我中心视角操作，即使其他视角（如鸟瞰图）在某些任务（如导航）中能提供更好的规划效果。需要开发能够跨视角进行预测和规划的世界模型

Method: 提出跨视角世界模型（XVWM），使用跨视角预测目标进行训练：给定一个视角的帧序列，预测采取行动后从相同或不同视角的未来状态。利用Aimlabs平台提供的同步多视角游戏数据，包含精确对齐的多摄像头记录和高频动作标签

Result: 模型能够为智能体提供跨视角的并行想象流，使智能体能够在最适合任务的参考系中进行规划，同时从自我中心视角执行。多视角一致性为空间基础表示提供了强大的学习信号

Conclusion: 跨视角世界模型通过几何正则化学习环境3D结构，为智能体提供灵活的规划能力。从他人视角预测行动后果可能为多智能体环境中的视角采择奠定基础

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [41] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的DeepLab-V3+模型，用于糖尿病视网膜病变（DR）相关病变的像素级分割，在DDR数据集上显著提升了病变检测性能，特别是对微动脉瘤的检测有临床意义的改进。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是导致视力丧失和失明的眼病，早期筛查至关重要。尽管已有许多基于深度学习的自动筛查算法，但在病变分割方面的临床应用仍然有限。本研究旨在提供像素级病变标注，支持眼科医生从眼底图像中筛查DR。

Method: 在757张DDR数据集图像上分割四种DR相关病变：微动脉瘤、软性渗出物、硬性渗出物和出血。将注意力机制与DeepLab-V3+模型集成，以增强病变分割能力。

Result: 与基线模型相比，Attention-DeepLab模型将平均精度（mAP）从0.3010提高到0.3326，平均交并比（IoU）从0.1791提高到0.1928。微动脉瘤检测从0.0205显著提升到0.0763，这是临床上的重要改进，因为微动脉瘤是DR最早可见的症状。

Conclusion: 集成注意力机制的DeepLab-V3+模型在DR病变分割方面表现出显著改进，特别是对微动脉瘤的检测能力有临床意义的提升，这有助于早期DR筛查和预防不可逆的视力丧失。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [42] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 该研究开发了一种基于线性遗传编程的过滤和分割算法，用于检测增材制造铌基铜合金显微图像中的析出物，解决了传统手动标注效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前增材制造铌基铜合金分析依赖人工标注显微图像，但由于图像对比度变化、噪声和伪影等问题，标注过程缓慢，阻碍了合金开发的迭代速度。

Method: 采用线性遗传编程优化过滤和分割算法，使用特定领域语言构建图像处理流程，通过遗传算法迭代优化由可调参数图像过滤块组成的处理管道。

Result: 在理想条件下（种群大小60，最大程序长度5个块），系统找到了接近人工精度的解决方案，平均评估误差为1.8%，处理3.6百万像素图像仅需约2秒。

Conclusion: 该自动化方法显著加快了迭代周期，促进了材料成分和加工空间的探索，最终有助于开发用于增材制造聚变反应堆部件的强韧、低活化、沉淀硬化铜合金。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [43] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: LUCID是一种统一的视觉-语言稀疏自编码器，通过学习共享的潜在字典来对齐图像块和文本标记表示，实现跨模态的可解释概念发现。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器按模态单独训练，产生的特征字典不可直接理解且解释无法跨域迁移。需要一种统一的方法来学习可解释的跨模态表示。

Method: 提出LUCID框架：1）学习图像块和文本标记表示的共享潜在字典，同时保留模态特定细节的私有容量；2）通过学习的优化传输匹配目标耦合共享代码实现特征对齐；3）开发基于术语聚类的自动字典解释流程。

Result: LUCID产生可解释的共享特征，支持块级定位，建立跨模态神经元对应关系，增强对相似性评估中概念聚类问题的鲁棒性。共享特征捕获了超越对象的多样化语义类别，包括动作、属性和抽象概念。

Conclusion: LUCID提供了一种全面的可解释多模态表示方法，通过学习统一的视觉-语言稀疏代码实现跨模态概念发现和解释，无需人工标注即可实现特征对齐和自动解释。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [44] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 提出了一种基于改进U-Net和行列分离注意力模块的低光图像/视频增强方法，通过全局信息指导局部信息，减少参数计算，并引入时间一致性损失函数。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net结构在低光增强中缺乏全局信息指导，导致局部噪声大、细节丢失；注意力机制能更好利用全局信息但参数和计算量大。

Method: 1. 改进U-Net结构；2. 提出行列分离注意力模块(RCSA)，输入特征图行列的均值和最大值，以较少参数利用全局信息指导局部信息；3. 针对视频增强提出两种时间损失函数保持时间一致性。

Result: 在LOL、MIT Adobe FiveK图像数据集和SDSD视频数据集上的大量实验证明了该方法的有效性。

Conclusion: 提出的URCSA方法通过行列分离注意力模块有效利用全局信息指导低光增强，减少参数计算，并在视频增强中保持时间一致性，代码已开源。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [45] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 该论文提出了一种从深度和法线图重建3D表面的方法，通过透视感知的对数深度融合技术，解决了现有正交梯度方法在透视投影下的精度问题，并利用法线信息填补深度缺失区域。


<details>
  <summary>Details</summary>
Motivation: 现有基于正交投影的深度-法线融合方法在处理透视相机获取的数据时存在精度问题，无法产生度量准确的3D重建。同时，深度测量中常存在缺失区域，需要有效的方法来填补这些空白。

Method: 提出透视感知的对数深度融合方法，扩展了现有的正交梯度深度-法线融合方法，明确考虑透视投影效应。该方法利用表面法线信息来填补深度测量中的缺失区域，实现更完整的3D重建。

Result: 在DiLiGenT-MV数据集上的实验证明了该方法的有效性，结果显示透视感知的深度-法线融合能够产生度量准确的3D重建，显著优于不考虑透视投影的方法。

Conclusion: 透视感知的深度-法线融合对于从单视角相机系统获取的深度和法线图进行准确3D表面重建至关重要，该方法能够处理透视投影效应并填补深度缺失，实现高质量的度量重建。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [46] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: PTB-XL-Image-17K是一个包含17,271个高质量12导联心电图图像的合成数据集，为心电图数字化研究提供完整的地面真实数据支持。


<details>
  <summary>Details</summary>
Motivation: 心电图数字化（将纸质或扫描的心电图图像转换回时间序列信号）对于利用数十年的临床数据在深度学习应用中至关重要，但缺乏大规模同时包含心电图图像和对应地面真实信号的数据集阻碍了研究进展。

Method: 基于PTB-XL信号数据库，开发了一个开源Python框架，生成包含五种互补数据类型的合成心电图图像数据集：真实的心电图图像、像素级分割掩码、地面真实时间序列信号、YOLO格式的边界框标注以及全面的元数据。

Result: 成功生成了17,271个高质量12导联心电图图像，实现了100%的生成成功率，平均每个样本处理时间为1.35秒。数据集支持完整的心电图数字化流程：导联检测、波形分割和信号提取。

Conclusion: PTB-XL-Image-17K填补了心电图数字化研究的关键空白，为研究人员提供了首个大规模、包含完整地面真实数据的资源，支持从导联检测到信号提取的完整评估流程。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [47] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-FlashHead是一个1.3B参数的实时音频驱动肖像生成框架，通过流式感知时空预训练和双向蒸馏技术，在保持高保真视觉质量的同时实现96 FPS的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动肖像生成方法面临高保真视觉质量与低延迟流式处理之间的平衡难题。大型模型计算成本过高，轻量级模型则牺牲了整体面部表示和时间稳定性。

Method: 1. 提出1.3B参数的统一框架SoulX-FlashHead；2. 引入流式感知时空预训练，配备时间音频上下文缓存机制；3. 提出Oracle引导的双向蒸馏，利用真实运动先验提供精确物理指导；4. 构建VividHead数据集（782小时严格对齐的视频数据）。

Result: 在HDTF和VFHQ基准测试中达到最先进性能；轻量版在单张RTX 4090上实现96 FPS推理速度，支持超快速交互而不牺牲视觉连贯性。

Conclusion: SoulX-FlashHead成功解决了音频驱动肖像生成中高保真质量与实时流式处理之间的矛盾，通过创新的预训练和蒸馏方法实现了高质量、低延迟的无限长度视频生成。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [48] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: SpatialReward：通过空间推理解决图像编辑在线强化学习中奖励信号不足的问题，显著提升评估准确性和编辑效果


<details>
  <summary>Details</summary>
Motivation: 当前在线强化学习在图像编辑中面临可靠奖励信号稀缺的问题，现有评估器存在"注意力崩溃"现象，即模型忽视跨图像比较和细粒度细节，导致感知不准确和评分偏差

Method: 提出SpatialReward奖励模型，通过显式空间推理进行精确验证，将推理锚定到预测的编辑区域，使语义判断基于像素级证据

Result: 在260k空间感知数据集上训练，在MMRB2和EditReward-Bench上达到SOTA性能，在MultiEditReward-Bench上超越专有评估器；作为在线RL信号，将OmniGen2在GEdit-Bench上提升+0.90，超越领先判别模型并两倍于GPT-4.1的增益

Conclusion: 空间推理对于实现图像编辑中的有效对齐至关重要，SpatialReward通过像素级证据的语义判断显著提升了评估准确性和编辑效果

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [49] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 该研究创建了一个名为GlobalWasteData(GWD)的大规模统一垃圾分类数据集，包含89,807张图像，涵盖14个主要类别和68个子类，旨在解决现有垃圾数据集碎片化、不一致和偏置的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公开垃圾分类数据集存在碎片化、不一致和特定环境偏置的问题，类别名称、标注格式、图像条件和类别分布差异使得难以组合这些数据集或训练出在真实场景中泛化良好的模型。

Method: 通过合并多个公开可用数据集创建统一的GlobalWasteData(GWD)档案，包含89,807张图像，涵盖14个主要类别和68个子类。采用质量过滤、重复去除和元数据生成等预处理步骤提高数据集可靠性。

Result: GWD数据集提供了统一的标注、改进的领域多样性和更平衡的类别表示，为开发稳健且可泛化的垃圾识别模型奠定了基础，可用于环境监测、回收自动化和废物识别等机器学习应用。

Conclusion: GlobalWasteData数据集解决了现有垃圾分类数据集的局限性，为机器学习在环境监测和回收自动化领域的应用提供了强大基础，并公开可用以促进未来研究和可重复性。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [50] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: TOM-GS：首个基于高斯泼溅的热成像相机SLAM系统，结合学习式里程计与密集建图，在恶劣环境下实现鲁棒的运动估计和高质量重建


<details>
  <summary>Details</summary>
Motivation: 热成像传感器在黑暗、灰尘和烟雾等恶劣条件下具有鲁棒性，但现有热成像里程计和建图方法主要是几何方法，在不同数据集上表现不稳定且无法生成密集地图。受近期高斯泼溅技术高效性和高质量重建能力的启发，需要开发专门针对热成像相机的SLAM系统。

Method: 提出TOM-GS方法，将学习式里程计与基于高斯泼溅的密集建图相结合。系统包含专门的热图像增强模块和单目深度集成模块，是首个针对热成像相机的高斯泼溅SLAM系统。

Result: 在运动估计和新视角渲染方面的广泛实验表明，TOM-GS优于现有的学习式方法，验证了学习式流程在鲁棒热成像里程计和密集重建方面的优势。

Conclusion: TOM-GS成功展示了将高斯泼溅技术应用于热成像SLAM的可行性，为恶劣环境下的机器人感知提供了更鲁棒、更高质量的解决方案。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [51] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 提出一种利用多预训练视觉编码器进行脑信号与视觉表示对齐的新方法，通过对比学习和融合先验提升解码效果


<details>
  <summary>Details</summary>
Motivation: 当前脑信号视觉解码方法大多关注高层语义特征而忽视像素级细节，限制了我们对人类视觉系统的理解，需要更好的脑-图像对齐策略

Method: 使用多个具有不同归纳偏置的预训练视觉编码器捕捉层次化多尺度视觉表示，采用对比学习目标实现脑信号与视觉嵌入的有效对齐，并引入融合先验学习稳定映射

Result: 大量定量和定性实验表明，该方法在检索准确性和重建保真度之间取得了良好平衡

Conclusion: 提出的脑-图像对齐策略能够有效解码视觉信息，为理解人类视觉系统提供了更好的工具

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [52] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的隐式运动表示方法，将每帧运动压缩为紧凑的1D运动token，解决了现有显式方法的空间不匹配问题和隐式方法的身份信息泄漏问题，并设计了时间一致的mask token重定向模块来提升重定向一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在角色动画方面取得了显著进展，但现有方法存在明显局限：显式方法（如使用骨架、DWPose等结构化信号）难以处理空间不匹配和不同身体比例问题；隐式方法虽然能捕捉高层运动语义，但存在身份信息泄漏以及运动与外观纠缠的问题。

Method: 提出一种新颖的隐式运动表示，将每帧运动压缩为紧凑的1D运动token，放松了2D表示的严格空间约束并防止运动视频中的身份信息泄漏。设计了时间一致的mask token重定向模块，通过时间训练瓶颈减少源图像运动的干扰并提升重定向一致性。采用三阶段训练策略来提高训练效率和保证高保真度。

Result: 大量实验表明，提出的隐式运动表示和IM-Animation生成能力在性能上达到或超越了最先进的方法。

Conclusion: 提出的隐式运动表示方法有效解决了现有角色动画方法的关键挑战，通过1D运动token表示和时间一致的mask token重定向模块，在保持身份一致性的同时实现了高质量的运动重定向。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [53] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: ZoomDet：一种用于无人机图像目标检测的自适应放大框架，通过非均匀放大目标区域来改善小目标检测性能


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标通常较小且稀疏，这阻碍了有效目标检测器的优化。传统方法难以处理这种小目标问题，需要一种能够自适应放大目标区域的方法来更好地捕捉目标特征。

Method: 提出轻量级偏移预测方案和基于边界框的放大目标，学习输入图像的非均匀放大变换。采用角对齐的边界框变换方法，将真实边界框变换到放大空间进行训练，在推理时将预测边界框变换回原始空间。

Result: 在VisDrone、UAVDT和SeaDronesSee三个无人机目标检测数据集上进行了广泛实验。在SeaDronesSee数据集上，ZoomDet使Faster R-CNN模型的mAP提升了8.4个绝对百分点，仅增加约3ms的延迟。

Conclusion: ZoomDet是一种架构无关的自适应放大框架，可应用于任意目标检测架构，有效解决了无人机图像中小目标检测的挑战，显著提升了检测性能而仅带来轻微的计算开销。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [54] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 本文提出了一种新的对象中心学习（OCL）评估框架，使用指令调优的视觉语言模型作为评估器，并引入统一的任务和指标来联合评估定位能力和表示有用性。


<details>
  <summary>Details</summary>
Motivation: 当前OCL模型主要关注对象发现和简单推理任务评估，缺乏对表示有用性的深入评估，且定位能力和表示有用性使用分离的指标进行评估，存在局限性。

Method: 1) 使用指令调优的视觉语言模型作为评估器，在多样化VQA数据集上进行可扩展的基准测试；2) 引入统一评估任务和指标，联合评估定位（where）和表示有用性（what）；3) 包含简单的多特征重建基线作为参考点。

Result: 提出的评估框架能够更全面地衡量OCL模型在复杂推理任务中的表示有用性，并通过统一指标消除分离评估带来的不一致性。

Conclusion: 该研究为OCL模型提供了更全面、一致的评估方法，有助于更好地衡量模型在组合泛化和OOD鲁棒性方面的能力。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [55] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 使用GCViT-Tiny架构在牛津-IIIT宠物数据集子集上实现猫品种分类，通过数据增强提升泛化能力，测试准确率达92.00%


<details>
  <summary>Details</summary>
Motivation: 由于猫品种间毛色图案、面部结构和颜色的细微差异，从图像中准确识别猫品种具有挑战性。需要开发有效的深度学习方法来解决这一细粒度图像分类问题。

Method: 采用全局上下文视觉变换器（GCViT）架构的Tiny版本进行猫品种识别。使用牛津-IIIT宠物数据集的子集，包含各种家猫品种的高分辨率图像。通过旋转、水平翻转和亮度调整等广泛的数据增强技术来提升模型泛化能力。

Result: GCViT-Tiny模型在测试集上达到92.00%的准确率，验证集准确率为94.54%。结果表明基于变换器的架构在细粒度图像分类任务中具有良好效果。

Conclusion: 基于变换器的架构在猫品种分类等细粒度图像识别任务中表现优异。该方法在兽医诊断、动物收容所管理和移动端品种识别系统等方面具有应用潜力，并提供了Hugging Face演示平台。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [56] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 提出双时相分析框架，利用统计描述符、影像组学纹理特征和深度特征嵌入来表征缺血组织，通过特征空间分析揭示卒中演化的组织表型。


<details>
  <summary>Details</summary>
Motivation: 单时间点分割无法捕捉卒中的生物学异质性和时间演化过程，需要开发能够反映组织状态转变的分析方法。

Method: 采用双时相分析框架（入院T1和随访T2），从CTP提取统计描述符、纹理特征和深度特征嵌入（mJ-Net和nnU-Net），构建六个ROI区域，在特征空间进行聚类分析。

Result: 在18名成功再灌注患者中，区域级表征呈现有意义的聚类：可恢复的缺血半暗带区域特征与正常脑组织相似，而梗死区域形成明显分组。深度特征空间（特别是mJ-Net）能显著区分可挽救与不可挽救组织。

Conclusion: 编码器衍生的特征流形反映了潜在的组织表型和状态转变，为基于影像的卒中演化量化提供了新的见解。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [57] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: LGDEA提出了一种基于LLM引导的诊断证据对齐方法，通过构建共享诊断证据空间实现证据级跨模态对齐，显著减少对配对数据的依赖


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法存在局限性：全局对齐容易被非诊断信息主导，局部对齐无法整合关键诊断证据，导致难以学习可靠的诊断表示，限制了在配对数据有限场景下的应用

Method: LGDEA利用大语言模型从放射学报告中提取关键诊断证据，构建共享诊断证据空间，实现证据感知的跨模态对齐，有效利用大量未配对的医学图像和报告

Result: 实验结果表明，该方法在短语定位、图像-文本检索和零样本分类任务上取得一致且显著的改进，性能甚至可与依赖大量配对数据的预训练方法相媲美

Conclusion: LGDEA通过证据级对齐更符合医学诊断过程，显著减轻了对配对数据的依赖，为医学视觉-语言预训练提供了更有效的解决方案

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [58] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: MUFASA是一个轻量级即插即用框架，通过在多层ViT特征上计算slot attention来提升无监督物体分割性能


<details>
  <summary>Details</summary>
Motivation: 当前基于slot attention的无监督物体中心学习方法仅使用预训练ViT的最后一层特征，忽略了其他层中丰富的语义信息，限制了分割性能

Method: 提出MUFASA框架，在ViT编码器的多个特征层上计算slot attention，并提出融合策略将多层获得的slot聚合成统一的物体中心表示

Result: 将MUFASA集成到现有OCL方法中，在多个数据集上提升了分割结果，达到新的SOTA，同时改善了训练收敛性，仅带来轻微推理开销

Conclusion: 通过充分利用ViT多层特征中的语义信息，MUFASA能够显著提升slot attention方法的无监督物体分割性能

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [59] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: DINOv3特征在少样本语义分割中表现出色，仅使用冻结特征和类原型就能达到与复杂方法竞争的性能，但存在"最安全vs最优"的层选择困境。


<details>
  <summary>Details</summary>
Motivation: 研究DINOv3等自监督ViT模型在少样本语义分割任务中的内在能力，探索冻结特征是否足以达到良好性能，避免复杂的解码器或测试时适应。

Method: 提出FSSDINO方法，使用冻结的DINOv3特征，通过类特定原型和Gram矩阵精炼进行训练自由的少样本分割。进行Oracle引导的层分析，比较不同层特征的表现。

Result: 在二元、多类和跨域基准测试中，这种最小化方法在最终骨干层上表现优异，与复杂方法竞争。Oracle分析显示中间层存在更高性能潜力，但当前选择指标无法可靠识别。

Conclusion: "最后一层"是一个欺骗性强的强基线，揭示了基础模型中存在"语义选择鸿沟"——传统启发式方法无法可靠识别高保真特征，而Oracle证明更高性能是可实现的。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [60] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: FlexID是一个无需训练的个人化文本到图像生成框架，通过意图感知调制解决身份保真度与文本适应性之间的冲突，实现了身份一致性和文本遵循的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法依赖刚性的视觉特征注入，导致身份保真度和文本适应性之间存在冲突。需要一种能够动态平衡这两方面的方法，以实现更自然的个人化图像生成。

Method: 提出FlexID框架，将身份正交解耦为两个维度：语义身份投影器（SIP）在语言空间注入高层先验，视觉特征锚（VFA）在潜在空间确保结构保真度。关键创新是上下文感知自适应门控（CAG）机制，根据编辑意图和扩散时间步动态调制这两个流的权重。

Result: 在IBench上的大量实验表明，FlexID在身份一致性和文本遵循之间实现了最先进的平衡，为复杂叙事生成提供了高效解决方案。

Conclusion: FlexID通过意图感知调制框架成功解决了个人化文本到图像生成中的身份保真度与文本适应性冲突问题，实现了动态平衡，为复杂场景生成提供了有效方法。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [61] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 提出一个紧凑的3B参数视觉-语言-动作（VLA）智能体，通过显式图像基础推理实现语言驱动的物体导航，无需多模型拼接，提升可解释性、泛化能力和导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两大问题：端到端训练模型泛化能力差且缺乏动作级可解释性；模块化零样本流水线存在错误传播、计算成本高且难以将推理整合到导航策略中。需要一种更高效、可解释且泛化能力强的解决方案。

Method: 提出紧凑的3B参数VLA智能体，通过显式图像基础推理直接回答"这是目标物体吗？"和"为什么采取这个动作？"。推理过程分三个阶段："思考"、"思考总结"和"动作"，实现人类化的具身推理。

Result: 该方法在物体识别和动作选择方面表现出改进的可解释性、更强的泛化能力和更高效的导航性能。代码和数据集将在论文接受后提供。

Conclusion: 提出的紧凑VLA智能体通过显式图像基础推理，有效解决了现有方法的局限性，实现了更高效、可解释且泛化能力强的语言驱动物体导航。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [62] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 该论文介绍了HID 2025竞赛，使用SUSTech-Competition数据集评估步态识别算法，参赛者在无专用训练数据情况下实现了94.2%的准确率，创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 远距离人体识别(HID)具有挑战性，因为传统生物特征如面部和指纹在真实场景中难以获取。步态识别提供了一种实用替代方案，可以在远距离可靠捕获。HID竞赛旨在促进步态识别进展并提供公平评估平台。

Method: 竞赛采用SUSTech-Competition数据集，包含服装、携带物品和视角的显著变化。不提供专用训练数据，参赛者需使用外部数据集训练模型。每年使用不同随机种子生成不同的评估分割，降低过拟合风险并支持跨域泛化的公平评估。

Result: 尽管难度增加，参赛者取得了进一步改进，最佳方法达到了94.2%的准确率，在该数据集上创下了新的基准。HID 2025明确检验了算法进步是否能超越先前观察到的准确率极限。

Conclusion: HID 2025竞赛展示了步态识别算法的持续进步，即使在更具挑战性的条件下也能实现性能提升。论文还分析了关键技术趋势，并概述了步态识别未来研究的潜在方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [63] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于解耦表征学习的跨摄像头奶牛识别框架，通过子空间可识别性保证理论分解图像特征，显著提升了在未见摄像头上的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有动物识别方法在受控的单摄像头环境下表现良好，但在跨摄像头泛化方面面临严重挑战。当模型从源摄像头部署到具有不同光照、背景、视角和成像特性的新监控节点时，识别性能会急剧下降，这限制了非接触式技术在大规模动态农场环境中的应用。

Method: 提出基于解耦表征学习的跨摄像头奶牛识别框架，利用子空间可识别性保证理论，设计原理驱动的特征解耦模块，将观测图像分解为多个正交潜在子空间，有效分离出跨摄像头不变的稳定身份相关生物特征。

Result: 构建了包含5个不同摄像头节点的高质量数据集，涵盖异构采集设备和复杂的光照角度变化。在7个跨摄像头任务上的实验表明，该方法平均准确率达到86.0%，显著优于源域基线（51.9%）和最强的跨摄像头基线方法（79.8%）。

Conclusion: 本研究建立了一个基于子空间理论的特征解耦框架，用于协作式跨摄像头奶牛识别，为无控制的智能农场环境中的精确动物监测提供了新范式。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [64] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了一个系统化的视频生成模型后训练框架，将监督策略塑造、奖励驱动的强化学习和基于偏好的精炼整合到单一稳定性约束优化堆栈中，旨在提升生成视频的感知保真度、时间一致性和提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 后训练是将预训练视频生成器转化为生产导向模型的关键步骤，需要解决实际视频生成约束，包括高计算成本、时间累积的失败模式，以及异构、不确定且通常弱区分的反馈信号。

Method: 提出一个系统化后训练框架，将监督策略塑造、奖励驱动的强化学习和基于偏好的精炼组织成稳定性约束优化堆栈，采用分阶段、诊断驱动的方法处理视频生成的实际约束。

Result: 该框架提供了清晰的蓝图，用于构建可扩展的后训练流程，在保持初始化时建立的可控性的同时，提升感知保真度、时间一致性和提示遵循能力。

Conclusion: 该系统性后训练框架能够构建稳定、可扩展且在实际部署环境中有效的视频生成模型后训练流程，为生产导向的视频生成模型开发提供了实用方案。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [65] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: Fine-R1是一个专门用于细粒度视觉识别的多模态大语言模型，通过R1风格训练框架，仅需4-shot训练就能超越现有通用MLLM和对比CLIP模型，在识别已见和未见子类别上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在粗粒度视觉任务上表现良好，但在细粒度视觉识别上存在困难。适应FGVR通常需要大量标注数据，成本高昂，且性能仍落后于专门用于判别任务的对比CLIP模型。此外，MLLM容易对已见子类别过拟合，对未见子类别泛化能力差。

Method: 提出Fine-R1，采用R1风格训练框架：1）思维链监督微调：构建高质量FGVR CoT数据集，包含"视觉分析、候选子类别、比较和预测"的推理过程，将模型转变为强大的开放世界分类器；2）三元组增强策略优化：类内增强混合同一类别内锚点和正样本图像的轨迹以提高对类内方差的鲁棒性，类间增强最大化跨子类别图像条件下的响应差异以增强判别能力。

Result: 仅需4-shot训练，Fine-R1在识别已见和未见子类别上均优于现有通用MLLM、推理MLLM甚至对比CLIP模型，在专家标注难以获取的知识密集型领域展现出潜力。

Conclusion: Fine-R1通过创新的训练框架解决了MLLM在细粒度视觉识别中的关键挑战，实现了数据高效、泛化能力强的细粒度视觉识别，为知识密集型领域的应用提供了有前景的解决方案。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [66] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: HistoMet：一种基于决策感知、概念对齐的多实例学习框架，直接从原发肿瘤全切片图像预测转移风险和转移部位，通过两阶段临床决策流程显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 转移进展是癌症相关死亡的主要原因，但直接从组织病理学预测原发肿瘤是否会转移以及转移部位仍然是一个基本挑战。现有计算方法通常将转移状态或部位预测作为孤立任务处理，没有明确模拟临床顺序决策过程。

Method: 提出HistoMet框架，采用两模块预测流程：首先估计原发肿瘤转移进展的可能性，然后对高风险病例进行转移部位的条件预测。通过预训练病理视觉语言模型整合语言定义和数据自适应的转移概念，指导表示学习并提高临床可解释性。

Result: 在6504名具有转移随访和部位注释的多机构泛癌队列中评估。在临床相关高灵敏度筛查设置（95%灵敏度）下，显著减少下游工作量同时保持高转移风险召回率。对于转移病例，宏观F1为74.6（标准差1.3），宏观一对多AUC为92.1。

Conclusion: 明确建模临床决策结构能够直接从原发肿瘤组织病理学实现稳健且可部署的转移进展和部位趋向性预后预测，为临床决策提供支持。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [67] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 该研究通过创建UMD数据集（包含PET/CT和PET/MRI扫描）评估3D医学基础模型，发现从结构成像转向功能成像时存在显著性能差距，表明当前模型远未达到真正的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学基础模型的验证主要局限于区域和结构成像，存在显著的模态差异未被探索。需要对这些模型在真实世界应用中的鲁棒性进行严格客观评估。

Method: 创建UMD数据集（490个全身PET/CT和464个全身PET/MRI扫描，约675k 2D图像，12k 3D器官标注），通过受试者内配对扫描的对照比较，将成像模态作为主要自变量，对代表性3D分割基础模型进行全面评估。

Result: 评估揭示了文献报道的基准与真实世界效能之间的显著差异，特别是在从结构域转向功能域时。这种系统性失败表明当前3D基础模型远未达到真正的通用性状态。

Conclusion: 需要向多模态训练和评估的范式转变，以弥合理想化基准测试与全面临床实用性之间的差距。该数据集和分析为未来开发真正模态无关的医学基础模型奠定了基石。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [68] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 该研究评估了医学图像3D重建流程中的误差，比较了不同分割算法和几何类型在体素和表面精度指标上的表现，发现Otsu方法最适用于各种几何形状，Jaccard指数比Dice更适合薄壁结构评估。


<details>
  <summary>Details</summary>
Motivation: 医学扫描创建3D模型的精度受多种因素影响，包括成像硬件、分割方法和网格处理技术等。几何类型、类别不平衡、体素和点云对齐对精度的影响尚未得到充分探索。

Method: 使用SLA技术打印球体、面罩和腹主动脉瘤模型，通过微CT扫描获取图像。采用GMM、Otsu和RG三种分割方法。使用KU算法对齐分割模型和参考模型，评估Dice、Jaccard分数和精度等指标。通过ICP对齐过程配准表面网格，评估Chamfer距离和平均Hausdorff距离。

Result: Otsu方法对所有几何形状都最适用。腹主动脉瘤因壁薄和对齐问题导致重叠分数较低。类别不平衡对腹主动脉瘤的特异性影响最大。表面精度指标与体素指标趋势不同：RG方法对球体表现最好，GMM和Otsu对腹主动脉瘤更好。面罩表面误差最大，可能是ICP对齐过程中的问题。

Conclusion: 分割精度是重建过程各阶段误差的累积总和。在高类别不平衡和对齐敏感的情况下，高体素精度指标可能具有误导性。Jaccard指数比Dice更严格，更适合薄壁结构的精度评估。必须确保体素和点云对齐才能对重建流程进行可靠评估。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [69] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文研究视觉语言模型在自动驾驶安全评估和决策中的应用，探索了三种系统级用例：基于CLIP的轻量级危险筛查、场景级嵌入在轨迹规划中的集成，以及自然语言作为运动规划的行为约束。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型能够将视觉观察与自然语言概念对齐，为安全关键自动驾驶中的语义推理提供了新机会。本文旨在探索视觉语言表示如何支持驾驶场景安全评估和决策制定。

Method: 研究了三种互补的系统级用例：1）基于CLIP图像-文本相似性的轻量级、类别无关的危险筛查方法；2）将场景级视觉语言嵌入集成到基于Transformer的轨迹规划框架中；3）使用自然语言作为运动规划的显式行为约束。

Result: 1）危险筛查方法能够低延迟检测多样化和分布外道路危险；2）简单地将全局嵌入条件化到规划器中不会提高轨迹精度；3）基于视觉场景元素的乘客式指令能够抑制罕见但严重的规划失败，并在模糊场景中改善安全对齐行为。

Conclusion: 视觉语言表示在表达语义风险、意图和行为约束方面对自动驾驶安全具有重要潜力。实现这一潜力本质上是一个工程问题，需要精心设计的系统架构和结构化基础，而不是简单的特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [70] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: U-KABS是一个新颖的混合框架，将Kolmogorov-Arnold Networks的表达能力与U型编码器-解码器架构相结合，用于医学图像分割，通过Bernstein多项式和B样条的混合设计提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在诊断和治疗规划中至关重要，但由于医学图像固有的复杂性和变异性，特别是捕捉数据中的非线性关系仍然具有挑战性。

Method: 提出U-KABS混合框架，集成卷积和挤压-激励阶段以增强通道特征表示，以及KAN Bernstein Spline阶段使用基于Bernstein多项式和B样条的可学习激活函数。编码器和解码器之间的跳跃连接支持有效的多尺度特征融合和空间细节保留。

Result: 在多个医学成像基准数据集上评估，U-KABS表现出优于强基线的性能，特别是在分割复杂解剖结构方面。

Conclusion: U-KABS通过结合Bernstein多项式的全局平滑性和B样条的局部适应性，能够有效捕捉医学图像中复杂结构分割所需的大范围上下文趋势和细粒度模式。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [71] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 提出Rolling Sink方法，无需额外训练即可解决自回归视频扩散模型在超长时域测试中的视觉退化问题


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在有限训练时长下存在训练-测试差距，特别是在测试时长超过训练时长时，会出现视觉质量快速退化问题。由于开放测试可以超越任何有限训练窗口，且长视频训练计算成本高，需要寻找无需训练的解决方案

Method: 通过对自回归缓存维护进行系统分析，提出了Rolling Sink方法。该方法基于Self Forcing（仅在5秒片段上训练），在测试时通过创新的缓存管理策略，将自回归视频合成扩展到超长持续时间（5-30分钟，16 FPS）

Result: Rolling Sink在超长时域测试中实现了卓越的视觉保真度和时间一致性，保持一致的物体、稳定的颜色、连贯的结构和平滑的运动，相比现有SOTA基线表现更优

Conclusion: Rolling Sink提供了一种无需训练的有效解决方案，成功弥合了自回归视频扩散模型在有限训练时长与开放测试时长之间的差距，实现了高质量的超长视频合成

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [72] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: UCATSC是一个基于模型的交通信号控制系统，通过随机决策过程建模，考虑视觉感知不确定性，使用硬约束确保安全和防止饥饿，提供可解释的控制策略。


<details>
  <summary>Details</summary>
Motivation: 现实世界中自适应交通信号控制部署有限，主要因为基于视觉感知的不确定性、隐含安全性问题，以及主要在仿真中学习和验证的非可解释控制策略。

Method: 使用具有约束的随机决策过程在部分可观测条件下建模交通信号控制，考虑视觉感知不确定性；通过信念空间中的反事实推演预测并强制执行与安全和防饥饿相关的硬约束。

Result: 系统旨在改善交通延迟和排放，同时防止安全关键错误，并基于显式模型提供可解释的控制策略输出。

Conclusion: UCATSC通过模型化方法解决了自适应交通信号控制中的不确定性、安全性和可解释性问题，为现实世界部署提供了更可靠的解决方案。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [73] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 该研究首次对16种最先进的AI生成图像检测方法进行了全面的零样本评估，覆盖23个预训练检测器变体和12个数据集，揭示了现有检测器在零样本场景下的性能局限性和系统性失败模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在数字平台上的激增，可靠的检测方法对于打击虚假信息和维护内容真实性变得至关重要。现有基准主要评估微调模型，而忽略了开箱即用性能这一实际部署中最常见的场景，存在重要研究空白。

Method: 对16种最先进的检测方法（包含23个预训练检测器变体）进行零样本评估，覆盖12个多样化数据集，包含260万张图像样本，涵盖291个不同的生成器（包括现代扩散模型）。采用系统化分析方法和统计检验（Friedman检验、Spearman相关性等）。

Result: 研究发现：(1)不存在通用最优检测器，检测器排名在不同数据集间极不稳定；(2)最佳检测器（75.0%平均准确率）与最差检测器（37.5%）之间存在37个百分点性能差距；(3)训练数据对齐对泛化能力影响显著，导致架构相同的检测器家族内出现20-60%性能差异；(4)现代商业生成器（Flux Dev、Firefly v4、Midjourney v7）能击败大多数检测器，平均准确率仅18-30%；(5)识别出三种影响跨数据集泛化的系统性失败模式。

Conclusion: 研究结果挑战了"一刀切"的检测器范式，表明从业者必须根据具体威胁环境仔细选择检测器，而不能依赖已发布的基准性能。研究为实际部署提供了可操作的指导方针。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [74] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 首个大规模跨范式面部年龄估计基准测试显示，零样本视觉语言模型(VLMs)在性能上显著超越大多数专用架构，最佳VLM比最佳非LLM模型提升15%，挑战了任务专用架构必要性的假设。


<details>
  <summary>Details</summary>
Motivation: 面部年龄估计对内容审核、年龄验证和深度伪造检测至关重要，但此前缺乏系统比较现代视觉语言模型与专用年龄估计架构的基准测试。

Method: 建立了首个大规模跨范式基准，评估34个模型（22个专用架构和12个通用VLMs），在8个标准数据集上使用总计1,100张测试图像，采用MAE指标，并分析18岁阈值年龄验证和14个年龄组的分层表现。

Result: 零样本VLMs平均MAE为5.65年，显著优于非LLM模型的9.88年；最佳VLM(Gemini 3 Flash Preview，MAE 4.32)比最佳非LLM模型(MiVOLO，MAE 5.10)提升15%；VLMs在未成年人年龄验证中的误判率(13-25%)远低于非LLM模型(60-100%)；所有模型在极端年龄(<5岁和65+岁)表现最差。

Conclusion: 研究挑战了任务专用架构对年龄估计必要的假设，建议领域应转向将VLM能力蒸馏到高效的专用模型中。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [75] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: OTA-Det是一个统一框架，将开放词汇航空检测(OVAD)和遥感视觉定位(RSVG)两种范式结合，实现多目标检测和细粒度语义理解，同时保持实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有OVAD方法仅限于粗粒度类别级语义，而RSVG方法结构上仅限于单目标定位，无法同时支持丰富语义理解和多目标检测。

Method: 1. 任务重构策略统一任务目标和监督机制；2. 密集语义对齐策略建立从整体表达到个体属性的多粒度对应关系；3. 基于RT-DETR架构扩展，引入高效模块实现开放文本检测。

Result: 在六个涵盖OVAD和RSVG任务的基准测试中达到最先进性能，同时保持34 FPS的实时推理速度。

Conclusion: OTA-Det首次将OVAD和RSVG统一到一个框架中，解决了现有方法的局限性，实现了同时支持丰富语义理解和多目标检测的能力。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [76] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: SPD-Faith Bench是一个诊断基准，用于评估多模态大语言模型推理过程的忠实性，发现两种系统性失败模式，并提出无需训练的SAGE框架来改善视觉证据校准。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型广泛使用思维链推理来提高可解释性，但生成的推理轨迹的忠实性仍不清楚。先前工作主要关注感知幻觉，而推理层面的不忠实性尚未充分探索。

Method: 引入SPD-Faith Bench诊断基准，基于细粒度图像差异推理，强制进行显式视觉比较。通过分析发现两种系统性失败模式，并提出SAGE框架——无需训练的视觉证据校准框架，改善视觉路由并使推理与感知对齐。

Result: 评估最先进的多模态大语言模型揭示了两种系统性失败模式：感知盲点和感知-推理分离。这些失败源于残差流中的视觉注意力衰减和表示偏移。SAGE框架能有效改善视觉路由和推理-感知对齐。

Conclusion: 研究强调了超越响应正确性来显式评估忠实性的重要性。提出的基准和SAGE框架为理解和改善多模态大语言模型的推理忠实性提供了工具。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [77] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace是一种无需训练、即插即用的视频人脸交换方法，可与基于扩散模型的图像人脸交换方法无缝集成，通过频率谱注意力插值、目标结构引导和流引导注意力时序平滑三大技术提升视频人脸交换的质量和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频人脸交换方法通常存在时序不一致问题，需要额外的训练或视频特定的微调。本文旨在开发一种无需训练、即插即用的方法，能够与现有的图像人脸交换方法无缝集成，同时提升视频中人脸交换的时序一致性和视觉保真度。

Method: VFace包含三个核心技术：1) 频率谱注意力插值技术，促进生成并保留关键身份特征；2) 目标结构引导，通过即插即用的注意力注入更好地对齐目标帧的结构特征；3) 流引导注意力时序平滑机制，在不修改底层扩散模型的情况下强制时空一致性，减少帧间生成中的时序不一致问题。

Result: 大量实验表明，该方法显著提升了时序一致性和视觉保真度，为视频人脸交换提供了一个实用且模块化的解决方案。该方法无需额外训练或视频特定微调。

Conclusion: VFace是一种无需训练、即插即用的高质量视频人脸交换方法，通过创新的频率谱注意力插值、目标结构引导和流引导注意力时序平滑技术，有效解决了视频人脸交换中的时序不一致问题，提供了实用且模块化的解决方案。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [78] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: ViewRope：一种几何感知的视频Transformer编码方法，通过将相机射线方向注入自注意力层，解决了世界模型中空间持久性不足的问题，显著改善了长期一致性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前预测性世界模型缺乏空间持久性，在长轨迹中无法维持稳定的场景结构，当相机重新访问先前观察的位置时经常产生幻觉细节。这种几何漂移源于对屏幕空间位置嵌入的依赖，这与3D一致性所需的投影几何相冲突。

Method: 提出了ViewRope几何感知编码，将相机射线方向直接注入视频Transformer自注意力层；通过相对射线几何而非像素局部性参数化注意力；提出几何感知帧稀疏注意力，利用几何线索选择性关注相关历史帧；还提出了ViewBench诊断套件来测量闭环保真度和几何漂移。

Result: ViewRope显著改善了长期一致性，同时减少了计算成本。通过几何感知编码和帧稀疏注意力，模型能够在长轨迹中维持稳定的场景结构，减少幻觉细节的产生。

Conclusion: 通过引入几何感知的ViewRope编码和几何感知帧稀疏注意力，解决了预测性世界模型中空间持久性不足的问题，为3D一致性提供了模型原生的归纳偏置，同时提高了计算效率。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [79] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出了一种从超高速运动模糊图像中恢复3D形状的逆渲染方法，通过快速重心坐标求解器显著提升计算效率，实现了对高速平移和旋转物体的3D重建。


<details>
  <summary>Details</summary>
Motivation: 在自然和工业场景中，高速运动物体（如体育中的球体或旋转机械）会产生严重的运动模糊，导致传统多视角立体视觉等3D重建技术失效，需要开发能够从极端运动模糊图像中恢复几何形状的新方法。

Method: 提出了一种新颖的逆渲染方法，包含快速重心坐标求解器来加速运动模糊合成过程。该方法完全可微分，允许梯度从渲染图像传播到底层3D形状，从而实现通过逆渲染进行形状恢复。

Result: 实验表明，该方法在正向模拟中能够高效真实地建模超高速运动物体，计算速度提升高达4.57倍。更重要的是，它成功地从经历极端平移和旋转运动的物体2D图像中恢复了3D形状。

Conclusion: 该方法突破了基于视觉的3D重建边界，为从超高速运动模糊图像中恢复3D几何形状提供了有效的解决方案，在体育分析和工业检测等实际应用中具有重要价值。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [80] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: SSI-Bench是一个针对视觉语言模型的空间推理基准测试，专注于受约束流形上的空间推理，包含1000个排序问题，评估模型在复杂3D结构中的几何和拓扑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多评估无约束场景，模型可以利用2D捷径，缺乏对真实物理世界中受几何、拓扑和物理约束的空间推理能力的评估。

Method: 通过完全人工中心化的流程创建：10名研究人员花费400多小时精心挑选图像、标注结构组件、设计问题以最小化像素级线索，构建包含几何和拓扑推理的1000个排序问题。

Result: 评估31个广泛使用的VLMs显示与人类存在巨大差距：最佳开源模型准确率22.2%，最强闭源模型33.6%，而人类得分91.6%。鼓励模型思考仅带来边际收益，错误分析显示模型在结构基础和约束一致的3D推理方面存在失败。

Conclusion: SSI-Bench揭示了当前视觉语言模型在受约束空间推理方面的严重不足，需要改进结构基础和约束一致的3D推理能力，为模型在物理世界中的空间智能发展提供了重要基准。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [81] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR：基于区域感知的儿科腕部X光片检索框架，通过全局和局部对比编码器实现两阶段检索，显著提升骨折模式检索和诊断性能


<details>
  <summary>Details</summary>
Motivation: 腕部X光片中骨折模式的检索具有挑战性，因为临床重要线索细微、高度局部化，且常被重叠解剖结构或不同成像视角所掩盖。现有进展受到大型、标注良好的医学图像检索数据集稀缺的限制。

Method: 使用MedGemma进行结构化报告挖掘生成全局和区域级描述，结合预处理的腕部图像和桡骨远端、尺骨远端、尺骨茎突的骨特异性裁剪。联合训练全局和局部对比编码器，采用两阶段检索：1）粗粒度全局匹配识别候选检查；2）基于预定义解剖骨区域的区域条件重排序。

Result: WristMIR显著提升检索性能，图像到文本的Recall@5从0.82%提高到9.35%。其嵌入表示在骨折分类上表现更强（AUROC 0.949，AUPRC 0.953）。在区域感知评估中，两阶段设计显著改善基于检索的骨折诊断，平均F1分数从0.568提高到0.753，放射科医生评价其检索病例临床相关性更高。

Conclusion: 解剖引导的检索在增强儿科肌肉骨骼成像中的诊断推理和临床决策支持方面具有潜力。WristMIR无需手动图像级标注即可学习细粒度、临床有意义的图像表示。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [82] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE是一个从原始视频流中扩展几何基础模型的框架，通过分层挖掘管道将视频转换为训练轨迹，结合稀疏几何锚点和密集可微分一致性监督，显著提升零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建方面有潜力，但受到大规模多样化3D标注数据稀缺的限制。虽然互联网视频提供了几乎无限的原始数据，但由于缺乏真实几何信息和存在观测噪声，将其用作几何学习的扩展源具有挑战性。

Method: SAGE采用分层挖掘管道：1) 信息丰富的训练轨迹选择；2) 通过SfM点云进行稀疏几何锚点，提供全局结构指导；3) 通过3D高斯渲染进行密集可微分一致性，提供多视角约束。为防止灾难性遗忘，引入基于锚点数据的正则化策略。

Result: 在未见过的基准测试（7Scenes、TUM-RGBD、Matterport3D）上，SAGE显著增强了零样本泛化能力，与最先进的基线相比，Chamfer距离减少了20-42%。

Conclusion: SAGE开创了通过互联网视频适应几何基础模型的方法，为通用3D学习建立了可扩展的范式，是首个利用互联网视频进行几何基础模型适应的框架。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [83] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: TLQ框架针对视觉语言模型提出细粒度校准策略，通过token级重要性感知和层量化方案，在保持量化性能的同时降低硬件依赖


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉token和文本token在激活分布和量化误差敏感性方面存在显著差异，给后训练量化校准带来挑战，需要重新思考校准策略

Method: 提出Token-level Importance-aware Layer-wise Quantization框架：1)基于梯度信息设计token级重要性集成机制构建token级校准集；2)引入多GPU、量化暴露的层校准方案，保持校准与真实量化推理路径一致

Result: 在两个模型、三种模型规模和两种量化设置下评估，在所有设置中均实现性能提升，表现出强大的量化稳定性

Conclusion: TLQ框架通过细粒度token级校准和分布式层校准方案，有效解决了视觉语言模型后训练量化的校准挑战，在保持性能的同时降低了对A100 GPU大内存的依赖

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [84] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID是一个无需训练的推理加速框架，通过注意力与多样性令牌选择和树状时空令牌合并技术，显著减少视频大语言模型的计算开销，在保留10%视觉令牌的情况下能保持99.1%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型需要处理大量视觉令牌，计算效率低下。现有加速框架通常独立压缩空间和时间冗余，忽略了时空关系，导致次优的时空压缩效果。

Method: 提出FlashVID框架：1) 使用注意力与多样性令牌选择选择最具代表性的令牌进行基本视频表示；2) 应用树状时空令牌合并进行细粒度时空冗余消除。

Result: 在三个代表性VLLM和五个视频理解基准上的实验证明方法的有效性和泛化性。仅保留10%视觉令牌时，FlashVID能保持LLaVA-OneVision 99.1%的性能。使Qwen2.5-VL的视频帧输入增加10倍，在相同计算预算下相对提升8.6%。

Conclusion: FlashVID是一个无需训练、即插即用的模块，能有效加速视频大语言模型的推理，通过保留关键时空信息实现高效压缩，显著提升长视频帧处理能力。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [85] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 视觉语言模型在隐私相关属性识别中的零样本评估显示，VLM比人类标注者更倾向于预测隐私属性的存在，但在高一致性情况下可以补充人类标注


<details>
  <summary>Details</summary>
Motivation: 评估开源视觉语言模型在隐私相关属性识别方面的零样本性能，探索VLM与人类标注在隐私属性识别上的差异和互补性

Method: 对开源视觉语言模型进行零样本评估，识别VLM表现出强标注者间一致性的属性，分析VLM与人类标注之间的分歧案例

Result: 与人类标注相比，VLM更倾向于预测隐私属性的存在；在高一致性情况下，VLM可以识别人类标注者忽略的属性，补充人类标注

Conclusion: 视觉语言模型在大规模图像数据集的隐私标注中具有支持潜力，能够补充人类标注，特别是在高一致性情况下

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [86] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 该论文研究了多模态大语言模型中的视觉空间推理问题，提出了自适应测试时视觉想象框架AVIC，通过选择性调用世界模型来优化空间推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉空间推理中存在局限性，特别是在需要从未见或替代视角理解场景时。现有方法通过世界模型增强视觉想象，但缺乏对想象何时必要、多少想象有益以及何时想象有害的系统分析。不加区分的想象会增加计算成本，甚至因引入误导性证据而降低性能。

Method: 提出了AVIC自适应测试时框架，该框架使用世界模型进行视觉想象，但首先明确推理当前视觉证据是否充分，然后选择性地调用和缩放视觉想象。框架在空间推理基准（SAT、MMSI）和具身导航基准（R2R）上进行评估。

Result: 研究揭示了想象在空间推理中的三种关键场景：关键、边际和有害。选择性控制方法能够匹配或超越固定想象策略，同时显著减少世界模型调用次数和语言标记数量。在多个基准测试中验证了自适应方法的有效性。

Conclusion: 研究强调了分析和控制测试时想象对于高效可靠空间推理的重要性。自适应选择性想象框架能够在保持或提升性能的同时显著降低计算成本，为多模态大语言模型的视觉空间推理提供了更精细的控制方法。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [87] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: Octopus框架通过合成密集的自校正示例来解决VLMs中自校正学习信号稀疏的问题，显著提升样本效率和RL优化稳定性，Octopus-8B模型在7个基准测试中达到开源VLMs的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在视觉语言模型中学习自校正行为面临挑战，因为有效的自校正行为出现频率极低，导致学习信号极其稀疏，难以有效训练模型进行复杂推理问题的自我修正。

Method: 提出校正特定rollouts（Octopus）框架：1）通过重组现有rollouts合成密集的自校正示例；2）引入响应掩码策略，将自校正与直接推理解耦；3）构建Octopus-8B模型，具备可控自校正能力。

Result: Octopus-8B在7个基准测试中达到开源视觉语言模型的SOTA性能，相比最佳RLVR基线提升1.0分，同时每个训练步骤仅需0.72倍训练时间，显著提高了训练效率和性能。

Conclusion: Octopus框架通过合成密集自校正示例和响应掩码策略，有效解决了VLMs中自校正学习信号稀疏的问题，显著提升了模型性能和训练效率，为复杂推理问题的自我修正提供了有效解决方案。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [88] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于少样本学习的跨场景人群计数方法，通过局部和全局密度特征引导模型适应未见过的监控场景。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数模型在不同监控场景间的泛化能力有限，因为不同摄像头捕获的人群场景差异很大。为了提升模型对未见过的监控场景的适应能力，需要解决跨场景泛化问题。

Method: 1. 将不同监控场景视为不同类别场景，引入少样本学习框架
2. 提出多重局部密度学习器，学习支持场景中不同密度分布的多原型表示
3. 编码多个局部密度相似性矩阵，以局部方式指导模型
4. 从支持图像中提取全局密度特征，以全局方式指导模型
5. 结合局部和全局密度特征引导模型适应目标场景

Result: 在三个监控数据集上的实验表明，该方法能够有效适应未见过的监控场景，并在少样本人群计数任务中优于当前最先进的方法。

Conclusion: 通过利用局部和全局密度特征指导模型，提出的方法显著提升了人群计数模型在跨监控场景中的泛化能力，为少样本人群计数提供了有效的解决方案。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [89] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: D-ORCA是一个专注于对话的跨模态大语言模型，用于鲁棒的视听字幕生成，在说话人识别、语音识别和时间定位方面显著优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 口语对话是视频中的主要信息来源，准确识别谁在什么时候说了什么对于深度视频理解至关重要。目前开源生态中缺乏高质量的多方对话视频数据集。

Method: 1) 构建DVD双语数据集（近4万训练视频+2000评估视频）；2) 采用分组相对策略优化，引入三个新颖的奖励函数：说话人归属准确性、全局语音内容准确性、句子级时间边界对齐。

Result: D-ORCA在说话人识别、语音识别和时间定位方面显著优于现有开源模型。尽管只有80亿参数，但在多个通用视听理解基准上与Qwen3-Omni表现相当。

Conclusion: D-ORCA通过对话中心的跨模态大语言模型和高质量数据集，显著提升了视听字幕生成的准确性，特别是在说话人归属和时间对齐方面。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [90] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: EasyTune提出了一种高效的运动生成模型对齐方法，通过分步微调扩散模型解决现有方法优化效率低、内存消耗大的问题，并引入自优化偏好学习机制解决偏好数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微分奖励的运动生成模型对齐方法存在两个主要问题：1) 优化效率低且粒度粗糙；2) 内存消耗高。这些问题源于去噪轨迹中不同步骤之间的递归依赖关系。

Method: 提出EasyTune方法：1) 在去噪过程的每个步骤分别微调扩散模型，解耦递归依赖，实现密集细粒度且内存高效的优化；2) 引入自优化偏好学习机制，动态识别偏好对并进行偏好学习，解决偏好运动对数据稀缺问题。

Result: 实验表明，EasyTune在MM-Dist对齐指标上比DRaFT-50提升8.2%，仅需其31.16%的额外内存开销，训练速度提升7.3倍。

Conclusion: EasyTune通过分步微调和自优化偏好学习，有效解决了运动生成模型对齐中的效率、内存和偏好数据问题，为扩散模型的高效对齐提供了新思路。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [91] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: FSP-Diff：一种全光谱先验增强的双域潜在扩散框架，用于超低剂量能谱CT重建，通过互补特征构建、全光谱先验集成和高效潜在扩散合成，显著提升图像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 超低剂量条件下能谱CT的能量特定投影信噪比急剧下降，导致重建图像出现严重伪影和结构细节丢失，需要开发有效的重建方法来应对这一挑战。

Method: 提出FSP-Diff框架，包含三个核心策略：1) 互补特征构建：结合直接图像重建和投影域去噪结果；2) 全光谱先验集成：融合多能量投影为高信噪比全光谱图像作为统一结构参考；3) 高效潜在扩散合成：将多路径特征嵌入紧凑潜在空间，在低维流形中进行交互特征融合。

Result: 在模拟和真实数据集上的广泛实验表明，FSP-Diff在图像质量和计算效率方面显著优于现有最先进方法，展示了其在临床可行超低剂量能谱CT成像中的潜力。

Conclusion: FSP-Diff框架通过创新的双域潜在扩散方法，成功解决了超低剂量能谱CT重建中的噪声和伪影问题，为临床可行的超低剂量能谱CT成像提供了有前景的解决方案。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [92] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出CSDN方法用于超稀疏角度CBCT重建，通过神经先验编码连续三维衰减表示，结合协同扩散策略恢复角度连续性和层间一致性


<details>
  <summary>Details</summary>
Motivation: CBCT临床应用面临辐射剂量与图像质量的权衡问题。超稀疏角度采样虽能降低剂量，但会导致严重的欠采样伪影和层间不一致性，影响诊断可靠性。现有重建方法难以平衡角度连续性和空间细节保真度。

Method: 提出CSDN方法：1) 引入神经先验作为结构基础，编码连续三维衰减表示，从超稀疏测量合成物理一致的密集投影；2) 基于神经先验初始化，开发协同扩散策略，包含正弦图细化扩散(Sino-RD)恢复角度连续性和数字放射摄影细化扩散(DR-RD)从投影图像角度增强层间一致性；3) 通过双投影重建融合(DPRF)模块自适应融合两个扩散路径输出，实现连贯的体积重建。

Result: 大量实验表明，CSDN在超稀疏角度条件下能有效抑制伪影并恢复精细纹理，性能优于现有最先进技术。

Conclusion: CSDN方法通过神经先验和协同扩散策略，成功解决了超稀疏角度CBCT重建中的角度连续性和层间一致性问题，为低剂量高质量CBCT成像提供了有效解决方案。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [93] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 深度伪造检测技术发展滞后于生成技术，现有检测模型对现代合成方法生成的深度伪造视频表现不佳，人类评估也显示对高质量伪造视频识别困难，亟需改进检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型、NeRF等新技术的发展，深度伪造生成技术变得更加逼真和易用，而检测技术虽然也有进步，但需要评估现有检测方法是否能跟上生成技术的发展步伐。

Method: 对最先进的深度伪造检测技术进行全面的实证分析，包括人类评估实验，测试这些检测方法对抗现代合成技术生成的深度伪造视频的性能。

Result: 研究发现许多最先进的检测模型在面对现代合成技术生成的深度伪造时表现显著不佳，人类参与者对最高质量的深度伪造也识别困难，显示检测技术落后于生成技术。

Conclusion: 当前检测方法与新一代深度伪造生成技术之间存在严重差距，迫切需要持续改进检测模型以跟上深度伪造生成技术的快速发展，这是该领域研究的当务之急。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [94] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: MIND是首个用于评估世界模型记忆一致性和动作控制能力的开放域闭环重访基准，包含250个高质量视频和多样化场景，设计了评估框架和基线模型


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的基准来评估世界模型在动态视觉环境中的核心能力，包括理解、记忆和预测能力

Method: 构建包含250个1080p、24FPS高质量视频的数据集，包括第一人称和第三人称视角，设计评估框架测量记忆一致性和动作控制能力，并引入MIND-World作为基线模型

Result: 实验验证了MIND基准的完整性，揭示了当前世界模型在保持长期记忆一致性和跨动作空间泛化方面的关键挑战

Conclusion: MIND基准填补了世界模型评估的空白，为未来研究提供了标准化测试平台，并指出了当前模型的主要局限性

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [95] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 提出一个系统化构建等变Vision Transformers的框架，通过使ViT的关键组件（包括patch embedding、self-attention、位置编码和采样模块）等变化，实现理论保证的等变性，并在多种视觉任务中提升性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有等变ViT在平衡性能和等变性方面存在困难，主要挑战在于难以在ViT的多样化模块（特别是协调Self-Attention机制与Patch Embedding）中实现整体的等变修改。

Method: 提出一个直接框架，系统性地使ViT的关键组件等变化，包括patch embedding、self-attention、位置编码和Down/Up-Sampling模块，构建具有理论保证等变性的ViT架构。

Result: 广泛的实验表明，提出的等变ViT在广泛的视觉任务中持续提升性能和数据效率，该架构可作为即插即用的替代方案，甚至可扩展到Swin Transformers。

Conclusion: 通过系统化地使ViT关键组件等变化，可以构建既具有理论保证等变性又具有实际性能优势的Vision Transformers，为视觉任务提供更高效和有效的解决方案。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [96] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: Picasso是一个物理约束的多物体场景重建系统，通过考虑几何、非穿透性和物理约束来构建物理上合理的场景重建，并提出了包含接触丰富场景的数据集和物理合理性评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有场景重建方法在遮挡和测量噪声下可能产生几何准确但物理上不合理的重建结果（如物体穿透或不稳定平衡），这影响了数字孪生中动态行为预测的可靠性，特别是在接触丰富的仿真规划和控制中。

Method: 提出Picasso物理约束重建流水线：1）整体考虑场景而非孤立处理单个物体；2）使用快速拒绝采样方法处理多物体交互；3）利用推断的物体接触图指导采样；4）考虑几何、非穿透性和物理约束。

Result: 在提出的Picasso数据集和YCB-V数据集上进行了广泛评估，结果显示Picasso大幅优于现有技术，同时提供既物理合理又更符合人类直觉的重建结果。

Conclusion: 物体姿态和形状估计需要整体考虑场景，考虑物体交互和物理合理性。Picasso通过物理约束重建方法实现了这一目标，为仿真规划和接触丰富行为的控制提供了更可靠的数字孪生基础。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [97] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: DICE提出了一种无需训练的艺术风格擦除框架，通过对比子空间分解实现风格与内容的解耦，有效防止风格模仿，同时保持内容完整性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的普及使得艺术风格模仿变得容易，引发了版权和知识产权风险。现有对策要么需要昂贵的权重编辑，要么依赖明确指定的编辑风格，限制了实际部署的实用性。

Method: DICE采用训练自由的框架，通过构建对比三元组让模型在潜在空间中区分风格与非风格特征，将解耦过程形式化为可解的广义特征值问题，并引入自适应注意力解耦编辑策略动态评估每个token的风格浓度。

Result: 实验表明DICE在风格擦除的彻底性和内容完整性保存方面达到了优越的平衡，仅需额外3秒时间进行风格解耦，提供了实用高效的技术来遏制风格模仿。

Conclusion: DICE为部署侧安全提供了一种实用高效的解决方案，通过风格净化而非风格编辑的方式，在保护艺术家风格的同时保持用户意图的内容完整性。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [98] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: ReRoPE是一个即插即用的框架，通过将相对相机姿态信息注入到预训练视频扩散模型的RoPE低频分量中，实现可控视角的视频生成，无需大量训练或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用相对于固定参考帧（如第一帧）的相机姿态，这种编码缺乏平移不变性，导致泛化能力差和累积漂移问题。相对相机姿态嵌入虽然更鲁棒，但难以在不增加大量训练成本或修改架构的情况下集成到预训练视频扩散模型中。

Method: ReRoPE基于Rotary Positional Embeddings（RoPE）在现有模型中未充分利用其全频谱带宽的观察，特别是低频分量。通过将相对相机姿态信息无缝注入这些未充分利用的频带，实现精确的相机控制，同时保留强大的预训练生成先验。

Result: 在图像到视频（I2V）和视频到视频（V2V）任务上评估了相机控制精度和视觉保真度。结果表明ReRoPE提供了训练高效的可控高保真视频生成路径。

Conclusion: ReRoPE是一个即插即用框架，能够在不损害生成能力的情况下将相对相机信息集成到预训练视频扩散模型中，为可控视频生成提供了高效解决方案。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [99] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 该论文提出了一种利用多模态大语言模型进行视频文本嵌入和检索的新方法，通过中间层分析和轻量级文本对齐策略，无需视觉监督即可实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成式多模态大语言模型通过微调产生通用表示，但在视频任务上的表现仍不如专门的视频基础模型。研究者希望探索如何更好地利用MLLMs进行视频文本嵌入和检索。

Method: 1. 首先进行系统性的逐层分析，发现中间层已编码大量任务相关信息；2. 将中间层嵌入与校准的MLLM头部结合，实现零样本检索；3. 引入轻量级文本对齐策略，将密集视频描述映射为简短摘要，实现无视觉监督的视频文本嵌入学习。

Result: 该方法在无需视觉监督的情况下，超越了现有方法，在常见视频检索基准测试中取得了最先进的结果，且优势明显。

Conclusion: 通过利用MLLMs中间层的丰富信息并结合文本对齐策略，可以在无需视觉监督的情况下实现强大的视频文本检索性能，为视频理解任务提供了新的有效途径。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [100] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: MMLSv2是一个用于火星表面滑坡分割的多模态数据集，包含7个波段图像，共664张训练/验证/测试图像和276张地理隔离测试图像，用于评估模型的空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的火星滑坡数据集有限，需要更全面的多模态数据集来支持滑坡分割研究，特别是评估模型在空间泛化方面的能力。

Method: 构建包含RGB、数字高程模型、坡度、热惯性和灰度通道的7波段多模态图像数据集，分为训练/验证/测试集和地理隔离测试集，使用多种分割模型进行评估。

Result: 数据集支持稳定训练并达到竞争性性能，但在碎片化、细长和小规模滑坡区域仍存在挑战；地理隔离测试集导致性能显著下降，验证了其评估模型鲁棒性和泛化能力的价值。

Conclusion: MMLSv2为火星滑坡分割研究提供了全面的多模态基准数据集，特别强调空间泛化评估，有助于推动滑坡检测模型在火星环境中的实际应用。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [101] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 本研究评估了Vision Transformer模型在xBD数据集上的建筑损伤分类性能，提出了一种针对性的基于补丁的预处理流程和冻结头微调策略，在噪声和不平衡数据上取得了与CNN基线相当的竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 灾后快速建筑损伤评估对应急响应至关重要。基于卫星影像的损伤分类模型提供了可扩展的态势感知手段，但卫星数据中的标签噪声和严重类别不平衡带来了重大挑战。xBD数据集为跨地理区域的建筑级损伤提供了标准化基准。

Method: 评估DINOv2-small和DeiT模型在xBD数据集上的多类别损伤分类性能。提出针对性的基于补丁的预处理流程来隔离结构特征并最小化训练中的背景噪声。采用冻结头微调策略以保持计算需求可控。

Result: 通过准确率、精确率、召回率和宏观平均F1分数评估模型性能。研究表明，采用新颖训练方法的小型ViT架构在灾害分类方面相对于先前的CNN基线取得了竞争性的宏观平均F1分数。

Conclusion: 小型Vision Transformer架构结合针对性的预处理和训练策略，能够在噪声和不平衡的卫星影像数据上实现有效的建筑损伤分类，为灾后快速评估提供了可行的技术方案。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [102] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: Fields of The World (FTW)生态系统提供全球农田边界数据集、预训练分割模型和推理工具，支持农田边界提取、作物分类和森林损失归因等农业应用


<details>
  <summary>Details</summary>
Motivation: 农田边界地图是农业数据产品的基础，支持作物监测、产量估算和病害评估等应用，但缺乏全球范围的标准化数据集和工具

Method: 构建包含24个国家160万农田多边形的基准数据集，提供预训练分割模型和命令行推理工具；使用MOSAIKS随机卷积特征和FTW农田边界进行农田级作物类型分类

Result: 作物类型分类的宏观F1分数达到0.65-0.75（使用有限标签）；在5个国家（476万平方公里）提供预计算预测，预测农田中位数面积从0.06公顷（卢旺达）到0.28公顷（瑞士）

Conclusion: FTW生态系统为全球农业监测提供了实用的数据集、模型和工具，能够支持从局部到国家尺度的农田边界提取和作物分类应用

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [103] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 该论文发现视觉语言模型(VLMs)在安全对齐中存在新漏洞：虽然模型预训练和指令调优能很好处理分割图像输入，但安全对齐通常只在完整图像上进行，无法识别分布在多个图像片段中的有害语义。作者提出了分割图像视觉越狱攻击(SIVA)，利用这种不对齐实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs通过偏好优化(如RLHF)进行了广泛的安全对齐，对传统的单图像/完整图像视觉越狱攻击表现出强大鲁棒性。然而，作者发现VLM预训练和指令调优能很好处理分割图像输入，但安全对齐通常只在完整图像上进行，无法识别分布在多个图像片段中的有害语义，这构成了新的安全漏洞。

Method: 提出了分割图像视觉越狱攻击(SIVA)，包含渐进式攻击策略：从简单的图像分割开始，到自适应白盒攻击，最终发展成黑盒迁移攻击。最强策略采用新颖的对抗知识蒸馏(Adv-KD)算法，显著提高了跨模型迁移性。

Result: 在三个最先进的现代VLMs和三个越狱数据集上的评估表明，作者的最强攻击比现有基线实现了高达60%的迁移成功率提升。

Conclusion: 该研究揭示了当前VLM安全对齐中的关键漏洞，并提出了高效的解决方案来应对这一安全威胁。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [104] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus是一个能够生成个性化3D形状的新框架，通过在几何和外观两个层面学习形状概念，实现从参考形状中提取可重用的属性并与文本结合生成新形状。


<details>
  <summary>Details</summary>
Motivation: 现有的3D形状生成方法通常缺乏细粒度控制和个性化能力，难以从参考形状中提取可重用的几何和外观属性，并在不同类别间灵活组合这些概念。

Method: 1. 将3D形状个性化定义为从参考形状中提取类别无关的几何和外观属性，并与文本组合生成新形状；2. 设计渐进优化策略，在几何和外观层面解耦形状概念学习；3. 扩展到区域级概念学习，使用上下文感知和无上下文损失。

Result: 实验结果表明，PEGAsus能够从广泛的参考形状中有效提取属性，并灵活地将这些概念与文本结合合成新形状，实现对形状生成的细粒度控制，支持创建多样化的个性化结果，甚至在跨类别场景中也能表现良好。

Conclusion: PEGAsus通过几何和外观层面的概念学习，实现了高质量的个性化3D形状生成，在定量和定性实验中都优于现有最先进方法，为细粒度形状控制提供了有效解决方案。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [105] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 提出MCSDR模型，使用基于分数的扩散模型进行生成式回归，解决超声心动图LVEF估计中的多模态分布问题


<details>
  <summary>Details</summary>
Motivation: 超声心动图估计左心室射血分数(LVEF)是一个不适定逆问题，存在噪声、伪影和有限视角导致的模糊性。传统深度学习方法使用均方误差回归，但只能学习条件期望，当后验分布为多模态或重尾时会产生误导性预测，这在病理场景中很常见。

Method: 提出多模态条件分数扩散回归模型(MCSDR)，这是一个概率框架，用于建模以超声心动图视频和患者人口统计学属性先验为条件的LVEF连续后验分布。采用生成式回归方法替代传统的确定性回归。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric和CAMUS数据集上的广泛实验表明，MCSDR实现了最先进的性能。定性分析显示，在高噪声或显著生理变异情况下，模型的生成轨迹表现出独特行为，为AI辅助诊断提供了新的可解释性层。

Conclusion: 从确定性回归向生成式回归的范式转变能更好地处理LVEF估计中的多模态分布问题，MCSDR框架不仅提高了性能，还通过生成轨迹提供了诊断可解释性。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [106] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出GR-CoT框架，通过地理空间推理链增强MLLMs的场景理解能力，解决遥感开放词汇分割中因光谱相似但语义不同导致的分类歧义问题


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割方法主要依赖视觉特征和文本嵌入的被动映射，缺乏地理空间上下文感知能力，导致在光谱特征相似但语义属性不同的地物类别上出现严重的语义歧义和误分类

Method: 提出Geospatial Reasoning Chain-of-Thought (GR-CoT)框架，包含离线知识蒸馏流和在线实例推理流。离线流建立细粒度类别解释标准；在线推理执行宏观场景锚定、视觉特征解耦和知识驱动决策合成的顺序推理过程，生成图像自适应词汇表

Result: 在LoveDA和GID5基准测试上进行了广泛实验，证明了该方法的优越性

Conclusion: GR-CoT框架通过增强MLLMs的地理空间推理能力，有效解决了遥感开放词汇分割中的语义歧义问题，实现了像素级的地理语义对齐

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [107] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的Chain-of-Caption框架，通过结合多种视觉和文本上下文信息，显著提升多模态大语言模型在指称表达理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在指称表达理解任务上已取得较高准确率，但通过工具使用等技术提供额外视觉或文本上下文可以进一步提升性能。本文旨在分析不同上下文提供技术对REC任务的影响。

Method: 提出无需训练的Chain-of-Caption框架，通过工具使用为MLLM提供额外的视觉和文本上下文信息，分析不同上下文组合对REC性能的影响。

Result: 在RefCOCO/RefCOCOg/RefCOCO+和Ref-L4数据集上的实验表明，单独的文本或视觉上下文都能在不微调的情况下提升REC性能。结合多种上下文后，框架在多个IoU阈值下的准确率比基线模型提升5%到30%。

Conclusion: 通过工具使用提供额外的视觉和文本上下文可以有效提升MLLM在REC任务上的性能，提出的Chain-of-Caption框架在无需训练的情况下实现了显著性能提升。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [108] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: 本文提出DeCI框架，通过分解BOLD信号中的周期和漂移成分，并采用通道独立建模，显著提升了fMRI脑疾病分类性能，优于传统功能连接和现有时序模型。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI脑疾病分类方法大多依赖基于皮尔逊相关的功能连接，将4D BOLD信号简化为静态2D矩阵，丢失了时序动态信息且只能捕捉线性区域间关系。

Method: 提出DeCI框架：1) 周期与漂移分解：将每个ROI的BOLD信号分解为周期性和漂移性成分；2) 通道独立性：独立建模每个ROI，提高鲁棒性并减少过拟合。

Result: 在五个公共数据集上的实验表明，DeCI在分类准确率和泛化能力上均优于基于功能连接的方法和现有时序模型基准。

Conclusion: 研究结果表明，直接建模fMRI时序信息（如周期振荡波动和缓慢基线漂移）对脑疾病分类至关重要，倡导在fMRI分析中转向端到时序建模以更好捕捉复杂脑动态。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [109] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: PISCO是一个视频扩散模型，用于通过任意稀疏关键帧控制实现精确的视频实例插入，解决了传统视频编辑中空间-时间定位、物理一致性和动态保持等挑战。


<details>
  <summary>Details</summary>
Motivation: AI视频生成正从依赖大量提示工程和"筛选"的通用生成，转向细粒度可控生成和高保真后处理。在专业AI辅助电影制作中，需要执行精确的目标修改，视频实例插入是实现这一转变的关键，要求保持场景完整性、精确时空定位、物理一致交互和原始动态保存。

Method: 提出PISCO视频扩散模型，支持单关键帧、起止关键帧或任意时间戳稀疏关键帧控制。引入可变信息引导以处理稀疏条件引起的分布偏移，分布保持时间掩码以稳定时间生成，以及几何感知条件以实现真实场景适应。

Result: 构建了PISCO-Bench基准数据集，包含已验证的实例标注和配对干净背景视频。实验表明PISCO在稀疏控制下持续优于强修复和视频编辑基线，随着控制信号增加呈现清晰单调的性能提升。

Conclusion: PISCO通过稀疏关键帧控制实现了精确的视频实例插入，解决了专业AI辅助电影制作中的关键需求，为可控视频生成提供了有效解决方案。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [110] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本文提出语言引导的运动离散化方法(LG-Tok)，通过自然语言对齐实现高效运动表征，在减少token数量的同时保持高质量重建，显著提升运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统运动离散化方法通常通过增加token数量来提高重建质量，但这会增加生成模型的学习难度。需要一种既能保持高质量重建又能降低生成复杂度的解决方案。

Method: 提出语言引导的离散化方法(LG-Tok)，将自然语言与运动在离散化阶段对齐，产生紧凑的高层语义表示。采用基于Transformer的Tokenizer利用注意力机制实现语言与运动的有效对齐，并设计语言丢弃方案使detokenizer支持无语言条件的生成。

Result: 在HumanML3D和Motion-X基准测试中，LG-Tok获得Top-1分数0.542和0.582，优于SOTA方法(MARDM: 0.500和0.528)；FID分数分别为0.057和0.088，优于0.114和0.147。LG-Tok-mini仅使用一半token仍保持竞争力。

Conclusion: 语言引导的运动离散化方法能够产生紧凑的语义表示，在减少token数量的同时保持高质量重建，显著简化生成模型学习，在运动生成任务中表现出优越性能。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [111] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: UGData数据集将街景图像与空间图对齐，提供空间推理路径和上下文描述；UGE两阶段训练策略对齐图像、文本和空间结构；UGBench基准测试评估空间嵌入在多种城市理解任务中的表现，在Qwen2.5-VL-7B上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 城市理解本质上是空间性的，但现有数据集和基准缺乏街景图像与城市结构的显式对齐，这限制了可迁移多模态嵌入的学习。

Method: 1) 引入UGData数据集，将街景图像锚定到结构化空间图，通过空间推理路径和空间上下文描述提供图对齐监督；2) 提出UGE两阶段训练策略，结合指令引导对比学习和基于图的空间编码，渐进稳定地对齐图像、文本和空间结构；3) 使用多个最先进VLM骨干（Qwen2-VL、Qwen2.5-VL、Phi-3-Vision、LLaVA1.6-Mistral）通过LoRA调优训练固定维度空间嵌入。

Result: 基于Qwen2.5-VL-7B骨干的UGE在训练城市上实现图像检索提升44%、地理位置排名提升30%；在未见城市上分别获得超过30%和22%的性能增益，证明了显式空间接地对空间密集型城市任务的有效性。

Conclusion: 显式空间接地对于学习可迁移的城市多模态嵌入至关重要，UGData数据集、UGE训练策略和UGBench基准测试共同构成了一个完整的框架，显著提升了空间密集型城市理解任务的性能。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [112] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 该研究提出了首个专门针对"图像思维"范式的过程奖励模型（PRM）综合基准，包含1,206条人工标注的推理轨迹，定义了7种细粒度错误类型，揭示了当前大视觉语言模型作为PRM的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大视觉语言模型（LVLMs）的发展，"图像思维"范式使模型能够在每个推理步骤中动态编辑和重新编码视觉信息。然而，这种范式带来了显著挑战，因为在推理过程中可能出现各种错误，需要过程奖励模型（PRMs）来区分正负推理步骤。现有的PRM基准主要是文本中心的，缺乏在这种范式下的全面评估。

Method: 1. 通过广泛分析推理轨迹和PRMs引导搜索实验，定义了7种细粒度错误类型；2. 构建了包含1,206条人工标注的"图像思维"推理轨迹的综合基准，涵盖4个类别和16个子类别；3. 通过实验分析评估当前LVLMs作为PRMs的有效性。

Result: 实验分析显示：1. 当前LVLMs作为有效的PRMs表现不足；2. 在视觉推理过程评估中能力有限；3. 在不同错误类型间存在显著的性能差异；4. 表现出正向评估偏差；5. 对推理步骤位置敏感。这些发现证明了所提基准的有效性。

Conclusion: 该研究建立了首个专门针对"图像思维"范式的PRM综合基准，揭示了当前LVLMs作为PRMs的局限性，为推进LVLMs中PRMs的发展奠定了重要基础。基准的有效性得到了验证，为未来研究提供了关键评估工具。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [113] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: GeoEdit是一个基于扩散模型的图像编辑框架，通过扩散变换器模块实现精确的几何变换（平移、旋转、缩放），并引入效果敏感注意力机制来增强光照和阴影效果的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像编辑中面临两个主要挑战：1）难以实现精确的几何变换（平移、旋转、缩放）；2）对复杂光照和阴影效果建模不足，导致结果不真实。

Method: 提出GeoEdit框架：1）利用扩散变换器模块通过上下文生成集成几何变换；2）引入效果敏感注意力机制增强光照和阴影建模；3）构建RS-Objects数据集（包含12万+高质量图像对）支持训练。

Result: 在公共基准测试上的大量实验表明，GeoEdit在视觉质量、几何精度和真实感方面持续优于最先进的方法。

Conclusion: GeoEdit通过结合几何变换和效果敏感注意力机制，有效解决了扩散模型在复杂场景中几何编辑和光照效果建模的挑战，显著提升了图像编辑的真实感和准确性。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [114] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: D²-VR：一种基于单图像扩散的视频修复框架，通过退化鲁棒流对齐和对抗蒸馏实现12倍加速，在保持高质量的同时提升时间稳定性


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散先验和时间对齐的视频修复方法虽然能提供优秀的感知质量，但在面对复杂真实世界退化时存在推理延迟高和时间不稳定的问题，限制了实际部署

Method: 1. 设计退化鲁棒流对齐模块，利用置信度感知注意力过滤不可靠运动线索；2. 采用对抗蒸馏范式将扩散采样轨迹压缩到快速少步机制；3. 设计协同优化策略平衡感知质量与时间一致性

Result: D²-VR在广泛实验中表现出最先进的性能，同时将采样过程加速12倍

Conclusion: 提出的D²-VR框架成功解决了扩散基视频修复方法的推理延迟和时间不稳定问题，实现了高效且高质量的实时视频恢复

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [115] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: RealSynCol是一个高度逼真的合成结肠镜数据集，用于解决深度学习在结肠镜检查中因缺乏大规模真实数据而受限的问题，显著提升了在临床图像上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在结肠镜检查中有潜力通过3D重建提供全面的黏膜表面和病变视图，并帮助识别未探索区域。然而，由于缺乏大规模真实数据，开发稳健方法受到限制。

Method: 从10个CT扫描中提取结肠几何结构，导入到模拟术中条件的虚拟环境中，使用真实血管纹理进行渲染。生成包含28,130帧的数据集，配有深度图、光流、3D网格和相机轨迹等真实标注。

Result: 进行了基准研究评估现有合成结肠数据集在深度和姿态估计任务上的表现。结果显示RealSynCol的高真实性和变异性显著提升了在临床图像上的泛化性能。

Conclusion: RealSynCol是一个强大的工具，可用于开发支持内镜诊断的深度学习算法，其高真实性和变异性使其在临床应用中具有重要价值。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [116] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 研究发现注意力稀疏图像匹配模型的关键设计选择，揭示检测器比描述符对性能影响更大，提出通用检测器无关模型，在零样本匹配中达到或超越专用模型性能。


<details>
  <summary>Details</summary>
Motivation: 重新审视基于注意力的稀疏图像匹配模型训练问题，识别先前被忽视的关键设计选择，研究检测器和描述符在基于transformer的匹配框架中的作用，旨在开发通用、检测器无关的图像匹配模型。

Method: 首先识别影响LightGlue模型性能的关键设计选择；然后分析检测器和描述符在transformer匹配框架中的作用；最后提出使用多种检测器的关键点微调现有图像匹配模型的方法，创建通用检测器无关模型。

Result: 发现检测器（而非描述符）通常是性能差异的主要原因；提出的通用检测器无关模型在作为零样本匹配器用于新检测器时，达到或超过专门为这些特征训练的模型的准确性。

Conclusion: 研究结果为基于transformer的匹配模型部署和局部特征未来设计提供了有价值的见解，证明了开发通用检测器无关匹配模型的可行性。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [117] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出了演示驱动的视频上下文学习任务，并构建了Demo-ICL-Bench基准来评估模型从少量示例中学习的能力，同时开发了Demo-ICL模型来应对这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估模型基于静态内部知识的能力，而非从动态、新颖上下文中进行少样本学习和适应的能力。为了弥补这一差距，需要开发能够评估视频上下文学习能力的新基准。

Method: 1. 提出演示驱动的视频上下文学习任务；2. 构建Demo-ICL-Bench基准，包含1200个教学视频及相关问题，提供文本和视频两种演示类型；3. 开发Demo-ICL模型，采用两阶段训练策略：视频监督微调和信息辅助直接偏好优化。

Result: 实验表明Demo-ICL-Bench对现有最先进MLLM具有挑战性，而Demo-ICL模型在该基准上表现出有效性，揭示了未来研究方向。

Conclusion: 该研究填补了视频上下文学习评估的空白，提出的基准和模型为视频理解中的少样本学习能力评估提供了新方向，推动了MLLM在动态上下文学习方面的发展。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [118] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Vista是一个用于流式视频问答的场景感知框架，通过动态场景分割、压缩和召回机制，在保持高效性的同时实现长上下文推理。


<details>
  <summary>Details</summary>
Motivation: 流式视频问答面临独特挑战：视频帧顺序到达，用户查询可在任意时间点发出。现有基于固定大小内存或简单压缩的方法常导致上下文丢失或内存溢出，限制了在长视频、实时场景中的有效性。

Method: Vista的创新包括三个方面：1) 场景感知分割 - 动态将传入帧聚类为时间和视觉一致的场景单元；2) 场景感知压缩 - 将每个场景压缩为紧凑的token表示存储在GPU内存中，全分辨率帧卸载到CPU内存；3) 场景感知召回 - 收到查询时选择性召回相关场景并重新整合到模型输入中。

Result: 在StreamingBench上的大量实验表明，Vista实现了最先进的性能，为真实世界流式视频理解建立了强大基准。

Conclusion: Vista是一个模型无关的框架，可与多种视觉语言骨干无缝集成，在不影响延迟或内存效率的情况下实现长上下文推理，为流式视频问答提供了高效可扩展的解决方案。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [119] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: TriC-Motion是一个新颖的基于扩散的文本到动作生成框架，通过整合时空频域建模与因果干预，解决了现有方法在联合优化和噪声解耦方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法主要关注时空建模或独立的频域分析，缺乏跨空间、时间和频率域的统一联合优化框架，这限制了模型同时利用所有域信息的能力，导致生成质量不理想。此外，动作生成框架中由噪声引起的动作无关线索常与有益特征纠缠，导致动作失真。

Method: 提出Tri-Domain Causal Text-to-Motion Generation (TriC-Motion)框架，包含三个核心建模模块：时间动作编码、空间拓扑建模和混合频率分析。通过分数引导的三域融合模块整合有价值信息，同时设计基于因果的反事实动作解耦器来消除噪声，解耦每个域的真实建模贡献。

Result: 在HumanML3D数据集上取得了出色的性能，R@1达到0.612，优于现有最先进方法。实验结果表明该框架能够生成高保真、连贯、多样且与文本对齐的动作序列。

Conclusion: TriC-Motion通过整合时空频域建模与因果干预，成功解决了文本到动作生成中的联合优化和噪声解耦问题，实现了高质量的动作生成，为相关领域提供了新的解决方案。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [120] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 提出基于2D姿态估计的手势分类框架，用于自动驾驶车辆理解行人手势，在真实交通视频上实现87%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 手势在交通非语言交流中至关重要，但自动驾驶车辆难以理解行人手势，当正式交通规则不足时，手势能帮助行人-驾驶员互动

Method: 使用2D姿态估计技术处理WIVW数据集中的真实世界视频序列，将手势分为四类（停止、前进、感谢问候、无手势），从归一化关键点提取76个静态和动态特征

Result: 分析显示手部位置和移动速度在区分手势类别方面特别有效，分类准确率达到87%

Conclusion: 该框架不仅提升了自动驾驶系统的感知能力，还增进了对交通环境中行人行为的理解

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [121] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 该研究探讨了光照变化对多类别食品识别系统的影响，通过构建合成光照增强数据集来提升模型在真实世界环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视觉食品识别系统（如自动传送带检测）对光照变化引起的领域偏移高度敏感。现有研究通常局限于单一食品类别或受控环境，且大多数公共食品数据集缺乏明确的光照标注，需要研究光照变化对多类别食品识别的影响。

Method: 使用Food-101和Fruits-360两个广泛采用的数据集，通过系统变化光照色温和强度构建合成光照增强数据集，进行跨数据集评估、迁移学习和领域泛化研究，特别关注苹果类等光照敏感目标类别。

Result: 实验结果显示，跨数据集评估中由于视觉条件不匹配导致准确率显著下降，而光照感知增强方法显著提高了在领域偏移下的识别鲁棒性，同时保持了实时性能。

Conclusion: 光照鲁棒性对于在真实世界检测场景中部署可靠的食品识别系统至关重要，光照感知增强方法能有效提升系统在光照变化环境下的性能表现。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [122] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: GeoFocus是一个解决几何问题的新框架，通过关键局部感知器和VertexLang语言，在多个几何数据集上取得4.7%的准确率提升


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在解决几何问题时面临挑战，需要同时关注全局形状识别和与几何理论相关的复杂局部关系

Method: 提出GeoFocus框架，包含两个核心模块：1) 关键局部感知器，通过13个基于理论的感知模板自动识别和强调关键局部结构；2) VertexLang，一种紧凑的拓扑形式语言，通过顶点坐标和连接关系编码全局图形

Result: 在Geo3K、GeoQA和FormalGeo7K数据集上，GeoFocus比领先的专业模型准确率提升4.7%；关键局部特征覆盖率提升61%；全局感知训练时间减少20%；在MATHVERSE中表现出更好的鲁棒性

Conclusion: GeoFocus通过结合局部结构感知和全局拓扑编码，有效提升了大型多模态模型解决几何问题的能力，在准确率和效率方面都有显著改进

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [123] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 提出一种基于双网格离散化的自动正则化参数选择方法，通过反馈控制算法动态调整正则化强度，在X射线断层扫描中实现参数自动优化。


<details>
  <summary>Details</summary>
Motivation: X射线断层扫描中的图像重建是一个不适定逆问题，特别是在数据有限的情况下。正则化是必要的，但其效果取决于正则化参数的选择，该参数需要在数据保真度和先验信息之间取得平衡。

Method: 提出一种基于两个不同计算离散化的自动参数选择方法。使用反馈控制算法动态调整正则化强度，驱动迭代重建朝着能够使两个网格上的重建结果达到足够相似性的最小参数值收敛。

Result: 该方法在真实断层扫描数据上证明了其有效性。

Conclusion: 提出的双网格离散化方法结合反馈控制算法，能够自动选择最优正则化参数，解决了X射线断层扫描中参数选择的难题。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [124] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏单目图的热成像SLAM系统，使用在可见光谱数据上训练的通用学习特征（SuperPoint检测器和LightGlue匹配器），通过预处理和置信度加权因子图提高热成像SLAM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 热成像在视觉退化环境（如低光照、烟雾或恶劣天气）中为视觉SLAM提供了实用的感知方式。然而，热图像通常具有低纹理、低对比度和高噪声的特点，这使得基于特征的SLAM变得复杂。现有方法在热成像数据上表现不佳，且缺乏高质量的热成像训练数据。

Method: 1. 使用在可见光谱数据上训练的通用学习特征（SuperPoint检测器和LightGlue匹配器），利用其跨域泛化能力；2. 引入预处理流程增强热图像输入的适用性；3. 修改核心SLAM模块以处理稀疏和异常值较多的特征匹配；4. 将SuperPoint的关键点置信度分数纳入置信度加权因子图，提高估计鲁棒性。

Result: 在公开热成像数据集上的评估表明，所提出的系统实现了可靠的性能，无需数据集特定的训练或微调特征检测器，解决了高质量热成像数据稀缺的问题。

Conclusion: 该研究提出了一种有效的热成像SLAM解决方案，通过利用可见光谱训练的通用学习特征和置信度加权因子图，在无需热成像特定训练的情况下实现了鲁棒的SLAM性能，为视觉退化环境下的定位与建图提供了实用方法。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [125] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出TIBR4D框架，通过两阶段迭代边界优化实现动态4D高斯场景中的高效无学习对象分割


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的对象级分割面临复杂运动、遮挡和模糊边界的挑战，现有方法难以有效处理这些问题

Method: TIBR4D框架包含两阶段：1) 时间片段级的迭代高斯实例追踪(IGIT)，通过迭代追踪逐步优化高斯到实例的概率；2) 帧级高斯渲染范围控制(RCC)，抑制边界附近高度不确定的高斯点。同时提出时间分割合并策略平衡身份一致性和动态感知

Result: 在HyperNeRF和Neu3D数据集上的实验表明，该方法相比SOTA方法能生成边界更清晰、效率更高的对象高斯点云

Conclusion: TIBR4D框架通过两阶段迭代边界优化，有效解决了动态4D高斯场景中的对象分割问题，在保持边界准确性和处理遮挡方面表现出色

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [126] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-Edit：一种在线跨模态模型编辑方法，通过将几何感知线索集成到通用目标跟踪器中，结合2D语义与3D几何推理，显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 人类感知利用3D先验知识和语义推理进行有效目标跟踪，而现有通用目标跟踪方法主要依赖2D特征，忽略了3D几何线索，导致在部分遮挡、干扰物以及几何和外观变化时性能下降。

Method: 提出GOT-Edit在线跨模态模型编辑方法：1）利用预训练的视觉几何基础Transformer从少量2D图像推断几何线索；2）通过零空间约束更新进行在线模型编辑，在保持语义区分能力的同时整合几何信息。

Result: 在多个通用目标跟踪基准测试上的广泛实验表明，GOT-Edit实现了卓越的鲁棒性和准确性，特别是在遮挡和杂乱场景下，为结合2D语义与3D几何推理建立了新范式。

Conclusion: GOT-Edit通过在线模型编辑成功地将几何感知线索集成到通用目标跟踪器中，显著提升了跟踪性能，特别是在具有挑战性的场景下，展示了结合2D语义与3D几何推理的有效性。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [127] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: SemiNFT是一个基于扩散变换器的色彩修图框架，通过模仿人类艺术训练轨迹（从刚性模仿到直觉创作），结合配对数据学习和无配对数据强化学习，实现了超越简单统计匹配的美学理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于参考图像的色彩修图方法主要依赖像素级统计的全局色彩映射，缺乏对语义上下文和人类美学的真正理解，无法为非专业人士提供高质量的修图效果。

Method: 提出SemiNFT框架：1）使用配对三元组数据进行基础训练，学习结构保持和色彩映射技能；2）在无配对数据上进行强化学习，培养细致的美学感知；3）设计混合在线-离线奖励机制，在美学探索中保持结构回顾，防止灾难性遗忘。

Result: SemiNFT在标准预设迁移基准测试中优于现有最先进方法，并在零样本任务（如黑白照片着色和跨域预设迁移）中表现出显著智能，证实其超越了简单的统计匹配。

Conclusion: SemiNFT通过模仿人类艺术学习轨迹，实现了对美学理解的复杂层次，为色彩修图领域提供了新的解决方案，使高质量修图对非专业人士更加可及。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [128] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 本文回顾了中国AVS PCC点云压缩标准，从技术工具和性能比较两个角度分析该标准的特点和优势。


<details>
  <summary>Details</summary>
Motivation: 点云作为重要的3D数据表示格式，在沉浸式媒体、自动驾驶、数字遗产保护等领域有广泛应用价值，但其大数据量对传输和存储带来挑战。虽然MPEG已制定G-PCC和V-PCC标准，但中国AVS工作组也开发了自己的第一代点云压缩标准AVS PCC，该标准采用了不同于其他标准的新编码工具和技术。

Method: 论文从两个角度回顾AVS PCC标准：1）相关技术分析，介绍AVS PCC采用的新编码工具和技术；2）性能比较，将AVS PCC与其他标准（如MPEG的G-PCC和V-PCC）进行对比评估。

Result: AVS PCC标准采用了多种新的编码工具和技术，这些工具在性能上与其他标准有所不同。通过技术分析和性能比较，展示了AVS PCC标准的特点和优势。

Conclusion: AVS PCC作为中国自主制定的点云压缩标准，采用了创新的编码工具和技术，为点云压缩提供了新的解决方案，对推动点云技术在实际应用中的部署具有重要意义。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [129] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: Inspiration Seeds是一个生成式框架，将图像生成从最终执行转向探索性构思，通过两个输入图像产生多样化的视觉连贯组合，无需文本提示


<details>
  <summary>Details</summary>
Motivation: 现有生成模型主要针对精心设计的文本提示进行优化，不支持开放式的视觉探索，而设计师经常从松散连接的视觉参考中寻找灵感，寻求激发新想法的涌现性连接

Method: 使用前馈模型，在通过CLIP稀疏自编码器提取CLIP潜在空间编辑方向并分离概念对的基础上，完全通过视觉手段训练合成三元组分解的视觉方面

Result: 模型能够产生多样化的视觉连贯组合，揭示输入图像之间的潜在关系，无需依赖用户指定的文本提示

Conclusion: 通过消除对语言的依赖并实现快速直观的重组，该方法支持创意工作早期和模糊阶段的视觉构思

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [130] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: LV-RAE是一种表示自编码器，通过增强语义特征的低级信息来提升潜在扩散模型的重建保真度，同时解决高维潜在空间导致的解码器敏感性问题


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型作为图像编码器虽然提升了潜在扩散模型的生成性能，但其语义特征缺乏低级信息（如颜色和纹理），导致重建保真度下降，成为扩展LDMs的主要瓶颈

Method: 提出LV-RAE表示自编码器，增强语义特征的低级信息；通过微调解码器提高鲁棒性，并通过受控噪声注入平滑生成的潜在表示

Result: LV-RAE显著提高了重建保真度，同时保持了语义抽象能力，实现了强大的生成质量

Conclusion: LV-RAE通过增强语义特征的低级信息和解决解码器敏感性问题，有效提升了潜在扩散模型的性能，为扩展LDMs提供了有效解决方案

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [131] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 该论文研究了Vision Transformers中全局特征（[CLS]类标记）与局部特征（补丁标记）之间的学习冲突，提出通过专门的归一化层和投影路径来解耦这两种标记的处理，从而提升密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers通常将可学习的[CLS]类标记与补丁标记一起处理，但两者本质不同（全局vs局部特征）。作者发现标准归一化层在这两种标记之间引入了隐式差异，导致全局和局部特征学习存在摩擦，影响密集预测任务的性能。

Method: 通过分析类标记和补丁标记之间的交互，提出专门的处理路径，特别是在归一化层和早期查询-键-值投影中解耦这两种标记的计算流程。这种有针对性的专业化处理保持了分类准确性，同时提升了补丁表示质量。

Result: 在标准基准测试中，分割性能提升了超过2 mIoU点，同时保持了强大的分类准确性。提出的修改仅增加了8%的参数，没有额外的计算开销。该方法在不同模型规模和学习框架中都具有良好的泛化能力。

Conclusion: 通过专门处理类标记和补丁标记，特别是解耦归一化层和早期投影，可以有效改善Vision Transformers在密集预测任务中的性能，同时保持分类能力。这种方法为Transformer架构的改进提供了新的方向。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [132] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 提出基于深度学习的模型，使用低分辨率预扫描缩略图预测病理切片固定类型，相比现有方法处理速度快400倍，在TCGA数据集上AUROC达0.88


<details>
  <summary>Details</summary>
Motivation: 病理切片固定类型的手动标注容易出错，影响下游分析和诊断准确性。现有方法需要全分辨率全玻片图像，限制了高通量质量控制的可扩展性。

Method: 开发深度学习模型，利用低分辨率预扫描缩略图像预测福尔马林固定石蜡包埋（FFPE）和冰冻切片（FS）固定类型。模型在TUM病理研究所的1200张WSI上训练，在TCGA、奥格斯堡和雷根斯堡数据集上评估。

Result: 模型在TCGA数据集上AUROC达到0.88，比可比预扫描方法提高4.8%。在雷根斯堡和奥格斯堡数据集上AUROC为0.72，揭示了扫描仪引起的领域偏移挑战。每张切片处理时间仅21毫秒，比现有高倍率全分辨率方法快400倍。

Conclusion: 该方法为检测标注错误提供了高效解决方案，无需依赖高倍率扫描，是病理高通量工作流程中有价值的质量控制工具。未来工作将改进模型对不同扫描仪类型的泛化能力。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [133] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow是一个基于WiFi信号的连续人体姿态估计框架，采用编码器-解码器架构，在自收集数据集上达到97.00% PCK@20和99.48% PCK@50的精度，模型参数仅4.82M，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 物联网中的人体姿态估计对智能感知至关重要，但现有WiFi方法在连续运动和计算开销方面存在不足。视觉方法将CSI视为图像处理，未能充分利用信号的原始时序结构。

Method: 提出WiFlow框架，采用编码器-解码器架构。编码器使用时序和非对称卷积捕获CSI的时空特征，保持信号原始时序结构；通过轴向注意力精炼人体关键点特征并捕捉结构依赖关系；解码器将编码的高维特征映射为关键点坐标。

Result: 在自收集的36万同步CSI-姿态样本数据集上，WiFlow达到PCK@20为97.00%，PCK@50为99.48%，平均关节位置误差0.008米。模型仅4.82M参数，显著降低了复杂度和计算成本。

Conclusion: WiFlow为实用的WiFi人体姿态估计建立了新的性能基准，在保持高精度的同时大幅降低模型复杂度，适用于物联网中的智能感知应用。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [134] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: ALIVE是一个音频-视频生成模型，通过改进MMDiT架构实现文本/参考图像到音视频的生成和动画功能，在百万级高质量数据上训练后性能优于开源模型并匹敌商业方案。


<details>
  <summary>Details</summary>
Motivation: 视频生成正快速向统一的音频-视频生成发展，现有文本到视频模型缺乏音频生成和参考动画能力，需要开发能够同时生成同步音频和视频内容的模型。

Method: 1. 基于预训练T2V模型改进，采用MMDiT架构并增加联合音频-视频分支；2. 引入TA-CrossAttn进行时间对齐的跨模态融合和UniTemp-RoPE实现精确的音频-视频对齐；3. 设计包含音频-视频标注和质量控制的完整数据管道收集高质量微调数据。

Result: 在百万级高质量数据上进行持续预训练和微调后，ALIVE表现出色，一致优于开源模型，匹配或超越最先进的商业解决方案，并建立了新的基准测试进行全面评估。

Conclusion: ALIVE成功实现了Sora风格的音频-视频生成和动画功能，通过详细的实现方案和基准测试，为社区开发音频-视频生成模型提供了高效支持。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [135] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: 该论文提出AGI本质是压缩问题，视觉架构应与视频信息论原理对齐。OneVision-Encoder通过Codec Patchification聚焦信号熵丰富的区域，使用3D RoPE统一时空推理，在大规模聚类判别目标下训练，在多项视觉任务上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现代视觉架构偏离了信息论基本原则：视觉信号高度冗余，判别信息稀疏。当前模型均匀处理密集像素网格，浪费大量计算在静态背景上，而非聚焦定义运动和意义的预测残差。需要使架构与视频信息论原理（编解码器）对齐。

Method: OneVision-Encoder采用Codec Patchification，放弃均匀计算，专注于信号熵丰富的3.1%-25%区域。使用共享3D RoPE统一不规则token布局下的时空推理，通过超过100万个语义概念的大规模聚类判别目标进行训练，联合捕捉物体持久性和运动动态。

Result: 验证了核心假设：效率与准确性正相关。集成到LLM后，在16个图像、视频和文档理解基准测试中持续优于Qwen3-ViT和SigLIP2等强视觉骨干，尽管使用更少的视觉token和预训练数据。在视频理解任务上平均提升4.1%。

Conclusion: 编解码器对齐的patch级稀疏性是基本原则，使OV-Encoder成为下一代视觉通用模型的扩展引擎。效率与准确性不是权衡关系，而是正相关的。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [136] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 提出Omni Dense Captioning新任务，构建高质量基准数据集OmniDCBench和训练数据集TimeChatCap-42K，开发TimeChat-Captioner-7B模型，在音频-视觉推理和时间定位任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述任务通常生成简短、不连续的描述，缺乏细粒度和时间结构信息。需要一种能够生成连续、细粒度、结构化音频-视觉叙事的方法，使读者能够像阅读电影剧本一样生动想象视频内容。

Method: 1) 提出六维结构模式创建"脚本式"描述；2) 构建高质量人工标注基准OmniDCBench；3) 提出SodaM统一评估指标；4) 构建训练数据集TimeChatCap-42K；5) 开发TimeChat-Captioner-7B模型，使用SFT和GRPO训练，并设计任务特定奖励。

Result: TimeChat-Captioner-7B在密集描述任务上超越Gemini-2.5-Pro，达到SOTA性能。生成的密集描述显著提升下游任务能力：音频-视觉推理（DailyOmni和WorldSense）和时间定位（Charades-STA）。

Conclusion: 提出的Omni Dense Captioning任务和相应方法能够生成高质量、结构化的音频-视觉叙事。TimeChat-Captioner-7B模型表现出色，不仅在本任务上领先，还能有效提升下游任务性能。所有资源将开源促进研究。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [137] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 该研究首次对视觉语言模型（VLM）的适应过程进行机制分析，通过阶段式模型差异分析技术，揭示了语言模型如何学习"看"的能力，识别出视觉偏好特征的出现和重定向，并追踪到这些特征的空间编码能力和因果激活机制。


<details>
  <summary>Details</summary>
Motivation: 尽管当代视觉语言模型在各种任务上表现出色，但语言主干表示在多模态训练中如何适应以及视觉特定能力何时出现仍不清楚。本研究旨在通过机制分析揭示VLM适应过程的内在机理。

Method: 采用阶段式模型差异分析技术，该技术能够分离多模态微调过程中引入的表征变化。通过识别视觉偏好特征的出现和重定向，分析这些特征如何编码空间关系，并追踪这些特征的因果激活到特定的注意力头。

Result: 研究发现：1）识别出在微调过程中出现或重定向的视觉偏好特征；2）这些特征的一个选择性子集能够可靠地编码空间关系，通过对空间提示的受控偏移得以揭示；3）这些特征的因果激活可追溯到一小部分注意力头。

Conclusion: 阶段式模型差异分析揭示了空间基础多模态特征何时何地出现，提供了对模态融合的更清晰视角，展示了视觉基础如何重塑先前仅为文本的特征。该方法增强了多模态训练的可解释性，为理解和改进预训练语言模型获取视觉基础能力提供了基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [138] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文研究了在零样本条件下使用预训练基础模型进行CT和MR图像解剖区域检测的方法，提出了三种无需训练的流程，其中基于分割的规则系统表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前解剖区域识别主要依赖不可靠的DICOM元数据或监督学习方法，限制了在实际医疗场景中的应用。本研究旨在探索是否可以利用预训练基础模型中的知识，实现完全零样本的CT和MR图像解剖区域检测。

Method: 提出了三种无需训练的流程：1）基于预训练多器官分割模型的分割驱动规则系统；2）遵循放射科医生定义规则的多模态大语言模型；3）结合视觉输入和解剖证据的分割感知多模态大语言模型。所有方法在887个CT和MR扫描上进行了评估。

Result: 分割驱动的规则方法表现最佳且最稳定，加权F1分数分别为0.947（CT）和0.914（MR），在不同模态和非典型扫描范围下都表现出鲁棒性。多模态大语言模型在视觉特征明显的区域表现有竞争力，而分割感知多模态大语言模型显示出基本局限性。

Conclusion: 研究表明，利用预训练基础模型中的知识可以实现有效的零样本解剖区域检测，其中基于分割的规则系统是最可靠的方法，为医疗影像工作流程提供了不依赖DICOM元数据的替代方案。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [139] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit是一个无需训练的文本引导图像编辑框架，通过软掩码融合和注意力融合实现精确可控的编辑，避免硬掩码边界带来的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像编辑方法使用显式二进制掩码约束编辑，但硬掩码边界会引入伪影并降低可编辑性，需要解决这些问题以实现更自然、更可控的图像编辑。

Method: 1) 通过测量源提示词和目标提示词之间的语义差异自动识别编辑区域和保留区域；2) 在区域边界进行距离感知的潜在融合生成软掩码，并使用总变差损失确保平滑过渡；3) 在DiT注意力层中利用AdaIN调制进行统计注意力融合，增强编辑区域的可编辑性同时保持与源图像的全局一致性。

Result: 大量实验表明，FusionEdit在文本引导图像编辑任务上显著优于现有最先进方法，能够实现更自然、更精确的编辑效果。

Conclusion: FusionEdit通过软掩码融合和注意力融合机制，有效解决了硬掩码边界带来的伪影问题，实现了更精确可控的文本引导图像编辑，无需训练即可获得高质量的编辑结果。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [140] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出一种混合2D-3D深度学习框架，用于从欠采样CT体积中去除伪影，平衡计算效率与体积一致性


<details>
  <summary>Details</summary>
Motivation: 欠采样CT体积虽然减少了采集时间和辐射暴露，但会引入伪影，降低图像质量和诊断效用。需要一种高效的方法来减少这些伪影，实现高质量成像。

Method: 提出两阶段混合深度学习框架：首先使用2D U-Net处理欠采样CT体积的单个切片提取特征图，然后将这些切片特征图堆叠作为输入，通过3D解码器利用切片间的上下文信息预测无伪影的3D CT体积。

Result: 该方法在冠状面和矢状面方向上显著改善了切片间一致性，同时保持了较低的计算开销。混合框架为高质量3D CT图像后处理提供了稳健高效的解决方案。

Conclusion: 提出的混合2D-3D深度学习框架成功平衡了2D处理的计算效率与3D建模的体积一致性，为欠采样CT体积的伪影去除提供了有效且高效的方法。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [141] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: CGA框架通过检测类别混淆、构建混淆感知提示和对齐特征表示，显著提升了源自由域自适应在细粒度场景下的性能


<details>
  <summary>Details</summary>
Motivation: 现有源自由域自适应方法在细粒度场景下表现不佳，主要原因是忽略了源模型在目标域中存在的非对称动态类别混淆问题，导致伪标签噪声大和目标域判别能力差

Method: 提出CLIP引导对齐框架，包含三个模块：MCA检测方向性混淆对，MCC利用CLIP构建混淆感知文本提示，FAM通过对比学习对齐CLIP和源模型的特征表示以减少表示空间模糊性

Result: 在多个数据集上的实验表明，CGA在源自由域自适应任务中持续优于现有方法，在易混淆和细粒度场景下提升尤为显著

Conclusion: 显式建模类别间混淆对于有效的源自由域自适应至关重要，CGA框架通过检测混淆、构建上下文敏感提示和对齐特征表示，显著提升了模型在目标域的适应能力

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [142] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: HATCH训练框架通过补丁级空间对齐和行动-答案推理机制，提升多模态大语言模型在多图像空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在单图像空间推理方面已有显著进展，但在需要整合多个视角信息的多图像空间推理任务上仍面临挑战。认知研究表明人类通过跨视角对应和逐步视角变换两种机制解决此类任务，但现有研究仅部分且隐式地结合这些机制。

Method: 提出HATCH训练框架，包含两个互补目标：1) 补丁级空间对齐，鼓励不同视角中空间对应区域的补丁表示对齐；2) 行动-答案推理，要求模型在预测最终答案前生成明确的视角转换行动。

Result: 在三个基准测试上的实验表明，HATCH始终以明显优势超越同等规模的基线模型，并与更大模型取得竞争性结果，同时保持单图像推理能力。

Conclusion: HATCH通过显式监督跨视角对应和逐步视角变换，有效提升了多模态大语言模型在多图像空间推理任务中的性能，为复杂空间推理提供了新的训练范式。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [143] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 提出Instance-Disentangled Attention机制，解决流匹配模型在多实例编辑中语义干扰问题，实现单次传递的实例级图像编辑


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图像编辑器主要支持全局或单指令编辑，在多实例场景中难以独立编辑多个部分而不产生语义干扰，这源于全局条件化的速度场和联合注意力机制导致并发编辑纠缠

Method: 引入Instance-Disentangled Attention机制，通过分区联合注意力操作，在速度场估计期间强制实例特定文本指令与空间区域之间的绑定，实现编辑解耦

Result: 在自然图像编辑和新引入的文本密集信息图基准测试中，该方法促进了编辑解耦和局部性，同时保持全局输出连贯性，实现了单次传递的实例级编辑

Conclusion: Instance-Disentangled Attention机制有效解决了流匹配模型在多实例编辑中的局限性，实现了更精确、独立的区域级编辑能力

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [144] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: MVAnimate是一个利用多视角先验信息生成高质量2D和3D角色动画的新框架，解决了现有动画生成算法输出质量低和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D或3D人体姿态建模的动画生成算法面临输出质量低和训练数据不足的问题，无法生成高质量动画视频，需要新的解决方案来提升动画质量。

Method: MVAnimate是一个新颖的框架，基于多视角先验信息合成动态人物的2D和3D信息，利用多视角先验生成时间一致和空间连贯的动画输出，并优化目标角色的多视角视频质量。

Result: 实验结果表明，该方法在多样数据集上表现出色，能够处理各种运动模式和外观，相比现有动画方法有显著改进，生成的动画质量更高。

Conclusion: MVAnimate通过利用多视角先验信息有效提升了角色动画的生成质量，为高质量动画视频生成提供了新的解决方案，在多样运动模式和外观处理上表现出鲁棒性。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [145] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 提出了一种基于符号吠陀计算的确定性CPU导向的说话头生成框架，用于教育资源受限环境中的教育虚拟人合成


<details>
  <summary>Details</summary>
Motivation: 现有的说话头生成方法依赖GPU神经渲染、大规模训练集或高容量扩散模型，难以在离线或资源受限的学习环境中部署，需要一种轻量级CPU友好的解决方案

Method: 使用符号吠陀计算方法：将语音转换为时间对齐的音素流，映射到紧凑的视素库，通过受吠陀经Urdhva Tiryakbhyam启发的符号协同发音生成平滑视素轨迹，轻量级2D渲染器执行ROI变形和嘴部合成与稳定

Result: 在仅CPU执行下实现了可接受的唇同步质量，显著降低了计算负载和延迟，支持在低端硬件上部署实用的教育虚拟人

Conclusion: 该确定性CPU导向框架能够在资源受限环境中实现可接受的说话头生成质量，为教育技术中的虚拟人部署提供了实用解决方案

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [146] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 提出多模态框架结合图像和力测量数据，改进受电弓-接触网接口电弧检测，解决电弧瞬态性、噪声环境、数据稀缺等问题


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网接口的电弧现象对铁路供电系统构成严重风险，包括加速部件磨损、性能下降和服务中断。电弧检测面临瞬态特性、噪声环境、数据稀缺以及与其他瞬态现象难以区分的挑战

Method: 提出多模态框架结合高分辨率图像和力测量数据；构建两个电弧检测数据集（瑞士联邦铁路数据和公开视频+合成力数据）；提出MultiDeepSAD算法扩展DeepSAD到多模态；引入针对每种数据类型的伪异常生成技术

Result: 通过大量实验和消融研究，证明该框架显著优于基线方法，即使在领域偏移和真实电弧观测数据有限的情况下，对真实电弧事件也表现出增强的敏感性

Conclusion: 多模态框架结合视觉和力测量数据能够更准确、更鲁棒地检测受电弓-接触网接口的电弧事件，为解决铁路供电系统中的电弧检测挑战提供了有效方案

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [147] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA是一个开源的音视频联合生成模型，采用混合专家架构，支持从图像和文本生成同步的高质量音视频内容。


<details>
  <summary>Details</summary>
Motivation: 当前音视频生成主要依赖级联管道，成本高且容易累积误差。现有系统如Veo 3和Sora 2虽然强调同时生成的重要性，但都是闭源的，限制了该领域的发展。需要开发开源的高质量音视频联合生成模型。

Method: 采用混合专家架构，总参数量320亿，推理时激活180亿参数。支持IT2VA任务，能够生成唇语同步的语音、环境感知音效和内容对齐的音乐。

Result: 开发出MOVA开源模型，能够生成高质量、同步的音视频内容。发布了模型权重和代码，支持高效推理、LoRA微调和提示增强。

Conclusion: MOVA作为开源音视频联合生成模型，解决了现有系统的局限性，旨在推动研究和创作者社区的发展。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [148] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出半监督师生框架，结合不确定性感知伪标签教师和基于置信度的渐进课程学习，在有限标注下实现高效脑肿瘤分割


<details>
  <summary>Details</summary>
Motivation: 解决MRI脑肿瘤分割中标注成本高和数据异质性（不同扫描仪和站点）的问题，在有限监督下提升分割性能

Method: 1) 不确定性感知伪标签教师模型生成概率掩码和像素级不确定性；2) 基于置信度的渐进课程学习，按图像级置信度分阶段引入未标注数据；3) 双损失目标函数，让学生从高置信区域学习并"忘记"低置信区域；4) 基于一致性的伪标签细化

Result: 在BraTS 2021上，验证集DSC从0.393（10%数据）提升到0.872（100%），早期阶段提升最大。教师模型DSC达0.922，学生在肿瘤子区域超越教师（NCR/NET 0.797，Edema 0.980），特别是在教师失败的增强类上恢复DSC 0.620

Conclusion: 置信度驱动的课程学习和选择性遗忘机制能够在有限监督和噪声伪标签下提供鲁棒的分割性能，显著提高数据效率

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [149] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2是一个可扩展的高效视频生成编辑模型，通过连接预训练多模态大语言模型和视频扩散模型，利用MLLM的理解推理能力生成明确目标描述来指导生成过程，实现高质量视频生成和复杂编辑任务。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成和编辑中复杂组合指令的遵循问题，通过利用多模态大语言模型的理解能力来更好地解释用户指令，提升对复杂编辑任务的处理能力。

Method: 1) 利用MLLM的理解推理能力生成明确的目标描述来解释用户指令；2) 开发轻量级适配器将多模态条件标记注入预训练文本到视频扩散模型；3) 在精心策划的训练数据上扩展到140亿参数的视频扩散模型。

Result: 在FiVE基准测试中表现出遵循复杂组合指令的卓越能力，在VBench基准测试中实现竞争性或更优的视频生成质量，支持高质量文本到视频生成和各种视频编辑任务。

Conclusion: Omni-Video 2通过连接MLLM和视频扩散模型，有效提升了视频生成和编辑的性能，特别是在处理复杂组合指令方面表现出色，同时保持了参数效率和计算效率。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [150] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 开发了一个统一的MRI合成基础模型，通过对比视觉表示学习和视觉语言对齐，实现任意模态到所有模态的MRI合成，提升鼻咽癌放疗规划精度。


<details>
  <summary>Details</summary>
Motivation: 临床实践中由于患者不适、扫描时间长、成本高等限制，鼻咽癌放疗常面临MRI模态不完整的问题，影响放疗规划准确性。传统MRI合成方法存在模态特异性、解剖适应性有限、缺乏临床可解释性等不足。

Method: 开发统一基础模型，整合对比视觉表示学习和视觉语言对齐：使用对比编码器提取模态不变表示，基于CLIP的文本信息解码器实现语义一致合成，支持通过单一模型完成任意到所有模态的MRI合成。

Result: 在13个机构的40,825张图像上训练，在26个内外部验证站点（15,748张图像）上表现一致优异（平均SSIM 0.90，PSNR 27），合成保真度高，对噪声和域偏移具有鲁棒性。统一表示还提升了放疗相关下游任务（如分割）的性能。

Conclusion: 该工作通过基础模型将技术合成与临床实用性相结合，推进了鼻咽癌护理的数字医学解决方案，为临床实践提供了更准确、高效的MRI合成工具。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [151] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: VideoVeritas框架通过联合偏好对齐和感知预文本强化学习，结合细粒度感知和基于事实的推理来检测生成视频，并在新数据集MintVid上验证了其平衡性能。


<details>
  <summary>Details</summary>
Motivation: 视频生成能力的提升带来了安全风险，需要可靠的检测方法。当前多模态大语言模型虽然推理能力强，但细粒度感知能力有限。

Method: 提出VideoVeritas框架，采用联合偏好对齐和感知预文本强化学习（PPRL）。在强化学习阶段使用通用的时空定位和自监督物体计数作为感知预文本任务，而不是直接优化检测任务。

Result: 实验结果表明，现有方法倾向于偏向表面推理或机械分析，而VideoVeritas在多样化基准测试中实现了更平衡的性能。

Conclusion: VideoVeritas通过增强细粒度感知能力，为生成视频检测提供了更有效的解决方案，并在新构建的MintVid数据集上验证了其优越性。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [152] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: TiFRe是一个文本引导的视频帧缩减框架，通过智能选择关键帧并融合非关键帧信息，在降低计算成本的同时保持视频语义完整性。


<details>
  <summary>Details</summary>
Motivation: 视频多模态大语言模型在处理大量视频帧时面临高计算成本问题，传统的固定帧率关键帧选择方法会丢失非关键帧中的有价值信息，导致性能下降。

Method: 提出TiFRe框架：1) 文本引导帧采样(TFS)：使用LLM根据用户输入生成CLIP风格提示，通过CLIP编码器计算提示与每帧的语义相似度，选择最相关的关键帧；2) 帧匹配与融合(FMM)：将非关键帧信息整合到选定的关键帧中，最小化信息损失。

Result: 实验表明TiFRe能有效降低计算成本，同时在视频语言任务上提升性能。

Conclusion: TiFRe通过文本引导的智能帧选择和语义保持机制，在减少输入帧数的同时保持了视频信息完整性，为视频MLLMs的高效处理提供了有效解决方案。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [153] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 该研究分析了3D高斯泼溅（3DGS）标准多视图优化中出现的结构模式，称为渲染最优参考（RORs），揭示了其统计特性，并探讨了参数可学习性的密度分层现象。


<details>
  <summary>Details</summary>
Motivation: 理解3D高斯泼溅在多视图优化中形成的结构模式，探究这些参数的决定因素，以及不同密度区域参数可预测性的差异，为改进训练鲁棒性和系统架构提供理论基础。

Method: 通过分析RORs的统计特性，使用可学习性探针训练预测器从点云重建RORs，进行方差分解来量化几何与外观参数的耦合关系，并提出密度感知策略。

Result: 发现RORs具有稳定的统计模式：混合结构尺度和双峰辐射分布；揭示了密度分层现象：密集区域参数可预测，稀疏区域参数预测失败；证明了稀疏区域中可见性异质性导致几何与外观参数强耦合。

Conclusion: RORs具有双重特性：在密集区域表现为几何基元（点云足够预测），在稀疏区域表现为视图合成基元（需要多视图约束）。这为自适应平衡前馈预测和渲染优化的系统架构提供了理论依据。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [154] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 该论文提出了一种3D高斯流场表示方法，用于建模植物生长过程中的时变3D外观，通过反向生长模拟植物发育历史，相比现有方法在图像质量和几何精度上表现更优。


<details>
  <summary>Details</summary>
Motivation: 植物生长建模面临独特挑战：植物会随时间生成新几何结构，而现有动态场景建模技术（如变形场和4D高斯泼溅）无法处理这种几何变化或无法跟踪相同高斯集随时间的变化。

Method: 提出3D高斯流场表示，将植物生长建模为高斯参数（位置、尺度、方向、颜色、不透明度）的时变导数，支持非线性连续时间生长动态。通过重建成熟植物并学习反向生长过程来初始化足够的高斯基元。

Result: 在多视角植物生长时间序列数据集上，该方法在图像质量和几何精度方面优于现有方法，为生长中3D结构的外观建模提供了新方法。

Conclusion: 3D高斯流场表示能够有效建模植物生长过程中的几何生成和外观变化，为动态植物建模提供了创新解决方案。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [155] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter是一个基于视频扩散的框架，能够从单目视频中联合重建4D几何并估计密集运动，通过新的联合表示和4D VAE实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法强制3D值和潜在变量与RGB VAE潜在变量严格对齐，尽管它们具有根本不同的分布，这导致次优性能。需要一种更好的方法来学习联合表示并提升重建质量。

Method: 提出了一种新颖的密集3D点图和3D场景流在共享坐标系中的联合表示方法，以及一个新的4D VAE来有效学习这种表示。引入了新的数据归一化和VAE训练策略，更好地传递扩散先验。

Result: 在多个数据集上的广泛实验表明，MotionCrafter在几何重建和密集场景流估计方面都达到了最先进的性能，几何重建提升38.64%，运动重建提升25.0%，且无需任何后优化。

Conclusion: MotionCrafter通过创新的联合表示和4D VAE设计，证明了强制对齐不同分布的潜在变量是不必要的，新的训练策略能更好地传递扩散先验，显著提升了4D重建性能。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [156] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: WorldArena是一个统一基准测试，用于系统评估具身世界模型在感知和功能两个维度上的表现，揭示了感知质量与功能效用之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前对具身世界模型的评估主要关注感知保真度（如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用，评估体系存在碎片化问题。

Method: 提出WorldArena基准测试，通过三个维度评估模型：1）视频感知质量（16个指标，6个子维度）；2）具身任务功能（作为数据引擎、策略评估器和动作规划器）；3）EWMScore综合指标（将多维性能整合为单一可解释指数）。

Result: 通过对14个代表性模型的广泛实验，揭示了显著的感知-功能差距：高视觉质量并不一定转化为强大的具身任务能力。

Conclusion: WorldArena基准测试为追踪具身AI中真正功能性世界模型的进展提供了框架，有助于推动该领域向更实用的方向发展。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [157] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: ArcFlow提出了一种非线性流轨迹蒸馏框架，通过参数化连续动量过程来逼近教师模型的推理轨迹，实现仅需2步推理即可达到40倍加速，且质量损失很小。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量出色，但需要大量顺序去噪步骤导致推理成本高昂。现有蒸馏方法使用线性捷径近似教师轨迹，难以匹配随时间步变化的切线方向，导致质量下降。

Method: ArcFlow将推理轨迹的底层速度场参数化为连续动量过程的混合，能够捕捉速度演化并外推连贯速度形成连续非线性轨迹。这种参数化允许对非线性轨迹进行解析积分，避免数值离散误差。通过轻量适配器在预训练教师模型上进行轨迹蒸馏训练。

Result: 在Qwen-Image-20B和FLUX.1-dev等大规模模型上，ArcFlow仅微调不到5%的原始参数，使用2步推理即可实现40倍加速，且没有显著的质量下降。基准测试显示ArcFlow在定性和定量上都有效。

Conclusion: ArcFlow通过非线性流轨迹蒸馏成功解决了扩散模型推理效率问题，在保持生成多样性和质量的同时实现了显著的加速，为大规模扩散模型的部署提供了实用解决方案。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [158] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: Raster2Seq：将平面图重建作为序列到序列任务，通过自回归解码器预测多边形角点，实现从栅格图像到结构化矢量图形的转换


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理复杂平面图时难以准确生成结构和语义信息，特别是包含大量房间和不同多边形角点数量的室内空间平面图

Method: 将平面图重建框架化为序列到序列任务，使用自回归解码器预测下一个角点，引入可学习锚点指导注意力机制聚焦信息丰富的图像区域

Result: 在Structure3D、CubiCasa5K和Raster2Graph等标准基准测试中达到最先进性能，在包含多样化房间结构和复杂几何变化的WAFFLE数据集上也表现出强大的泛化能力

Conclusion: Raster2Seq通过自回归序列建模有效解决了复杂平面图的结构化重建问题，为自动化理解和CAD工作流提供了可靠的前置处理工具

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [159] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass是一个基于强化学习的后训练框架，用于提升长时程交互式视频世界模型的探索准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的长时程交互式视频世界模型在探索世界时缺乏准确性和一致性，需要更有效的后训练方法来基于交互信号引导模型探索。

Method: 提出了三个核心创新：1) 片段级展开策略，在单个目标片段生成和评估多个样本以提高效率；2) 互补奖励函数，设计交互跟随准确性和视觉质量的奖励函数；3) 高效RL算法，采用负感知微调策略和多种效率优化。

Result: 在SoTA开源世界模型WorldPlay上的评估表明，WorldCompass显著提高了各种场景下的交互准确性和视觉保真度。

Conclusion: WorldCompass是一个有效的RL后训练框架，能够显著提升视频世界模型的探索能力和生成质量。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [160] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 本文挑战了视觉生成中连续管道的统治地位，通过研究发现离散与连续方法之间的性能差距主要源于潜在空间中分配的比特总数（压缩比），并提出BAR框架使离散方法达到或超越连续方法性能。


<details>
  <summary>Details</summary>
Motivation: 挑战视觉生成领域连续管道的统治地位，系统研究离散与连续方法之间的性能差距，纠正离散标记器本质劣质的错误观念，探索离散方法的潜力。

Method: 提出掩码比特自回归建模（BAR）框架，通过为自回归变换器配备掩码比特建模头，支持任意码本大小，通过逐步生成构成比特来预测离散标记。

Result: BAR在ImageNet-256上实现了0.99的gFID新SOTA，超越了连续和离散范式的领先方法，同时显著降低了采样成本，收敛速度比先前的连续方法更快。

Conclusion: 离散标记器并非本质劣质，性能差距主要源于压缩比；通过扩大码本规模可以弥合这一差距；BAR框架为离散生成提供了可扩展的解决方案，实现了卓越的性能和效率。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [161] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 像素语言建模旨在通过将文本渲染为图像来绕过子词分词瓶颈，但多模态变体如DualGPT重新引入文本分词器以提升自回归性能。研究发现，即使采用视觉渲染，重新整合文本分词器仍会重新引入分词器对齐问题，特别是在印尼低资源本地语言中。


<details>
  <summary>Details</summary>
Motivation: 研究像素语言建模是否真正能够使模型摆脱分词约束，特别是在多模态变体重新引入文本分词器的情况下。关注印尼四种使用非拉丁文字的低资源本地语言（爪哇语、巴厘语、巽他语、楠榜语），评估文字-分词器对齐对模型性能的影响。

Method: 在DualGPT架构中评估不同分词器的影响，比较Llama 2分词器与自定义分词器在四种印尼低资源本地语言上的表现，使用OOV（未登录词）率、fertility率等指标，最终通过chrF++分数评估性能差异。

Result: 尽管视觉渲染，重新整合文本分词器仍会重新引入分词器对齐问题。Llama 2分词器虽然具有较低的OOV和fertility率，但性能显著差于自定义分词器，改进幅度高达30.15 chrF++。

Conclusion: 文本分词器仍然是实现公平模型的重要障碍，即使在使用视觉渲染的多模态变体中。研究结果对未来多模态变体提出警告，需要关注分词器对齐问题，特别是在处理低资源和非拉丁文字语言时。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [162] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: ILA-agent框架让大语言模型通过推理时与有限外部资源交互来掌握陌生编程语言，显著优于检索增强基线


<details>
  <summary>Details</summary>
Motivation: 大语言模型在编码任务中的能力依赖于大量预训练数据，但面对陌生编程语言时性能会急剧下降。传统的数据密集型微调方法不适用于资源有限的场景，需要探索推理时语言获取的新范式。

Method: 提出ILA-agent框架，将人类关键行为建模为一套工具，使LLM能够通过结构化交互（访问官方文档和执行环境）来增量式探索、应用和验证语言知识。构建Cangjie-bench多任务基准，并在Cangjie语言上实例化ILA-agent进行评估。

Result: 使用多种LLM的实验结果表明，ILA-agent在代码生成、翻译和程序修复任务上显著优于检索增强基线方法。轨迹分析揭示了涌现的行为模式，同时指出了持续存在的性能差距。

Conclusion: ILA-agent为LLM掌握陌生编程语言提供了一种有效的推理时学习框架，通过结构化交互和工具化行为建模实现了在低资源环境下的语言获取能力提升。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [163] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

TL;DR: 提出Anchored Decoding方法，在推理时抑制语言模型的逐字复制行为，通过将生成约束在安全模型附近，实现可调节的风险-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型倾向于记忆训练数据并逐字输出，当数据源涉及敏感或受版权保护内容时，这引发了创作者同意与补偿问题以及开发者的合规风险。

Method: 提出Anchored Decoding方法：1）在推理时使用，通过将生成约束在训练于许可数据的安全模型附近来抑制逐字复制；2）自适应分配用户选择的信息预算，实施每步约束以获得序列级保证；3）引入TinyComma 1.8B安全模型和Anchored$_{\mathrm{Byte}}$ Decoding字节级变体，支持跨词汇表融合。

Result: 在6个模型对上评估版权风险和效用：Anchored和Anchored$_{\mathrm{Byte}}$ Decoding定义了新的帕累托前沿，在保持接近原始流畅性和事实性的同时，将可测量复制差距（6个复制指标平均）减少高达75%，推理开销适中。

Conclusion: Anchored Decoding是一种即插即用的推理时方法，能有效抑制语言模型的逐字复制行为，在风险与效用之间提供可调节的权衡，为处理混合许可数据训练的语言模型提供实用解决方案。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [164] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: Open TutorAI是一个基于大语言模型和生成技术的开源教育平台，通过动态个性化辅导、3D虚拟化身和嵌入式学习分析，创建沉浸式学习环境。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人系统缺乏上下文适应性、实时响应性和教学灵活性，限制了学习者的参与度和教学效果，需要结合AI和沉浸式技术的开放集成平台来支持个性化学习体验。

Method: 基于LLMs和生成技术构建开源平台，集成自然语言处理和可定制的3D虚拟化身实现多模态交互，通过结构化引导过程捕获学习者目标和偏好，配置个性化的AI助手，提供文本和虚拟化身界面。

Result: 开发了Open TutorAI平台，包含内容组织工具、嵌入式反馈机制，以及面向学习者、教育者和家长的专用界面，通过助手生成流程和虚拟化身集成增强参与度和情感存在感。

Conclusion: Open TutorAI将模块化架构、生成式AI和学习者分析统一在开源框架中，为下一代智能辅导系统的发展做出贡献，创造了更加人性化、沉浸式的学习环境。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [165] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 该研究提出利用人格特质作为潜在信号来优化基于用户偏好的个性化LLM回答，通过人格对齐的偏好选择显著提升回答准确性，并构建了人格标注的偏好数据集PACIFIC。


<details>
  <summary>Details</summary>
Motivation: 当前利用用户偏好个性化LLM回答的方法面临偏好信号可能嘈杂、不完整甚至误导的问题，这会影响回答质量。研究发现稳定的人格特质会影响日常偏好，因此探索将人格作为偏好的潜在信号来提升个性化问答效果。

Method: 1) 通过实验验证人格对齐偏好的有效性；2) 构建PACIFIC数据集，包含1200个跨领域偏好陈述，标注了Big-Five人格特质方向；3) 提出一个框架，使LLM能够自动检索人格对齐的偏好并在生成答案时加以利用。

Result: 使用人格对齐的偏好显著提升了个性化问答的准确性：与随机选择偏好相比，选择与用户推断人格一致的偏好将答案选择准确率从29.25%提高到76%。

Conclusion: 人格特质可以作为偏好的有效潜在信号，通过人格对齐的偏好选择能够显著改善LLM的个性化回答质量。提出的PACIFIC数据集和框架为实现这一目标提供了实用工具。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [166] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 提出针对法律文档长上下文问答系统，解决法律文档复杂布局、专业术语和长篇幅答案的挑战


<details>
  <summary>Details</summary>
Motivation: 法律文档具有复杂的文档布局（多层嵌套章节、长脚注）、专业术语和复杂语法结构，这使得长上下文问答（答案跨越多页）和全面性答案生成变得困难

Method: 提出问答系统：1) 解构领域特定词汇以改进文档检索；2) 解析复杂文档布局，隔离章节和脚注并适当链接；3) 使用精确领域词汇生成全面答案；引入基于召回率的覆盖度指标，便于人工评估

Result: 通过法律和公司税务等专业人士构建QA数据集，进行综合实验和消融研究，证明了所提系统的实用性和价值

Conclusion: 提出的系统有效解决了法律文档长上下文问答的挑战，通过专业词汇处理、复杂布局解析和全面答案生成，为法律文档问答提供了实用解决方案

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [167] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

TL;DR: 该研究探索如何让大语言模型具备定向多说话人语音理解能力，特别是在智能眼镜应用场景中，提出了两种集成方向性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型主要基于单通道、单说话人数据训练，难以直接应用于多说话人、多通道的语音理解任务，特别是在智能眼镜等需要定向语音理解的实际场景中。

Method: 提出了两种方法：1）级联系统，利用源分离前端模块；2）端到端系统，采用序列化输出训练。两种方法都利用智能眼镜中的多麦克风阵列，以流式方式优化方向性解释和处理。

Result: 实验结果表明，所提方法能有效赋予大语言模型定向语音理解能力，在语音识别和语音翻译任务中均表现出色。

Conclusion: 该研究成功实现了让大语言模型具备定向多说话人语音理解能力，为智能眼镜等实际应用场景提供了有效的解决方案。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [168] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 本文提出了一种风险敏感的幻觉评估框架，专注于识别医疗问答中可能造成危害的风险性语言，而非仅评估事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉评估标准主要关注事实准确性，将所有错误视为同等严重，这掩盖了临床相关的失败模式，特别是当模型生成无支持但可执行的医疗语言时。需要一种能评估潜在危害风险的评估框架。

Method: 提出风险敏感评估框架，通过检测风险性语言（包括治疗指令、禁忌症、紧急提示和高风险药物提及）来量化幻觉。结合风险评估与相关性度量，识别高风险、低基础支持的失败案例。在三个指令调优语言模型上应用该框架，使用受控的患者面向提示作为安全压力测试。

Result: 结果显示，具有相似表面行为的模型展现出显著不同的风险特征，标准评估指标无法捕捉这些差异。模型在风险性语言生成方面存在明显差异。

Conclusion: 将风险敏感性纳入幻觉评估至关重要，评估有效性严重依赖于任务和提示设计。需要超越事实准确性的评估方法来捕捉医疗问答中的潜在危害。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [169] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 论文提出"对话迷失"现象的根本原因是意图对齐差距而非模型能力缺陷，通过解耦意图理解与任务执行的调解者-助手架构来解决多轮对话性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中相比单轮交互会出现显著的性能下降（"对话迷失"现象）。先前研究将此归因于模型不可靠性，但本文认为根本原因在于用户与模型之间的意图对齐差距，而非模型内在能力缺陷。

Method: 提出调解者-助手架构，通过经验驱动的调解者将模糊的用户输入转化为明确、结构化的指令。调解者基于历史交互模式来显式化用户意图，从而在意图理解与任务执行之间建立桥梁。

Result: 实验结果表明，该方法显著减轻了多轮对话中的性能下降问题，在不同的大型语言模型上都取得了良好效果。

Conclusion: "对话迷失"现象源于用户意图与模型解释之间的结构性模糊，而非模型能力限制。调解者-助手架构通过解耦意图理解和任务执行，有效解决了多轮对话中的意图对齐问题，为改进LLM对话系统提供了新方向。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [170] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: 提出了越南医疗法规多跳推理数据集ViHERMES，用于评估医疗监管文档的多跳问答系统，并开发了图感知检索框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 医疗监管文档具有层次化结构和频繁修订的特点，需要跨文档多跳推理，但目前缺乏针对低资源语言（如越南语）的系统性评估基准数据集。

Method: 1) 提出基于语义聚类和图启发数据挖掘的控制多跳QA生成管道，结合大语言模型生成结构化证据和推理标注；2) 开发图感知检索框架，在法规单元层面建模正式法律关系，支持原则性上下文扩展。

Result: ViHERMES数据集为评估多跳监管QA系统提供了具有挑战性的基准，提出的图感知方法在实验中持续优于强检索基线模型。

Conclusion: ViHERMES填补了越南语医疗监管多跳推理数据集的空白，图感知检索框架能有效处理法规间的依赖关系，为监管文档问答系统提供了新的评估标准和解决方案。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [171] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: TernaryLM是一个132M参数的Transformer模型，采用原生1位三元量化{-1, 0, +1}训练，显著减少内存占用而不牺牲语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能卓越但需要大量计算资源，限制了在边缘设备和资源受限环境中的部署。需要开发更高效的内存优化方法。

Method: 采用原生1位三元量化{-1, 0, +1}进行训练，使用直通估计器和自适应逐层缩放因子，从零开始学习量化感知表示，而不是对预训练的全精度模型进行后训练量化。

Result: 1) 在TinyStories上验证困惑度为58.42；2) 在MRPC复述检测任务上F1分数达82.47%；3) 内存减少2.4倍（498MB vs 1197MB），推理延迟相当；4) 在不同语料库上训练动态稳定；5) 中间Transformer层与极端量化兼容性最高。

Conclusion: 原生1位训练是高效神经语言模型的有前景方向，为未来非均匀精度策略提供了指导。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [172] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 提出一种基于一阶统计特性的轻量级后训练剪枝框架，通过统计校准减少激活异常值影响，使用能量补偿修正分布失真，无需重训练或二阶信息。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法在剪枝质量和计算效率之间存在权衡：启发式方法高效但对激活异常值敏感，重建方法保真度高但计算量大。需要一种既高效又高质量的剪枝方法。

Method: 基于模型权重和激活的一阶统计特性：1) 剪枝时使用通道级统计校准基于幅值的重要性分数，减少激活主导通道的偏差；2) 剪枝后应用解析能量补偿修正权重移除引起的分布失真。两个步骤都不需要重训练、梯度或二阶信息。

Result: 在多个LLM家族、稀疏模式和评估任务上的实验表明，该方法提高了剪枝性能，同时保持了与启发式方法相当的计算成本。

Conclusion: 简单的统计校正对于LLM的后训练剪枝是有效的，可以在不增加计算成本的情况下改善剪枝质量。

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [173] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: Demo-SafetyBench是一个解决LLM安全评估中人口统计学多样性不足的数据集，通过建模人口统计学多元主义，在提示层面解耦价值框架与响应，实现可扩展且人口统计学鲁棒的安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐数据集（如ANTHROPIC-HH和DICES）使用人口统计学狭窄的标注者池，忽略了不同社区间安全感知的差异，缺乏对人口统计学多元主义的考虑。

Method: 采用两阶段方法：第一阶段将DICES提示重新分类为14个安全领域，保留人口统计学元数据，通过SimHash去重扩展低资源领域；第二阶段使用LLMs-as-Raters评估多元敏感性，采用平衡阈值实现高可靠性和低人口统计学敏感性。

Result: 构建了43,050个样本的数据集，通过平衡阈值（delta=0.5, tau=10）实现了高可靠性（ICC=0.87）和低人口统计学敏感性（DS=0.12），证明多元安全评估既可扩展又具有人口统计学鲁棒性。

Conclusion: Demo-SafetyBench通过直接在提示层面建模人口统计学多元主义，解决了现有安全评估数据集的人口统计学偏见问题，为LLM安全评估提供了更全面、更具代表性的基准。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [174] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: AlignX是一个两阶段框架，通过提示注入微调和几何校准的MoE来解决多目标对齐中的轴崩溃问题，在帮助性、无害性和诚实性方面显著提升性能，同时降低延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法（如SFT和MoE）在多目标设置中面临挑战：SFT导致冲突目标间的干扰，MoE存在路由校准问题，这种现象被称为"轴崩溃"，表现为特征空间分离导致的灾难性遗忘和专家误路由导致的不可靠推理。

Method: 提出AlignX两阶段框架：第一阶段使用提示注入微调提取轴特定任务特征，缓解灾难性遗忘；第二阶段部署MoCaE模块，利用分形和自然几何校准专家路由，提高推理可靠性。

Result: 在Alpaca（帮助性）、BeaverTails（无害性）和TruthfulQA（诚实性）上取得显著提升：胜率+171.5%，真实性-信息性+110.1%，安全违规减少4.3%。相比先前MoE方法，延迟和内存使用降低超过35%。在四个LLM上的结果验证了其泛化能力。

Conclusion: AlignX有效解决了多目标对齐中的轴崩溃问题，通过特征提取和几何校准的路由机制，在保持模型性能的同时显著提升了效率，为大语言模型的安全部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [175] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该研究提出了一种改进印度法律文本摘要的方法，通过向摘要模型注入法律领域知识，生成英语和印地语的法律判决摘要，解决了印度法律文本复杂性和语言障碍问题。


<details>
  <summary>Details</summary>
Motivation: 印度法律判决摘要面临双重挑战：一是法律文本语言复杂且结构不规范，二是大部分印度人口不理解法律文本使用的复杂英语，需要印度语言摘要。这导致法律信息获取存在障碍。

Method: 提出两种方法：1) 在抽取式神经摘要模型中融入针对法律文本的领域特定预训练编码器；2) 通过在大规模英语和印地语法律语料上进行持续预训练，将法律领域知识注入生成式模型（包括大语言模型）。

Result: 提出的方法在英语到英语和英语到印地语的印度法律文档摘要任务中，在标准评估指标、事实一致性指标和法律领域特定指标上都取得了统计显著的改进。这些改进得到了领域专家的验证。

Conclusion: 通过向摘要模型注入法律领域知识，可以有效提升印度法律文本的摘要质量，生成更准确、更符合法律专业要求的英语和印地语摘要，有助于提高法律信息的可及性。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [176] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

TL;DR: 提出了一种基于词汇相似度的计算指标来评估罗曼语族语言间的相互可懂度，通过表面和语义相似度分析，验证了该指标与人类实验结果的显著相关性。


<details>
  <summary>Details</summary>
Motivation: 研究罗曼语族语言间的相互可懂度，需要一种客观的计算方法来替代传统的主观评估，以更准确地衡量语言间的理解程度。

Method: 提出基于词汇相似度的计算指标，结合表面相似度和语义相似度，使用正字法和语音形式，并比较不同平行语料库和词向量表示模型。

Result: 计算得到的可懂度分数证实了语言间可懂度不对称的直觉，并与人类完形填空实验结果显著相关。

Conclusion: 基于词汇相似度的计算指标能够有效评估罗曼语族语言间的相互可懂度，为语言理解研究提供了可靠的计算方法。

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


### [177] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: SED-SFT提出了一种选择性熵正则化方法，通过选择性掩码机制解决SFT中的模式崩溃问题，增强生成多样性，提升后续RL性能


<details>
  <summary>Details</summary>
Motivation: 传统SFT使用交叉熵损失导致模式崩溃，模型过度集中于特定响应模式，缺乏分布多样性，严重限制了后续RL的探索效率

Method: 提出SED-SFT框架，基于token探索空间自适应鼓励多样性，在优化目标中引入带有选择性掩码机制的选择性熵正则化项

Result: 在8个数学基准测试中，SED-SFT显著增强生成多样性，计算开销可忽略，相比标准CE基线，在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct上后续RL性能平均提升2.06和1.20分

Conclusion: SED-SFT有效解决了SFT中的模式崩溃问题，平衡了多样性和准确性，为后续RL提供了更好的探索基础

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [178] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出一个评估框架来诊断视觉语言模型在多语言表情包检测中的跨文化鲁棒性，发现翻译后检测会降低性能，而文化对齐的干预措施（本地语言提示和单样本学习）能显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 文化背景深刻影响人们对在线内容的理解，但当前的视觉语言模型主要基于西方或英语中心视角训练，这限制了它们在仇恨表情包检测等任务中的公平性和跨文化鲁棒性。

Method: 引入一个系统评估框架，通过多语言表情包数据集诊断和量化最先进视觉语言模型的跨文化鲁棒性，分析三个维度：学习策略（零样本 vs 单样本）、提示语言（本地语言 vs 英语）以及翻译对意义和检测的影响。

Result: 结果显示常见的"翻译后检测"方法会降低性能，而文化对齐的干预措施——本地语言提示和单样本学习——能显著提升检测效果。研究发现模型系统性地趋向西方安全规范。

Conclusion: 研究揭示了视觉语言模型在跨文化应用中的系统性偏见，并提供了可操作的策略来减轻这种偏见，为设计全球鲁棒的多模态内容审核系统提供指导。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [179] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

TL;DR: 该论文提出了一种通过动态路径规划、语义感知示例选择和对话历史链式推理的框架，将复杂句子简化分解为可管理步骤，在五个语言的两个基准测试中提高了简化效果，同时减少了22-42%的计算步骤。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在可控难度句子简化方面能力有限，特别是在跨大阅读难度级别进行简化时表现不佳。需要解决复杂简化任务中的控制性和语义保持问题。

Method: 提出一个框架，通过动态路径规划将复杂简化分解为可管理步骤，采用语义感知的示例选择方法，并结合对话历史进行链式推理生成，确保推理的连贯性。

Result: 在五个语言的两种基准测试中，该方法提高了简化效果，同时减少了22-42%的计算步骤。人类评估证实了简化效果与意义保持之间的基本权衡关系。

Conclusion: 逐步简化方法虽然提高了控制性，但在广泛简化过程中保持语义保真度仍然是一个开放挑战。值得注意的是，即使是人类标注者在语义保持判断上也难以达成一致，突显了这项任务的内在复杂性。

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [180] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: LR-DLLM提出了一种长度正则化推理框架，解决扩散大语言模型在变长生成中的长度诱导偏差问题，通过显式长度正则化实现可靠的生成长度确定。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）本质上不适合变长生成，因为其推理基于固定长度画布并隐含假设已知目标长度。在现实场景如补全和填充中，当长度未知时，简单地比较不同掩码长度的置信度会产生系统性偏差，导致生成不足或冗余延续。

Method: 提出LR-DLLM框架，将生成长度作为显式变量，通过显式长度正则化分离语义兼容性和长度诱导不确定性，校正有偏的置信度估计。该框架无需修改底层DLLM或其训练过程，即可实现生成跨度的动态扩展或收缩。

Result: 在完全未知长度条件下，LR-DLLM在HumanEvalInfilling上达到51.3% Pass@1（比DreamOn提升13.4%），在四语言McEval上平均达到51.5% Pass@1（比DreamOn提升14.3%）。

Conclusion: LR-DLLM通过长度正则化推理框架有效解决了DLLMs在变长生成中的长度确定问题，显著提升了在未知长度场景下的生成性能，为扩散语言模型的实用化提供了重要改进。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [181] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: LLMs在生成推理路径方面表现出色，但在自我验证能力上存在明显不足，形成生成与验证之间的能力不对称。研究发现，提升验证能力可以有效改善生成性能，而反向关系不成立。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务中能够生成有前景的推理路径，但缺乏有效的自我验证能力，这种生成与验证之间的能力不对称限制了模型的整体性能。研究旨在深入探究这种不对称现象及其对模型训练的影响。

Method: 通过分析训练演化过程中的能力不对称现象，发现学习自我验证可以改善生成性能。基于此观察，提出一个多任务强化学习框架，将生成和自我验证作为两个独立但互补的目标进行优化。

Result: 实验表明，学习自我验证能够有效提升生成性能，达到与标准生成训练相当的准确性，同时产生更高效有效的推理轨迹。多任务强化学习框架在多个基准测试和模型上都显示出优于纯生成训练的性能提升。

Conclusion: 生成与自我验证之间存在显著的能力不对称，但学习验证能力可以改善生成性能。通过多任务强化学习框架整合这两种能力，可以同时提升模型的生成和验证能力，为LLM训练提供了新的优化方向。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [182] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 本文探索如何利用人类导师-学生对话中嵌入的导师角色来引导大语言模型行为，通过修改双向偏好优化学习激活空间中的转向向量，实现不同导师风格的个性化控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的辅导系统通常学习单一导师策略，无法捕捉真实世界中导师风格的多样性。真实导师会根据学生需求调整脚手架水平、指导直接性、反馈和情感支持，这些差异会影响对话动态和学生参与度。

Method: 修改双向偏好优化方法，学习一个转向向量（激活空间中的方向），该向量能够将模型响应转向特定的导师角色，而不依赖显式的提示指令。

Result: 转向向量能够捕捉不同对话情境下的导师特定变化，提高与真实导师话语的语义对齐，增加基于偏好的评估，同时基本保持词汇相似性。学习到的方向系数显示出可解释的结构，对应导师行为的一致差异。

Conclusion: 激活转向提供了一种有效且可解释的方法，利用直接从人类对话数据中提取的信号来控制大语言模型中导师特定的变化，为个性化辅导系统开发提供了新途径。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [183] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: 研究发现LLM评估器在摘要任务中存在偏见：随着被评估摘要与人工摘要相似度降低，LLM评估器越来越偏好其他LLM生成的摘要而非人工摘要，且几乎所有模型都存在此模式，与模型自身的位置偏见无关。


<details>
  <summary>Details</summary>
Motivation: LLM评估器在摘要等任务中常被用作传统算法指标的补充，因其能更好捕捉语义信息、推理能力和对改写的鲁棒性。然而，LLM评估器存在长度偏见、顺序偏见等多种偏见，且易受对抗性输入影响。虽然已有研究关注这些偏见，但很少有研究在更细粒度上分析与明确定义的重叠度指标的关系。

Method: 在摘要领域，以与人工撰写回答的重叠度作为函数进行LLM评估器偏见分析。测试了9个参数规模从10亿到120亿的近期LLM，包括Gemma 3和LLaMA 3的变体。使用ROUGE和BLEU指标测量相似度，分析LLM评估器偏好与相似度的关系。

Result: 发现随着被评估摘要与人工摘要的相似度（通过ROUGE和BLEU测量）降低，LLM评估器越来越偏好其他LLM生成的摘要而非人工摘要。这一模式在所有测试模型中除一个外均存在，且与模型自身的位置偏见无关。此外，模型甚至在有限重叠的摘要评估中也存在困难。

Conclusion: 在摘要领域使用LLM作为评估器时，应依赖超越简单比较的技术，因为LLM评估器存在系统性偏见，特别是在与人工摘要相似度较低的情况下会偏好其他LLM生成的摘要。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [184] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

TL;DR: Attn-GS：基于注意力引导的上下文压缩框架，利用LLM注意力模式识别个性化重要信号，显著减少token使用同时保持接近完整上下文的性能


<details>
  <summary>Details</summary>
Motivation: 个性化大语言模型需要整合大量用户交互历史和资料，但输入token限制导致高推理延迟和API成本。现有启发式方法（如选择最近交互或提示摘要模型）将上下文视为整体，未能考虑LLM内部如何处理和优先处理不同资料组件。

Method: 提出Attn-GS注意力引导上下文压缩框架：1）通过初步研究发现LLM注意力模式能自然揭示重要信号，微调能增强区分相关信息能力；2）利用标记模型基于注意力反馈标记重要个性化句子；3）指导压缩模型生成任务相关、高质量的压缩用户上下文。

Result: Attn-GS在不同任务、token限制和设置下显著优于各种基线方法，在减少50倍token使用的同时，性能接近使用完整上下文。

Conclusion: LLM注意力模式能有效识别个性化重要信号，Attn-GS框架通过注意力引导的上下文压缩，在显著降低计算成本的同时保持个性化性能，为大规模个性化应用提供实用解决方案。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [185] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 研究发现大语言模型在上下文概念推理时会动态构建和使用结构化潜在表征，这些表征在中间到深层形成概念子空间，对模型预测具有因果作用。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究发现大语言模型内部存在类似人类的概念表征结构，但尚不清楚这些模型是否在推理过程中功能性地依赖这些表征。本研究旨在探究大语言模型在上下文概念推理时的内部处理机制。

Method: 通过分析大语言模型在上下文概念推理任务中的内部处理，识别概念子空间的出现位置和特征，并使用因果中介分析验证该子空间对模型预测的功能性作用。

Result: 研究发现：1）在中间到深层出现概念子空间，其表征结构在不同上下文中保持稳定；2）该子空间对模型预测具有因果作用，不是附带现象；3）存在分层处理过程，早期到中间层的注意力头整合上下文线索构建和精炼子空间，后续层利用该子空间生成预测。

Conclusion: 大语言模型在上下文推理时会动态构建和使用结构化潜在表征，这为理解模型灵活适应的计算过程提供了新见解，表明模型确实功能性地依赖概念表征进行推理。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [186] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

TL;DR: LLMs在混合表示的数字比较上表现不佳，但通过隐藏状态分析发现它们能编码数字的对数大小信息，通过线性分类器可以准确比较数字大小，而显式询问时准确率反而更低。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的LLMs能够解决数学问题，但研究发现它们在混合表示的数字比较（如"5.7×10² vs 580"）上容易出错，这引发了一个基本问题：LLMs是否真正理解这些数字的大小？

Method: 通过分析多个开源LLMs的隐藏状态，使用线性投影从合适的隐藏层中提取数字的对数大小信息，并训练线性分类器来比较数字对的大小排名。

Result: 1）线性投影可以从隐藏状态中恢复数字大小，相对误差约2.3%（合成文本）或19.06%（科学论文）；2）隐藏状态编码了数字对的排名信息，线性分类器准确率超过90%；3）但显式询问LLMs时准确率只有50-70%，且探针效果越差的模型表现越差；4）将分类器探针的对数损失作为微调辅助目标，可使口头回答准确率提升3.22%。

Conclusion: LLMs的隐藏状态确实编码了数字的大小信息，但模型无法有效利用这些信息进行显式推理。通过改进内部大小表示可以增强模型的数值推理能力，这为提升LLMs的数学能力提供了新方向。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [187] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: TodoEvolve是一个元规划范式，能够自主合成和动态修订任务特定的规划架构，通过统一的PlanFactory设计空间和多目标强化学习训练，在多个智能体基准测试中超越手工设计的规划模块。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统主要依赖固定的手工规划结构，缺乏适应开放性问题结构多样性的灵活性，需要能够自主适应不同任务结构的规划方法。

Method: 首先构建PlanFactory模块化设计空间，统一标准化不同规划范式；然后收集高质量规划轨迹，通过阻抗引导偏好优化(IGPO)多目标强化学习训练Todo-14B模型，生成性能稳定且令牌高效的规划系统。

Result: 在五个智能体基准测试中，TodoEvolve持续超越精心设计的规划模块，同时保持经济的API成本和运行时开销。

Conclusion: TodoEvolve提供了一种能够自主适应不同任务结构的元规划范式，解决了现有固定规划架构在开放性问题中的局限性，为智能体系统提供了更灵活高效的规划能力。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [188] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在多答案场景下的置信度校准问题，发现现有方法在多有效答案情况下会系统性低估置信度，提出了新的基准MACE和解决方案SCA。


<details>
  <summary>Details</summary>
Motivation: 现有置信度校准方法主要针对单答案问答场景研究，但在存在多个有效答案的情况下，这些方法会失效，导致置信度系统性低估，影响模型可靠性。

Method: 1. 引入MACE基准，包含12,000个事实性问题，涵盖6个领域，具有不同数量的正确答案；2. 提出语义置信度聚合（SCA）方法，通过对多个高概率采样响应的置信度进行聚合来解决多答案校准问题。

Result: 实验表明：1. 准确率随答案基数增加而提高；2. 估计的置信度却持续下降；3. 在混合答案数量的问题上存在严重校准错误；4. SCA方法在混合答案设置下实现了最先进的校准性能，同时在单答案问题上保持强校准能力。

Conclusion: 多答案场景下的置信度校准是一个重要但被忽视的问题，SCA方法能有效解决这一问题，在保持单答案校准性能的同时，显著提升多答案场景下的校准效果。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [189] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: SparseEval：一种基于稀疏优化的高效大语言模型基准测试方法，通过梯度下降优化锚点权重和迭代精化策略，显著降低评估成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，在大量基准样本上进行推理评估的计算成本越来越高，需要开发高效的评估方法来降低开销。

Method: 将模型-项目性能矩阵视为稀疏矩阵，选择代表性项目作为锚点，将高效基准测试问题转化为稀疏优化问题。采用梯度下降优化锚点权重，使用迭代精化策略进行锚点选择，利用MLP的表征能力处理稀疏优化，并提出锚点重要性分数和候选重要性分数来评估每个项目的价值。

Result: 在多种基准测试上的广泛实验表明，该方法具有较低的估计误差和较高的Kendall's τ相关性，展现了在实际场景中的优越鲁棒性和实用性。

Conclusion: SparseEval通过稀疏优化方法有效解决了大语言模型评估的高成本问题，为高效基准测试提供了实用解决方案。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [190] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文通过因果中介分析研究指令调优语言模型中指令特定表征的构建与使用机制，发现指令表征相对局部化，称为"指令向量"，表现出线性可分性与非线性因果交互的并存，挑战了机制可解释性中的线性表征假设。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型取得了成功并被广泛应用，但人们对其内部如何处理指令知之甚少。本文旨在从机制角度填补这一空白，研究指令特定表征在不同后训练阶段（监督微调和直接偏好优化）中如何构建和利用。

Method: 使用因果中介分析识别指令表征的局部化特征；提出一种新颖的方法来定位语言模型中的信息处理，该方法不受基于修补技术的隐式线性假设限制；分析指令向量作为"电路选择器"的作用机制。

Result: 发现指令表征相对局部化，称为"指令向量"；这些表征表现出线性可分性与非线性因果交互的并存；指令向量在早期层形成任务表征后，在后续层中选择不同的信息通路来完成任务，即作为电路选择器。

Conclusion: 指令向量展现出线性可分性与非线性因果交互的奇特并存，广泛质疑了机制可解释性中常见的线性表征假设的范围；指令向量作为电路选择器，在早期层形成任务表征后，在后续层中选择不同的信息处理通路。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [191] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: Bielik Guard是一系列波兰语内容安全分类器，包含0.1B和0.5B参数两个变体，用于检测仇恨/攻击、粗俗内容、性内容、犯罪和自残等五类不安全内容，在真实用户提示上表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在波兰语应用中的部署增加，需要高效准确的内容安全分类器来确保内容安全。

Method: 基于MMLW-RoBERTa-base（0.1B参数）和PKOBP/polish-roberta-8k（0.5B参数）构建紧凑模型，在6,885个社区标注的波兰文本数据集上进行微调，分类五个安全类别。

Result: 0.5B变体在测试集上获得最佳整体区分能力（微平均F1 0.791，宏平均F1 0.785）；0.1B变体在真实用户提示上表现优异，精度达77.65%，误报率仅0.63%，优于同尺寸的HerBERT-PL-Guard。

Conclusion: Bielik Guard系列模型在波兰语内容安全分类中表现出色，提供适当的响应而非简单的内容屏蔽，特别是对自残等敏感类别，模型已公开可用。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [192] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: 论文提出CompositeHarm基准，通过翻译方法研究LLM安全对齐在多语言环境下的表现，发现攻击成功率在印度语言中显著上升，特别是对抗性语法攻击，而上下文危害转移较为温和。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全评估主要基于英语，翻译作为多语言行为探测的捷径往往无法捕捉完整情况，特别是当有害意图或结构在不同语言中发生变化时。需要研究安全对齐在语法和语义变化时的表现。

Method: 引入CompositeHarm基准，结合两个英文数据集AttaQ（结构化对抗攻击）和MMSafetyBench（上下文现实世界危害），扩展到六种语言（英语、印地语、阿萨姆语、马拉地语、卡纳达语、古吉拉特语）。采用轻量级推理策略，受边缘AI设计原则启发，减少冗余评估同时保持跨语言保真度。

Result: 使用三个大型模型发现，攻击成功率在印度语言中急剧上升，特别是在对抗性语法下，而上下文危害转移较为温和。轻量级推理策略使大规模多语言安全测试在计算上可行且环保。

Conclusion: 翻译基准是构建基于资源、语言自适应安全系统的必要第一步，但还不够充分。需要更全面的多语言安全评估方法。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [193] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

TL;DR: SynCog框架通过可控零样本多模态数据合成和思维链推理微调，解决MCI诊断中的数据稀缺和可解释性问题，提升跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基于语音的数字生物标志物为轻度认知障碍早期识别提供了可扩展、非侵入性的方法，但面临临床数据稀缺、缺乏可解释性推理以及跨语言泛化能力不足等挑战。

Method: 提出SynCog框架：1）通过可控零样本多模态数据合成模拟具有不同认知特征的虚拟受试者，缓解数据稀缺问题；2）使用思维链推理策略微调多模态大语言模型，使模型能够明确表达诊断思维过程。

Result: 在ADReSS和ADReSSo基准测试中，通过合成数据增强获得了80.67%和78.46%的Macro-F1分数，优于现有基线模型。在独立真实世界汉语队列（CIR-E）中实现了48.71%的Macro-F1，展示了强大的跨语言泛化能力。

Conclusion: SynCog框架为解决临床数据稀缺和可解释性问题提供了有效方案，为全球医疗保健提供临床可信且语言包容的认知评估工具迈出了关键一步。

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [194] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 研究发现大型语言模型作为自动评估器时，其判决会受到无关上下文线索（如来源、时间、人口统计信息）的显著影响，但这些影响很少在模型的解释中被明确承认，存在解释差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验LLM作为自动评估器是否忠实——即是否仅基于内容质量进行评判，对无关上下文保持不变性，并透明反映决策驱动因素。当前LLM评估的可靠性存在疑问。

Method: 通过控制性线索扰动实验，向评估提示中注入合成元数据标签（来源、时间、年龄、性别、民族、教育状况），测试6个模型在ELI5（事实问答）和LitBench（创意写作）两个数据集上的表现。引入判决转移率和线索承认率两个指标。

Result: 发现模型判决对无关线索敏感：存在来源层级偏好（专家>人类>LLM>未知）、时效性偏好（新>旧）、教育状况偏爱等。但线索承认率通常接近零，表明模型依赖捷径却很少报告。线索承认率还受数据集影响。

Conclusion: LLM作为评估器存在显著判决敏感性和有限线索承认之间的解释差距，这对研究和部署中基于模型的评估可靠性提出了担忧。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [195] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: DeltaKV：基于残差的KV缓存压缩框架，通过编码语义残差而非丢弃token来减少内存占用，结合Sparse-vLLM推理引擎实现2倍吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 高效长上下文LLM部署面临KV缓存内存线性增长的瓶颈，现有压缩和淘汰方法难以平衡准确性、压缩比和硬件效率

Method: 基于两个经验发现（长距离token间相似性和KV表示中高度共享的潜在组件），提出DeltaKV框架：编码语义残差相对于检索到的历史参考，而非直接丢弃token；进一步开发Sparse-vLLM推理引擎，具有解耦内存管理和针对稀疏不规则KV布局优化的内核

Result: DeltaKV将KV缓存内存减少到原始的29%，在LongBench、SCBench和AIME上保持近乎无损的准确性；与Sparse-vLLM集成后，在长上下文场景中相比vLLM实现高达2倍的吞吐量提升

Conclusion: DeltaKV和Sparse-vLLM为可扩展的长上下文LLM部署提供了实用路径，通过残差压缩和高效推理引擎的结合解决了KV缓存内存瓶颈问题

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [196] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: DIP框架通过生成多样化推理策略并整合为最终计划，提升零样本推理准确性，无需依赖资源密集型采样。


<details>
  <summary>Details</summary>
Motivation: 传统Chain-of-Thought提示中无指导推理路径不稳定，而现有方法仅依赖单一推理策略限制了在多样化任务上的性能表现。

Method: 提出Diverge-to-Induce Prompting（DIP）框架：1）为每个问题生成多个多样化高层推理思路；2）将每个思路扩展为详细的分步草稿计划；3）将这些草稿计划整合诱导为最终计划。

Result: 实验表明DIP优于单一策略提示方法，证明了多计划诱导在基于提示的推理中的有效性。

Conclusion: 通过首先生成多样化推理策略然后整合为最终计划，DIP能够在不依赖资源密集型采样的前提下增强零样本推理准确性。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [197] [TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs](https://arxiv.org/abs/2602.08048)
*Arshia Hemmat,Philip Torr,Yongqiang Chen,Junchi Yu*

Main category: cs.CL

TL;DR: 本文提出TDGNet，一种用于扩散语言模型幻觉检测的时序动态图框架，通过建模去噪过程中的注意力图演化来提升检测性能


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有并行去噪和双向上下文优势，但其幻觉检测研究不足。现有的自回归LLM检测器依赖单次推理线索，无法直接迁移到扩散生成场景，因为事实证据分布在去噪轨迹中，可能随时间出现、漂移或自我修正

Method: 提出TDGNet框架，将幻觉检测建模为在演化令牌级注意力图上的学习问题。在每个去噪步骤中：1) 稀疏化注意力图；2) 通过消息传递更新每个令牌的记忆；3) 使用时序注意力聚合整个轨迹的证据进行最终预测

Result: 在LLaDA-8B和Dream-7B模型上的QA基准测试显示，TDGNet在AUROC指标上持续优于基于输出、基于潜在表示和静态图的基线方法，具有单次推理和适度计算开销

Conclusion: 结果表明，对注意力图进行时序推理对于扩散语言模型的鲁棒幻觉检测至关重要，TDGNet框架为这一领域提供了有效的解决方案

Abstract: Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.

</details>


### [198] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: LRTs在隐藏空间中进行推理，自发学习结构化搜索过程：探索阶段→暂定选择→收敛或回溯，回溯有益且主要远离语义相近的干扰项


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在无词思考时的推理过程，探索潜在推理转换器（LRTs）如何在连续隐藏空间中进行审议，与传统的链式思维（CoT）方法形成对比

Method: 使用潜在推理转换器（LRTs），在多项选择QA基准上解码模型每一步演变的信念，分析其在隐藏空间中的结构化搜索过程

Result: 模型自发学习结构化搜索：探索阶段概率分布扩散，暂定选择领先选项，然后收敛或回溯；回溯普遍（32%实例）且有益（准确率提升34%），主要远离语义最接近的干扰项；搜索具有适应性，替换干扰项可缩短探索时间54%

Conclusion: 潜在推理模型在激活空间中实现了链式思维通过文字实现的功能：能够犯错、察觉并恢复，展示了隐藏空间推理的结构化搜索能力

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [199] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 该研究首次系统性地探索了大型语言模型在生成产品推荐时存在的性别和种族偏见，通过提示工程和多种分析方法揭示了推荐结果中的显著人口统计学差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地用于生成消费者产品推荐，但其可能嵌入和放大性别与种族偏见的潜力尚未得到充分探索。本研究旨在填补这一空白，成为首批系统研究LLM推荐系统中偏见问题的尝试之一。

Method: 研究采用提示工程技术，引导LLM为不同种族和性别群体生成产品推荐，然后运用三种分析方法：标记词分析、支持向量机和Jensen-Shannon散度，来识别和量化推荐中的偏见。

Result: 研究发现，针对不同人口统计学群体的推荐存在显著差异，揭示了LLM推荐系统中存在的不平等现象。

Conclusion: 研究结果强调了开发更公平的LLM推荐系统的必要性，为未来减少算法偏见提供了实证基础。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [200] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: DIALSUMMER框架用于评估对话摘要，解决了对话到摘要的结构转换和叙述视角转换的复杂性，提出了分层错误分类法并创建了人工标注数据集。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要评估方法忽略了对话摘要任务特有的复杂性：从多说话者分散讨论到摘要句子的结构转换，以及从第一/第二人称到标准化第三人称的叙述视角转换。

Method: 提出DIALSUMMER框架，包含分层错误分类法：对话级别关注说话者/轮次，轮次内级别关注轮次内的信息；创建人工标注的对话摘要数据集，标注细粒度错误。

Result: 通过实证分析发现有趣趋势：对话中间轮次最容易被摘要遗漏，外部幻觉主要出现在摘要末尾；LLM-Judges在检测这些错误方面表现有限，显示数据集的挑战性。

Conclusion: DIALSUMMER框架和数据集为对话摘要评估提供了全面工具，揭示了现有方法的局限性，强调了未来需要提升LLM在对话摘要错误检测方面的性能。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [201] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: LLMs与人类在跨模态沟通游戏中形成惯例的能力对比：同类配对（人-人、AI-AI）能成功形成沟通惯例，但人-AI异类配对失败，即使AI模仿人类表面行为也无法达到同等沟通效果。


<details>
  <summary>Details</summary>
Motivation: 研究人类在对话中会相互协调形成共享惯例以促进沟通，探索LLMs是否能在跨模态沟通游戏中形成类似的沟通惯例，以及人-AI配对能否达到与同类配对相同的协调效果。

Method: 通过跨模态沟通游戏实验，比较人类-人类、AI-AI、人类-AI三种配对类型的沟通表现。实验1测试自然状态下的沟通协调能力，实验2通过提示让LLMs模仿人类表面行为，观察是否能改善人-AI配对的沟通效果。

Result: 同类配对（人-人、AI-AI）都显示出惯例形成迹象：准确性和一致性提高，信息长度减少。但人-AI异类配对失败，即使实验2中LLMs模仿人类信息长度，准确性和词汇重叠度仍落后于同类配对。

Conclusion: 对话协调不仅需要模仿先前互动的能力，还需要共享对所传达意义的解释偏见。LLMs与人类在沟通倾向上的差异导致人-AI配对难以形成有效的沟通惯例。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [202] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种在预训练中通过内部化潜在思维链来增加每个token计算量的方法，无需扩展参数规模


<details>
  <summary>Details</summary>
Motivation: 通过增加参数和训练数据来扩展大语言模型受到高质量语料库有限和通信成本上升的限制，需要探索替代方案

Method: 提出预训练中的token级自适应潜在思维链方法，模型在生成每个token前生成可变长度的潜在思维链轨迹，对困难token分配更长轨迹，对简单token分配较短或零轨迹

Result: 在Llama架构上的实验表明，自适应潜在思维链方法持续改善语言建模困惑度和广泛的下游任务准确率，即使训练FLOPs比先前循环基线更少

Conclusion: 通过内部化潜在思维链来增加每个token计算量是扩展大语言模型的有效替代方案，能够自适应分配计算资源并减少训练和推理计算量

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [203] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: CoRect通过对比上下文化和非上下文化前向传播的logits来识别参数偏差层，然后修正隐藏状态以保留基于证据的信息，从而解决RAG中的知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: RAG在处理知识冲突时存在问题，模型内部的参数知识会覆盖检索到的证据，导致输出不忠实。现有方法要么依赖表面解码调整，要么需要真实目标进行权重编辑，存在局限性。

Method: 通过层间分析发现参数抑制现象，提出CoRect方法：对比上下文化和非上下文化前向传播的logits来识别高参数偏差层，然后修正隐藏状态以保留证据基础信息。

Result: 在问答和摘要基准测试中，CoRect相比强基线方法持续提高了忠实度并减少了幻觉。

Conclusion: CoRect通过识别和修正参数偏差层，有效解决了RAG中的知识冲突问题，提高了生成输出的忠实度。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [204] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 本文提出了一种无监督强化学习方法，通过让大语言模型在长文档中识别和排序缺失段落来提升长上下文能力，无需人工标注或教师模型监督。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习验证奖励方法通常依赖昂贵的黄金标准答案或教师模型评估标准，成本高且耗时。本文旨在探索无监督方法来增强大语言模型的长上下文能力，避免对人工标注或教师模型监督的依赖。

Method: 首先在长文档中用特殊占位符替换几个段落，然后通过强化学习训练大语言模型从候选选项集中正确识别和排序缺失段落来重构文档。这种训练范式使模型能够捕捉全局叙事连贯性。

Result: 在RULER和LongBench v2两个基准测试上验证了方法的有效性。在RULER上获得显著提升，在LongBench v2上也能实现合理改进，且无需人工策划的长上下文QA数据。通过广泛的消融研究分析了奖励设计、数据策划策略、训练方案和数据规模对性能的影响。

Conclusion: 提出了一种有效的无监督强化学习方法来增强大语言模型的长上下文能力，无需昂贵的人工标注或教师模型监督。该方法通过文档重构任务训练模型捕捉全局叙事连贯性，在多个基准测试上表现出色，并公开了代码、数据和模型。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [205] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

TL;DR: 该研究分析了人类语义类别系统的两个特征：凸性和效率性。通过信息瓶颈框架，发现两者本质不同但常共存于颜色命名系统，其中效率性比凸性更能解释语义类型学现象。


<details>
  <summary>Details</summary>
Motivation: 人类语义类别系统有两个广泛认可的特征：1）在概念空间中形成凸分区，2）对交流具有效率性。虽然先前研究观察到颜色命名中凸性和效率性共存，但两者之间的分析关系以及为何共存尚未得到充分理解。

Method: 结合分析和实证研究，基于信息瓶颈（IB）框架来评估语义效率。首先分析凸性和效率性的逻辑关系，然后在颜色命名领域检验IB最优系统是否具有凸性，最后比较两者在区分实际颜色命名系统与假设变体时的预测能力。

Result: 1）凸性和效率性本质不同：存在凸但低效的系统，也存在最优效率但非凸的系统；2）在颜色命名领域，IB最优系统大多具有凸性，这解释了凸性方法的主要实证基础；3）效率性在区分实际颜色命名系统与假设变体方面是更强的预测因子，凸性在此基础上几乎没有额外改进；4）效率性能解释凸性无法解释的一系列实证现象。

Conclusion: 凸性和效率性虽然能产生相似的结构观察结果，但本质上是不同的概念。效率性为语义类型学提供了更全面的解释框架，而凸性只是效率性在特定领域（如颜色命名）中的伴随特征。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [206] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

TL;DR: 本文提出了一种基于认知语言模式、大语言模型和隐式隐喻的认知语言身份融合评分方法，用于测量身份融合，该方法在预测验证融合分数方面优于现有方法，并揭示了极端主义暴力中的两种高融合路径。


<details>
  <summary>Details</summary>
Motivation: 随着社会极化加剧和政治暴力增多，理解极端主义的心理根源变得日益重要。先前研究表明身份融合能够预测个体参与极端行为的意愿，因此需要更有效的测量方法来研究这一现象。

Method: 研究者开发了认知语言身份融合评分方法，该方法结合认知语言模式、大语言模型和隐式隐喻分析，从语言中测量身份融合。研究在英国和新加坡的数据集上验证了该方法，并将其应用于极端主义宣言分析。

Result: 该方法在预测验证融合分数方面优于现有方法。分析极端主义宣言发现两种不同的高融合暴力路径：意识形态驱动者倾向于以群体框架定义自我，形成亲属关系纽带；而怨恨驱动者则将群体框架纳入个人身份认同。

Conclusion: 研究结果完善了身份融合理论，并提供了一种可扩展的工具，有助于身份融合研究和极端主义检测，为理解极端主义心理机制提供了新的视角和方法。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [207] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 该论文提出将释义分解为不同的语言方面（释义类型），为语义等价性提供更细粒度和认知基础的观点，并证明基于释义类型训练的模型在相关任务和下游应用中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多将释义简化为两个文本之间的二元决策或单一改写，掩盖了哪些语言因素负责意义保留。需要更细粒度地理解语义等价性，这对于计算语言模型理解意义至关重要。

Method: 提出将释义分解为构成的语言方面（释义类型），并训练模型识别这些类型。通过在不同数据集上训练和评估模型，验证基于释义类型训练的方法。

Result: 基于释义类型训练的模型在多项任务中表现优异：在抄袭检测中超越人类基线（维基百科案例89.6% vs 78.4%，arXiv科学论文案例66.5% vs 55.7%）；在Quora重复问题识别中优于基于二元对训练的模型。

Conclusion: 分解释义为语言方面（释义类型）提供了更细粒度的语义等价性视角，基于此训练的模型在相关任务和下游应用中表现更强，表明这种方法对计算语言模型理解意义具有重要价值。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [208] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: RLVR通过概率框架增强LLMs能力，而非仅激发潜在能力，通过优化原子步骤概率来克服多步推理中的指数衰减问题


<details>
  <summary>Details</summary>
Motivation: 解决关于RLVR是赋予LLMs新能力还是仅激发潜在能力的争议，提出能力应通过实例级可解性来定义

Method: 提出概率框架，假设复杂推理能力可通过锐化原子步骤概率来驱动；使用Algebrarium框架，仅在单步操作上训练模型，评估其在未见多步任务上的表现

Result: (1) RLVR通过放大现有技能激励探索先前无法访问的解决路径；(2) 复合性能严格受原子步骤联合概率控制（皮尔逊相关系数ρ∈[0.69, 0.96]）；(3) RLVR作为全局优化器可能导致特定技能被牺牲以最大化总体奖励

Conclusion: RLVR中的涌现能力源于可解问题的迭代优化，使模型能够发展出解决先前不可解场景的能力，支持RLVR赋予新能力的观点

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [209] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 论文提出了一个统一的理论框架来分析Transformer大语言模型中任意上下文信息的影响，通过输出误差动态来表征上下文影响，并推导出误差减少的几何条件。


<details>
  <summary>Details</summary>
Motivation: 当前对于推理时上下文信息（如演示、检索知识、交互历史）如何影响大语言模型的理论理解仍然有限，主要局限于特定设置如上下文学习。需要建立一个统一的理论框架来分析任意上下文信息在Transformer模型中的作用机制。

Method: 提出了一个分析Transformer大语言模型中上下文影响的理论框架，通过输出误差动态来表征上下文影响。在单层Transformer中证明上下文条件误差向量可加性分解为基线误差向量和上下文修正向量，推导出误差减少的几何条件：上下文修正必须与负基线误差对齐并满足范数约束。进一步证明上下文修正范数存在由上下文-查询相关性和互补性决定的上界，并将结果扩展到多上下文和多层Transformer。

Result: 理论分析表明上下文修正必须与负基线误差对齐且满足范数约束才能减少误差，上下文修正范数存在由上下文-查询相关性和互补性决定的上界。在上下文学习、检索增强生成和记忆演化等任务上的实验验证了理论，并基于理论提出了原则性的上下文选择策略，性能提升了0.6%。

Conclusion: 该研究为理解大语言模型中上下文信息的作用提供了统一的理论框架，揭示了上下文影响误差动态的机制，并基于理论推导出了有效的上下文选择策略，为优化上下文使用提供了理论指导。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [210] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

TL;DR: JUSTICE框架通过模拟法官"搜索→预判→撰写"的认知流程，引入预判阶段来提升判决文书生成的法律准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有判决文书生成方法过于简化法律推理过程，特别是忽略了人类法官形成初步结论的"预判"阶段，导致基础司法要素获取不足和预判过程建模不充分，影响了最终文书的法律严谨性。

Method: 提出JUSTICE框架，包含三个核心组件：参考性司法要素检索器（RJER）检索法律条文和先例案例；中间结论模拟器（ICE）生成可验证的中间结论以操作化预判阶段；司法统一合成器（JUS）综合所有输入生成最终判决。

Result: 在领域内法律基准和分布外数据集上的实验表明，JUSTICE显著优于强基线方法，在法律准确性方面取得实质性提升，包括刑期预测准确率提高4.6%。

Conclusion: 明确建模预判过程对于增强生成判决文书的法律连贯性和准确性至关重要，JUSTICE框架通过模拟人类法官的认知工作流程有效解决了现有方法的局限性。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [211] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: 该研究开发了Dr.SCI数据集和训练流程，用于提升大语言模型在开放式科学问题上的表现，通过系统性数据处理、动态难度课程和基于评分标准的强化学习，显著提升了科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在开放式科学问题上的挑战，主要瓶颈在于科学后训练的数据构建和奖励设计，需要处理不可靠的监督和评估问题。

Method: 1) 开发Dr.SCI数据集：将异构开源科学数据转化为包含100万个问题、八个STEM学科的数据集，具有明确的验证/开放式划分、可扩展难度标注和细粒度评分标准；2) Dr.SCI后训练流程：包含探索扩展的监督微调、动态难度课程和基于科学评分标准的强化学习。

Result: 使用Dr.SCI流程训练的Qwen3-4B-Base模型在GPQA-diamond上达到63.2分，在GPQA-general上达到32.4分，持续优于o1-mini和GPT-4o等强基线模型，在科学推理特别是开放式场景中取得显著提升。

Conclusion: Dr.SCI数据集和训练流程有效解决了大语言模型在开放式科学问题上的挑战，通过系统性数据构建和创新的训练策略显著提升了模型的科学推理能力。

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [212] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

TL;DR: 提出基于注意力机制的生成式框架，用于同时处理多意图检测和槽填充任务，解决了现实对话中用户表达多个意图的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话系统主要关注单意图场景，但现实对话中用户常表达多个意图，这对现有系统和数据集构成挑战。

Method: 提出生成式框架，采用注意力-注意力解码器处理可变数量意图和子任务间干扰，通过BERT的NSP头构建新的多意图SLU数据集。

Result: 在MixATIS、MixSNIPS两个公开数据集和自建数据集上，注意力-注意力生成模型均达到最先进性能。

Conclusion: 提出的生成式框架能有效处理多意图SLU任务，注意力机制设计解决了意图数量可变和任务干扰问题，构建的数据集为多意图研究提供了资源。

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [213] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: Thinking States方法让大语言模型在处理输入时并行生成推理标记，将思考嵌入后续输入中，减少推理延迟，在数学推理和问答任务上接近或超越传统思维链性能。


<details>
  <summary>Details</summary>
Motivation: 传统思维链推理虽然能提升大语言模型解决复杂任务的能力，但生成长推理过程会带来显著的推理成本和时间延迟，需要一种更高效的推理方法。

Method: 提出Thinking States方法：在处理输入时每隔几个输入标记就生成一系列思考标记，将这些思考转换回嵌入空间并添加到后续输入标记中，实现并行化推理。

Result: 在多个推理任务上优于其他潜在推理方法，在数学问题上缩小了与传统思维链的差距，在2-Hop QA任务上性能相当但延迟更低，在状态跟踪任务上表现出比思维链更强的推理能力。

Conclusion: Thinking States通过并行生成思考标记的方法，在保持推理能力的同时显著降低了延迟，并能泛化到比训练时更长的序列，是一种高效的推理方法。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [214] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: UReason是一个诊断性基准测试，用于评估推理在图像生成中的实际效果，揭示了"推理悖论"：推理痕迹通常能提升性能，但将中间思考作为条件上下文反而会阻碍视觉合成。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型采用思维链推理来指导图像生成，但推理对视觉合成的实际效果尚不明确。需要评估推理是否能在像素层面忠实执行。

Method: 开发UReason基准测试，包含2,000个实例，涵盖代码、算术、空间、属性和文本推理五个任务族。引入评估框架比较直接生成、推理引导生成和去上下文化生成（仅基于精炼提示）。

Result: 在八个开源统一模型中观察到一致的"推理悖论"：推理痕迹通常优于直接生成，但将中间思考作为条件上下文会阻碍视觉合成，而仅基于精炼提示的条件能带来显著提升。

Conclusion: 瓶颈在于上下文干扰而非推理能力不足。UReason为研究统一模型中的推理提供了原则性测试平台，激励未来方法在有效整合推理进行视觉生成的同时减轻干扰。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [215] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 该研究构建了越南语情感语料库ViGoEmotions，包含20,664条社交媒体评论，标注27种细粒度情感，并评估了8种预训练Transformer模型在三种预处理策略下的情感分类性能。


<details>
  <summary>Details</summary>
Motivation: 情感分类在情感预测和有害内容检测中具有重要作用。虽然NLP领域特别是大语言模型取得了显著进展，但越南语情感分类研究相对缺乏，需要高质量的数据集来支持该领域的发展。

Method: 构建ViGoEmotions越南语情感语料库（20,664条社交媒体评论，27种情感类别）。评估8种预训练Transformer模型在三种预处理策略下的性能：1）保留原始表情符号并进行规则标准化；2）将表情符号转换为文本描述；3）应用ViSoLex模型进行词汇标准化。

Result: 将表情符号转换为文本通常能提升BERT类模型的性能，而保留表情符号对ViSoBERT和CafeBERT效果最佳。移除表情符号通常导致性能下降。ViSoBERT获得最高Macro F1-score（61.50%）和Weighted F1-score（63.26%），CafeBERT和PhoBERT也表现良好。

Conclusion: ViGoEmotions语料库能有效支持多种架构，但预处理策略和标注质量仍是影响下游性能的关键因素。表情符号处理方式对模型性能有显著影响，需要根据具体模型选择适当的预处理方法。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [216] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出基于认知启发的长上下文推理框架，通过分块压缩和选择性记忆召回，而非处理所有原始token，解决LLM长上下文处理中的计算成本、信息遗忘和RAG上下文碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文处理中面临三大挑战：二次计算成本、信息遗忘以及检索增强生成中的上下文碎片化问题。需要一种更高效的长上下文推理方法。

Method: 采用认知启发的分块压缩和选择性记忆召回框架：1) 将长输入分段为块；2) 使用学习到的压缩器将每个块编码为压缩记忆表示；3) 门控模块动态选择相关记忆块；4) 推理模块通过演化的工作记忆迭代处理选中的记忆块解决下游任务。压缩器和推理器通过端到端强化学习联合优化，门控模块作为分类器单独训练。

Result: 在RULER-HQA等多跳推理基准上达到竞争性准确率，上下文长度从7K扩展到1.75M tokens，相比强基线在准确率-效率权衡上表现优越。具体而言，相比MemAgent实现峰值GPU内存使用减少2倍，推理速度提升6倍。

Conclusion: 提出的认知启发框架通过分块压缩和选择性记忆召回，有效解决了LLM长上下文处理的关键挑战，在保持准确率的同时显著提升了计算效率和可扩展性。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [217] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

TL;DR: TEAM框架通过利用专家路由决策的时空一致性，在保持性能的同时加速MoE扩散大语言模型的推理速度


<details>
  <summary>Details</summary>
Motivation: 当前MoE扩散大语言模型存在架构与扩散解码的根本不匹配问题：每个去噪步骤激活大量专家，但最终只接受少量token，导致推理开销大，限制了在延迟敏感应用中的部署

Method: TEAM框架利用专家路由决策在去噪层级间的时间一致性和token位置间的空间一致性，采用三种互补的专家激活和解码策略：保守选择已解码和掩码token的必要专家，同时跨多个候选进行激进的推测性探索

Result: 实验结果显示TEAM相比原始MoE扩散大语言模型实现了最高2.2倍的加速，且性能下降可忽略不计

Conclusion: TEAM是一个即插即用框架，能够有效加速MoE扩散大语言模型，使其更适合延迟敏感的应用场景

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [218] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

TL;DR: 该研究通过实验检验乔姆斯基对ChatGPT的批评，构建不可能语言测试LLMs的学习能力，发现GPT-2小模型在学习不可能语言时表现不佳，而LSTM模型表现符合乔姆斯基论点，揭示了Transformer架构的独特作用。


<details>
  <summary>Details</summary>
Motivation: 乔姆斯基在《ChatGPT的虚假承诺》中批评大型语言模型只是模式预测器，缺乏人类语言习得的内在因果和自我修正结构，无法区分不可能语言。该研究旨在从语言学、心理学和实验角度检验这一著名批评。

Method: 通过应用特定转换（如反转整个句子、基于词数奇偶性添加否定）构建一组语法上不可能的语言。在GPT-2小模型和LSTM模型上进行了两轮对照实验，使用Welch's t-test进行统计分析。

Result: GPT-2小模型在学习所有不可能语言时表现均不如可能语言（p<.001）。LSTM模型的表现与乔姆斯基论点一致，表明Transformer架构的演变具有不可替代的作用。

Conclusion: 基于理论分析和实证发现，提出在乔姆斯基理论框架内对LLMs的新视角，以及从乔姆斯基的"理性主义-浪漫主义"范式转向功能主义和经验主义的理论范式转变。

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [219] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 该论文提出了一个统一框架来解决大型推理模型的三个核心问题：定义高质量推理、评估复杂推理轨迹、以及利用评估信号进行优化。通过ME²原则、DAG建模和TRM奖励模型，实现了推理质量的评估和优化。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型缺乏对推理质量的统一定义，难以可靠评估具有复杂内部结构的推理轨迹，并且不知道如何利用评估信号进行推理优化。现有工作未能统一解决这三个基本问题。

Method: 1. 提出ME²原则，从宏观和微观两个层面（效率和有效性）定义推理质量；2. 将推理轨迹建模为有向无环图（DAG），开发基于DAG的成对评估方法；3. 构建TRM-Preference数据集，训练Thinking Reward Model（TRM）来大规模评估推理质量。

Result: 实验表明：1. 在测试时，选择更好的推理轨迹能带来更好的结果（最高19.3%的提升）；2. 在强化学习训练中，思考奖励能增强推理能力和性能（最高3.9%的提升），在各种任务中均有效。

Conclusion: 该研究提供了一个统一的视角来解决大型推理模型的核心挑战，通过ME²原则、DAG建模和TRM奖励模型，成功实现了对复杂推理轨迹的评估和优化，为推理质量的提升提供了有效方法。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [220] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: GISA是一个用于评估通用信息寻求助手的新基准，包含373个人工构建的真实查询，支持四种结构化答案格式，并提供完整的人类搜索轨迹作为参考。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在三个主要问题：1）查询构建方式不自然（从答案反向构建），与真实需求脱节；2）要么专注于定位特定信息，要么专注于多源信息聚合，缺乏统一；3）依赖静态答案集，容易受到数据污染影响。

Method: GISA基准包含373个人工构建的真实信息寻求场景查询，采用四种结构化答案格式（项目、集合、列表、表格）以实现确定性评估。基准整合了深度推理和广泛信息聚合任务，包含定期更新的实时子集以抵抗记忆，并为每个查询提供完整的人类搜索轨迹。

Result: 实验显示，即使在主流LLM和商业搜索产品中，表现最佳的模型也只能达到19.30%的精确匹配分数。在需要复杂规划和全面信息收集的任务上，性能显著下降。

Conclusion: GISA基准揭示了当前信息寻求助手在真实场景中的局限性，特别是在复杂规划和全面信息收集方面存在显著不足，为未来改进提供了明确方向。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [221] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文通过激活修补和可解释性技术，揭示了LLM处理表格的内部机制，将其分解为语义绑定、坐标定位和信息提取三阶段流程，发现模型通过计数分隔符的序数机制定位单元格，列索引编码在线性子空间中，多单元格定位复用相同注意力头。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型越来越多地用于表格相关任务，但它们处理线性化二维结构化表格的内部机制仍然不透明。本研究旨在通过解析单元格定位这一原子任务来探究表格理解的过程。

Method: 使用激活修补和互补的可解释性技术，将表格理解机制分解为顺序的三阶段流程：语义绑定、坐标定位和信息提取。通过分析模型如何通过计数离散分隔符来解析坐标，并研究列索引在线性子空间中的编码方式。

Result: 研究发现模型通过计数分隔符的序数机制来定位目标单元格，列索引编码在一个线性子空间中，可以通过向量算术精确引导模型注意力。此外，模型通过复用原子定位过程中识别的相同注意力头来泛化到多单元格定位任务。

Conclusion: 这些发现为Transformer架构中的表格理解提供了全面解释，揭示了模型处理结构化表格的内部工作机制，包括坐标解析、注意力引导和多任务泛化机制。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [222] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 本文提出了ALOPE-RL框架，结合强化学习和错误感知奖励，用于英语-马拉雅拉姆语这种低资源语言对的机器翻译质量评估，在有限数据和计算资源下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前质量评估方法主要依赖标量质量分数，缺乏对翻译错误的明确解释，且在低资源语言（如马拉雅拉姆语）中由于标注数据有限，现有方法性能不佳。

Method: 1. 创建了首个英语-马拉雅拉姆语片段级质量评估数据集，包含直接评估分数和翻译质量评注；2. 提出了ALOPE-RL框架，基于策略的强化学习，利用DA分数和TQR作为奖励训练高效适配器；3. 使用LoRA和4位量化技术微调紧凑型大语言模型（≤4B参数）。

Result: ALOPE-RL在英语-马拉雅拉姆语质量评估任务上实现了最先进的性能，超越了更大的LLM基线和领先的基于编码器的质量评估模型，尽管只在小型数据集上训练。

Conclusion: 错误感知的策略学习能够在有限数据和计算预算下提供强大的质量评估性能，为低资源语言对的机器翻译质量评估提供了有效解决方案。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [223] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: VocalNet-MDM采用掩码扩散建模作为非自回归范式，解决了语音大语言模型中自回归方法的效率限制，实现了3.7-10倍解码加速和34%首块延迟降低。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型主要采用自回归范式，存在严格的序列约束，限制了生成效率并引入了暴露偏差。需要探索非自回归范式来提高语音交互的效率和降低延迟。

Method: 提出VocalNet-MDM，采用掩码扩散建模作为非自回归范式。为了解决流式语音交互中的训练-推理不匹配和迭代开销问题，提出了分层块级掩码来对齐训练目标与块扩散解码中的渐进掩码状态，以及迭代自蒸馏来将多步精炼压缩为更少步骤以实现低延迟推理。

Result: 在仅6K小时语音数据上训练，VocalNet-MDM实现了3.7-10倍解码加速，首块延迟降低34%。在保持竞争性识别准确率的同时，实现了最先进的文本质量和语音自然度。

Conclusion: 掩码扩散建模是低延迟、高效语音大语言模型的一个有前景且可扩展的替代方案，能够显著提升语音交互的效率和响应速度。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [224] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

TL;DR: 研究探索多语言大语言模型是否具有语言特定的注意力头，并尝试移除不需要语言的注意力头而不影响目标语言性能


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在生产部署中效率低下，特别是当只需要支持部分语言时。目前缺乏关于多语言LLMs是否具有语言特定注意力头的研究，而机器翻译模型已有相关研究

Method: 研究多语言LLMs是否具有专门的语言注意力头，并探索移除不需要语言的特定注意力头而不影响目标语言性能的可能性

Result: 研究发现多语言LLMs确实具有语言特定的注意力头，并且可以移除不需要语言的注意力头而不显著降低目标语言性能

Conclusion: 该研究为多语言LLMs的高效部署策略提供了依据，能够在保持目标语言高准确性的同时减少模型复杂度

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [225] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 该研究探讨了演绎、归纳和溯因三种基本推理范式如何影响大语言模型的推理行为，通过构建符号任务数据集并采用多种微调方法，显著提升了模型在现实任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管提升大语言模型推理能力的研究很多，但基本推理范式（演绎、归纳、溯因）如何影响模型的泛化能力尚未得到系统探索。研究者希望了解这些核心范式之间的相互作用如何影响LLMs的推理行为。

Method: 首先收集了针对三种基本推理范式的符号任务数据集，以抽象出具体世界知识。然后研究了将这些推理技能注入LLMs的有效方法，包括简单微调、增加模型深度以及将密集模型转换为专家混合模型等多种方法。

Result: 研究结果显示，该方法在现实跨域任务中表现出强大的泛化能力，性能提升显著（最高达14.60分）。这些任务完全用自然语言表述并包含真实世界知识。

Conclusion: 通过系统研究基本推理范式对大语言模型的影响，并采用适当的训练方法，可以显著提升模型在现实任务中的推理泛化能力，为LLM推理能力的提升提供了新思路。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [226] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: LLMs能自主生成评估标准并应用于文本质量评估，但评估可靠性在事实密集型任务中下降，不同模型间评估标准存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有的人工评估标准往往是静态的，与LLMs内部的语言质量表示方式不一致。研究者希望探索LLMs能否设计和应用自己的评估标准，以改善评估的可靠性和可解释性。

Method: 提出GER-Eval方法，让LLMs生成评估标准并应用于自然语言生成评估。研究评估了LLM定义标准的语义一致性、评分可靠性以及与人类标准的对齐程度。

Result: LLMs能可靠地生成可解释且任务感知的评估维度，并在模型内部一致应用。但评分可靠性在事实和知识密集型场景中下降。闭源模型（如GPT-4o）比开源模型（如Llama）具有更高的一致性和跨模型泛化能力。

Conclusion: 评估是LLMs的一种学习到的语言能力，在模型内部一致但在不同模型间存在差异。需要新方法来联合建模人类和LLM的评估语言，以提高可靠性和可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [227] [Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement](https://arxiv.org/abs/2602.08688)
*Hossein Kermani,Fatemeh Oudlajani,Pardis Yarahmadi,Hamideh Mahdi Soltani,Mohammad Makki,Zahra HosseiniKhoo*

Main category: cs.CL

TL;DR: 比较三种检测波斯语推文不文明内容的方法：人工编码、ParsBERT监督学习和大型语言模型（ChatGPT），发现在检测仇恨言论方面ParsBERT显著优于ChatGPT


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较不同方法在低资源语言（波斯语）环境下检测仇恨言论和不文明内容的准确性和效率，特别是在#MahsaAmini运动背景下

Method: 使用47,278条波斯语推文，比较三种方法：1）人工定性编码；2）基于ParsBERT的监督学习；3）ChatGPT等大型语言模型。评估了七种ChatGPT模型，并测试了提示语言（英语vs波斯语）的影响

Result: ParsBERT在识别仇恨言论方面显著优于所有评估的ChatGPT模型。ChatGPT不仅在微妙案例上表现不佳，在处理明确的不文明内容时也存在困难。提示语言（英语vs波斯语）对ChatGPT输出没有显著影响

Conclusion: 研究提供了不同方法在低资源语言环境下分析仇恨言论的详细比较，阐明了各自的优势和局限性。对于波斯语等低资源语言的仇恨言论检测，基于特定语言训练的模型（如ParsBERT）比通用大型语言模型（如ChatGPT）表现更好

Abstract: This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.

</details>


### [228] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

TL;DR: 该研究探讨了机器翻译在孟加拉语、马拉雅拉姆语和泰卢固语等印度语言中的实际应用和方法论意义，重点关注新兴翻译工作流程和现有评估框架，使用NPTEL MOOC平台作为语料库。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于印度语言多样性背景下教育技术的多语言适应需求，特别是在NEP 2020政策框架下。选择这三种语言体现了语言多样性的三角验证，同时NPTEL作为印度最大的MOOC平台为研究提供了丰富的语料资源。

Method: 研究方法包括：1）使用NPTEL MOOC平台作为主要语料库；2）构建自然口语语料库，考虑技术概念的清晰表达、适当语域和词汇选择；3）针对形态丰富和语义紧凑的语言特征，测试表面重叠度指标的敏感性。

Result: 研究发现：1）指标对特定语言特征具有敏感性；2）形态丰富和语义紧凑的语言特征在表面重叠度指标测试中面临挑战；3）技术概念的自然口语表达需要考虑语域和词汇选择的保留。

Conclusion: 研究结论强调了在印度这样语言多样化的国家中，构建考虑技术概念清晰表达、适当语域和词汇选择的自然口语语料库的重要性，同时指出现有评估框架在处理形态丰富语言时的局限性。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [229] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 研究探讨了在对话式搜索系统中，将图像融入澄清问题对用户性能的影响。通过73名参与者的用户研究，比较了多模态（文本+图像）和纯文本澄清问题在回答澄清问题和查询重构两个任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 虽然文本澄清问题已被证明能提升检索性能和用户体验，且图像在各种上下文中能改善检索性能，但将图像融入澄清问题对用户性能的影响尚未得到充分探索。本研究旨在填补这一空白，探究多模态澄清问题在对话式搜索中的作用。

Method: 对73名参与者进行用户研究，比较多模态（文本+图像）和纯文本澄清问题在对话式搜索环境中的效果。研究聚焦两个搜索相关任务：1) 回答澄清问题；2) 查询重构。从多个角度分析两种问题形式的影响。

Result: 在回答澄清问题时，参与者强烈偏好多模态问题，但纯文本设置表现出更好的用户性能（因提供更全面的文本信息）。在查询重构任务中，偏好更平衡，图像帮助生成更精确的查询并提升检索性能。图像效果因任务类型和用户专业水平而异：在回答澄清问题时，图像帮助维持不同专业水平用户的参与度；在查询重构中，图像提升查询精确度和检索性能。

Conclusion: 视觉增强的益处是任务依赖性的，应根据具体搜索上下文和用户特征进行战略性实施。多模态澄清问题在对话式搜索中的效果因任务而异：在回答澄清问题时，纯文本可能更有效；在查询重构中，图像能带来实质性改进。这些发现为设计有效的多模态对话式搜索系统提供了重要见解。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [230] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

TL;DR: 该论文提出了一种新的全自动评估方法，用于评估生成式AI在意见摘要任务中的事实一致性，通过测量摘要中的主张与原始评论的相似性来克服传统自动指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于自动指标的方法在评估生成式AI的意见摘要时存在局限性，特别是由于大型语言模型带来的范式转变。需要更全面和精确的评估技术来评估摘要的事实一致性。

Method: 提出了一种全自动方法，通过测量摘要中的主张与原始评论的相似性来评估事实一致性。采用简单的方法从文本中提取事实评估，然后比较并总结为合适的分数。

Result: 提出的指标能为相似的主张分配更高的分数，无论主张是否被否定、改写或扩展。与最先进的指标相比，该分数与人类判断具有高度相关性。

Conclusion: 该方法为生成式AI在意见摘要任务中的事实一致性评估提供了一种有效的全自动解决方案，克服了传统评估方法的局限性。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [231] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

TL;DR: PERSPECTRA是一个评估LLM多元主义能力的基准，结合了Kialo辩论图的结构清晰性和Reddit讨论的语言多样性，包含3,810个扩展论点，覆盖100个争议话题。


<details>
  <summary>Details</summary>
Motivation: 多元主义（能够处理不同观点而不将其简化为单一视角）对LLM忠实反映人类异质性至关重要，但这一特性在LLM研究中尚未得到仔细检验，且大多对齐研究中缺失。现有辩论数据源存在局限性：Reddit有语言多样性但缺乏清晰论证结构，Kialo有明确正反方图但过于简洁且脱离自然话语。

Method: 采用受控检索和扩展流程，将Kialo辩论图的结构清晰性与Reddit讨论的语言多样性相结合。构建了3,810个扩展论点，覆盖762个正反方立场和100个争议话题。每个观点扩展为多个自然变体，用于评估多元主义能力。

Result: 实验显示最先进的开源和专有LLM存在系统性失败：高估观点数量、误分类让步结构等，突显了多元主义理解和推理的困难。PERSPECTRA是首个可扩展、可配置的基准，用于评估模型如何表示、区分和推理多个视角。

Conclusion: PERSPECTRA通过结合多样性和结构，建立了首个可扩展、可配置的基准，用于评估模型在表示、区分和推理多个视角方面的能力，揭示了当前LLM在多元主义理解方面的系统性缺陷。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [232] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

TL;DR: 本文提出了一种大规模比较和可视化句子编码器的方法，通过创建编码器地图，将每个句子编码器与其他编码器的关系可视化。该方法使用量子相对熵特征向量表示编码器，构建了包含1101个公开可用句子编码器的地图。


<details>
  <summary>Details</summary>
Motivation: 随着预训练句子编码器数量的快速增长，需要一个系统性的方法来比较和可视化这些编码器，理解它们之间的关系和性能特征。传统方法难以在大规模上有效比较编码器。

Method: 首先用句子集的嵌入矩阵表示每个句子编码器，然后计算其成对内积（PIP）矩阵，最后为每个编码器创建反映其与单位基础编码器量子相对熵（QRE）的特征向量，从而构建编码器地图。

Result: 构建了包含1101个公开可用句子编码器的地图，该地图准确反映了编码器之间的各种关系，具有相似属性的编码器在地图上位置相近。编码器特征向量能够准确预测编码器在下游任务（如检索和聚类）中的性能。

Conclusion: 该方法提供了一种有效的大规模句子编码器比较和可视化框架，编码器地图能够准确反映编码器关系，特征向量可用于预测下游任务性能，为预训练句子编码器的景观提供了新的视角。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [233] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: LakeHopper：一个通过识别知识差距、聚类选择目标数据、增量微调来将预训练语言模型适应新数据湖的框架，减少标注需求


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的列类型标注方法需要大量标注数据，且通常针对特定数据湖训练。当需要将模型应用到新数据湖时，面临源-目标知识差距、如何选择信息丰富的目标数据、以及微调时不丢失共享知识等挑战。

Method: 提出LakeHopper框架：1）通过语言模型交互识别和解决源-目标知识差距；2）采用基于聚类的数据选择方案从未标注列中选择信息丰富的样本；3）使用增量微调机制逐步将源模型适应到目标数据湖。

Result: 在两个不同数据湖迁移任务上进行了实验验证，包括低资源和高资源设置，结果证明了LakeHopper的有效性。

Conclusion: LakeHopper能够有效减少新数据湖的标注需求，通过解决知识差距、智能数据选择和渐进式微调，成功将预训练模型适应到新的数据环境。

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [234] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: AFlow框架通过建模多轮对话中的连续情感流，为情感支持对话提供细粒度监督，显著提升策略连贯性和响应质量


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统依赖稀疏的结果级信号，对中间策略决策的监督有限，导致复杂多轮支持效果不佳

Method: 提出AFlow框架，通过建模连续情感流来监督对话前缀，估计中间效用并学习偏好一致的策略转换，引入子路径级流平衡目标提升策略连贯性

Result: 在多样化情感场景中显著超越现有基线，紧凑的开源骨干模型甚至优于GPT-4o和Claude-3.5等专有大语言模型

Conclusion: AFlow通过细粒度情感流监督有效解决了情感支持对话中的中间策略决策问题，为复杂多轮情感支持提供了新思路

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [235] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: WildReward：直接从用户交互中训练奖励模型，无需人工标注偏好对，性能媲美传统奖励模型


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖大规模人工标注的偏好对，成本高昂。随着大语言模型的广泛部署，用户交互成为丰富的隐式奖励信号来源，能否直接从用户交互中训练奖励模型？

Method: 采用WildChat作为交互数据源，提出从用户反馈中提取可靠人类反馈的流程，通过序数回归直接训练WildReward，无需偏好对标注

Result: WildReward在18.6万高质量实例上训练，性能与传统奖励模型相当甚至更优，具有更好的校准性和跨样本一致性；用户多样性直接提升模型性能；应用于在线DPO训练在各种任务上取得显著改进

Conclusion: 直接从用户交互中训练奖励模型是可行的，WildReward展示了无需人工标注偏好对也能获得高质量奖励模型，为奖励模型训练提供了新范式

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [236] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 该研究提出了一个复杂性控制的评估范式，通过算法和合成语言任务直接测试token级计算分配，开发了ANIRA统一循环Transformer框架，并系统分析了token级自适应计算与复杂性对齐、泛化和决策时机的关系。


<details>
  <summary>Details</summary>
Motivation: 先前关于token级自适应计算的研究主要在自然语言基准上使用任务级指标进行评估，其中token级难度不可观察且与架构因素混淆，无法确定计算分配是否真正与底层复杂性对齐。

Method: 1. 引入复杂性控制评估范式，使用参数化难度的算法和合成语言任务；2. 提出ANIRA统一循环Transformer框架，支持每个token可变深度计算，同时将计算分配决策与其他模型因素隔离；3. 使用该框架系统分析token级自适应计算。

Result: 结果显示：与任务复杂性对齐的计算分配可以在没有显式难度监督的情况下出现，但这种对齐并不意味着算法泛化：模型无法泛化到未见过的输入大小，尽管分配了额外的计算。早期计算决策依赖于静态结构线索，而在线停止更紧密地跟踪算法执行状态。

Conclusion: 研究通过复杂性控制评估和ANIRA框架，揭示了token级自适应计算与复杂性对齐的机制，但发现这种对齐不能保证算法泛化能力，为理解自适应计算提供了新的见解。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [237] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

TL;DR: LLM结合智能体地理编码的两步框架，显著提升了人道主义文本中地理位置提取的准确性和公平性，特别是对欠代表地区。


<details>
  <summary>Details</summary>
Motivation: 人道主义危机需要及时准确的地理信息，但现有自动化系统在提取文本位置时往往复制现有的地理和社会经济偏见，导致危机受影响地区的可见性不均。

Method: 提出了一个两步框架：1) 基于LLM的少样本命名实体识别；2) 基于智能体的地理编码模块，利用上下文解析模糊地名。在扩展的HumSet数据集上进行了评估。

Result: LLM方法在准确性和公平性指标上都显著优于最先进的预训练和基于规则的系统，特别是在欠代表地区的地理位置提取方面有显著改进。

Conclusion: 通过将LLM推理进展与负责任和包容性AI原则相结合，这项工作为人道主义响应贡献了更公平的地理空间数据系统，推进了危机分析中"不让任何地方掉队"的目标。

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [238] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

TL;DR: 研究发现，在长上下文环境中，更强的推理能力并不能自动提高LLM的安全性。通过组合推理攻击，即使有害意图被分解并分散在长文本中，模型仍可能合成有害内容并执行。安全对齐效果随上下文长度增加而下降，但增加推理时计算能显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证一个假设：更强的推理能力应该通过帮助模型识别隐含的有害意图来提高安全性。研究者在长上下文环境中测试这个假设，其中有害意图是隐含的，需要通过推理来推断。

Method: 研究者引入了组合推理攻击这一新威胁模型，将有害查询分解为不完整的片段，分散在长上下文中。然后使用中性推理查询诱导模型进行检索和合成，使有害意图仅在组合后出现。在14个前沿LLM上评估，上下文长度达64k token。

Result: 三个主要发现：1）具有更强通用推理能力的模型对组合推理攻击并不更鲁棒，经常组装意图却未能拒绝；2）安全对齐效果随上下文长度增加而一致下降；3）推理时计算是关键缓解因素：增加推理时计算使GPT-oss-120b模型的攻击成功率降低超过50个百分点。

Conclusion: 安全性不会自动随推理能力扩展，特别是在长上下文推理下。需要专门的安全措施来应对组合推理攻击，不能依赖通用推理能力的提升。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [239] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

TL;DR: 该论文认为当前语言识别系统覆盖范围有限的问题源于将LID视为去语境化的文本分类，忽视了先验概率估计的重要性，并提出应将LID重构为路由问题，结合环境线索来提高尾部语言的识别覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前商业语言识别系统仅能可靠识别几百种书面语言，研究级系统在某些情况下扩展了覆盖范围，但对大多数语言来说覆盖率仍然零散或不存在。作者认为这种情况主要是自我造成的，源于将语言识别框架化为去语境化的文本分类问题。

Method: 该立场论文提出需要重新思考语言识别问题：将其视为路由问题而非简单的文本分类。主张开发原则性方法来整合环境线索，使语言在本地环境中变得合理可识别，而不是依赖全局、固定先验的模型。

Result: 论文没有提供具体的实验结果，而是提出了一个理论框架和分析。作者指出当前方法的问题在于忽视了先验概率估计的核心作用，以及制度激励偏向于全球固定先验模型，这限制了尾部语言的覆盖。

Conclusion: 要提高尾部语言的覆盖范围，需要从根本上重新思考语言识别问题：将其从去语境化的文本分类重构为路由问题，并开发系统性的方法来整合环境线索，使语言在特定上下文中变得可识别。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [240] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出Next Concept Prediction (NCP)预训练范式，通过预测跨越多个token的离散概念来构建更具挑战性的预训练目标，相比传统token级模型获得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统Next Token Prediction (NTP)在token级别进行预测，而NCP旨在预测更高层次的离散概念，这些概念跨越多个token，形成更困难的预训练任务，从而训练出更强大的语言模型。

Method: 提出ConceptLM模型，使用向量量化(Vector Quantization)对隐藏状态进行量化，构建概念词汇表。模型同时利用NCP和NTP驱动参数更新，先生成概念再指导后续token的生成。在70M到1.5B参数规模上从头训练，使用Pythia和GPT-2架构，训练数据达300B。

Result: 在13个基准测试上，NCP相比传统token级模型获得一致性能提升。在8B参数的Llama模型上的持续预训练实验表明，NCP能进一步提升已通过NTP训练的模型性能。

Conclusion: NCP通过引入更困难的预训练任务，能够训练出更强大的语言模型，为改进语言建模提供了有前景的路径。概念级别的预测比token级别的预测更具挑战性，从而带来更好的模型性能。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>
